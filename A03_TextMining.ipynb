{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinamilo6/Textmining/blob/main/A03_TextMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X1D-_ww7fQTB"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas sklearn nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS1_jLQofQTD"
      },
      "source": [
        "# Assignment 3 - Text Mining\n",
        "\n",
        "Project management and tools for health informatics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufi0zQidfQTE"
      },
      "source": [
        "## 1. Download and prepare data:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "The code in this section downloads the [IMDB IMDB Large Movie Review Dataset]('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz') which is the dataset you will be working on in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aOzKCiw4fQTE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sq9rCBBrfQTE"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('aclImdb'):\n",
        "    # download data:\n",
        "    urlretrieve('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', 'aclImdb.tar.gz')\n",
        "\n",
        "    # unzip data:\n",
        "    with tarfile.open('aclImdb.tar.gz') as file:\n",
        "        file.extractall('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9pzG__QfQTF"
      },
      "source": [
        "## 2. Some helper Functions:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "This section contains the code for some helper functions that will be useful for solving the assignment. Example code on how to use the functions is provided in section 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LaM3GoopfQTF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from typing import Literal, Tuple, Iterable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxsGe7MufQTF"
      },
      "source": [
        "Function for loading data into a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tKoAn698fQTG"
      },
      "outputs": [],
      "source": [
        "def load_data(split:Literal['train', 'test'], texts_per_class:int=500) -> pd.DataFrame:\n",
        "    ''' Loads the data into a pandas dataframe.'''\n",
        "    paths  = []\n",
        "    labels = []\n",
        "\n",
        "    for label in ('pos', 'neg'):\n",
        "        # get all files in the folder:\n",
        "        files = os.listdir(os.path.join('aclImdb', split, label))[:texts_per_class]\n",
        "\n",
        "        # append them to the lists:\n",
        "        paths.extend([os.path.join('aclImdb', split, label, f) for f in files])\n",
        "        labels.extend([label] * len(files))\n",
        "\n",
        "    return pd.DataFrame({'path':paths, 'label':labels})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C1jyAtfQTG"
      },
      "source": [
        "Function for loading a specific text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y0QGuZMXfQTH"
      },
      "outputs": [],
      "source": [
        "def load_text(path:str) -> str:\n",
        "    ''' Reads a single text given the path. '''\n",
        "    # read file from disk:\n",
        "    with open(path, 'r', encoding='utf8') as file:\n",
        "        s = file.read()\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR52KxrvfQTH"
      },
      "source": [
        "Function for iterating through multiple texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NImrB2NzfQTH"
      },
      "outputs": [],
      "source": [
        "def iterate_texts(data:pd.DataFrame) -> Iterable[Tuple[str, str]]:\n",
        "    ''' Iterates through a pandas dataframe. '''\n",
        "\n",
        "    for path in data['path'].values:\n",
        "        # read file from disk:\n",
        "        with open(path, 'r', encoding='utf8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        yield text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzYgen2AfQTH"
      },
      "source": [
        "## 3. Text Mining Pipeline\n",
        "\n",
        "This section will cover the text mining steps for this assignment. The following steps will be performed:\n",
        "\n",
        "1. **Analyze the Data for Difficult Parts**  \n",
        "   - Review the data to identify challenging aspects such as contractions, informal language, and complex sentence structures.\n",
        "\n",
        "2. **Replace Contractions and Informal Language**  \n",
        "   - Expand common contractions and replace informal phrases, if necessary, to standardize the text for processing.\n",
        "\n",
        "3. **Tokenize the Texts**  \n",
        "   - Apply a suitable tokenizer to break down the text into individual tokens for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2o7r5Bug-U"
      },
      "source": [
        "### Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "57LKdqhTug-V"
      },
      "outputs": [],
      "source": [
        "# Import needed libaries for the preparation of the texts\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3FnCEu6fQTH"
      },
      "source": [
        "### Load the training data\n",
        "The loaded data frpom the zip file should be saved into a `data_train` and `data_test` DataFrame.\n",
        "These can be further on be used to access the train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CeshtSl9fQTI",
        "outputId": "fa88fb56-74c9-47ca-d8fa-c3b9b6e19fe5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aclImdb\\train\\pos\\0_9.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aclImdb\\train\\pos\\10000_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aclImdb\\train\\pos\\10001_10.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aclImdb\\train\\pos\\10002_7.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aclImdb\\train\\pos\\10003_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>aclImdb\\train\\neg\\10446_2.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>aclImdb\\train\\neg\\10447_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>aclImdb\\train\\neg\\10448_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>aclImdb\\train\\neg\\10449_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>aclImdb\\train\\neg\\1044_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               path label\n",
              "0         aclImdb\\train\\pos\\0_9.txt   pos\n",
              "1     aclImdb\\train\\pos\\10000_8.txt   pos\n",
              "2    aclImdb\\train\\pos\\10001_10.txt   pos\n",
              "3     aclImdb\\train\\pos\\10002_7.txt   pos\n",
              "4     aclImdb\\train\\pos\\10003_8.txt   pos\n",
              "..                              ...   ...\n",
              "995   aclImdb\\train\\neg\\10446_2.txt   neg\n",
              "996   aclImdb\\train\\neg\\10447_1.txt   neg\n",
              "997   aclImdb\\train\\neg\\10448_1.txt   neg\n",
              "998   aclImdb\\train\\neg\\10449_4.txt   neg\n",
              "999    aclImdb\\train\\neg\\1044_4.txt   neg\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train = load_data('train')\n",
        "data_test  = load_data('test')\n",
        "data_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qXdudbDfQTI"
      },
      "source": [
        "### 1. Assess the data / texts for difficult parts\n",
        "In this part, sample texts are printed and then analyzed for diffult parts, that could affect the text mining process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soE8Qj9hsF0E",
        "outputId": "9f3f109e-dedd-4ec9-a9ce-6fb530a35fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1:\n",
            "\n",
            "We so often talk of cinema landmarks - Kane, The Godfather, A Bout de Souffle. One film however is too often overlooked by \"serious\" film critics. I am talking of course about the classic Doc Savage (M.o.B.)<br /><br />This film is not only exciting but also seriously explores the issue of exploitation of the developing nations by US imperialism. Not to mention kung-fu.<br /><br />It also possessed the greatest soundtrack in film history (until of course Queen's breathtaking work on Flash Gordon). Although a bit of a rarity, this film is well worth seeking out - it will repay the effort of your search ten-fold.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 2:\n",
            "\n",
            "I think James Cameron might be becoming my favorite director because this is my second review of his movies. Anyway, everyone remembers the RMS Titanic. It was big, fast, and \"unsinkable\"... until April 1912. It was all over the news and one of the biggest tragedies ever. Well James Cameron decided to make a movie out of it but star two fictional characters to be in the spotlight instead of the ship. Well, onto the main review but let me remind you that this is all opinion and zero fact and the only fact that will be present is an event from the film.<br /><br />So our two main characters are Jack (Leonardo DiCaprio) and Rose (Kate Winslet). They're not annoying too much but watch this and you'll find out why they could become annoying ( http://tinyurl.com/ojhoyn ). The main villain I guess is bad luck, fate, hand of God (no blasphemy intended), or just plain Caledon Hockley (Billy Zane). Combine all of the above and what do you get?! Oh yes! We get a love story on a sinking boat. The supporting characters are the following: My personal favorite, Mr. Andrews (Victor Garber)(idk he was so nice), Lovejoy(David Warner), Murdoch(Ewan Stewart), Lightoller (Jonathan Phillips), Captain Smith(Bernard Hill), Molly Brown(Kathy Bates), and many more. We also got the present day treasure hunter, Brock Lovett (Bill Paxton). They add something to the story, something good. The action in here is awesome, especially in the second half, the drama as also good. In the end you can have your eyes dropping rainstorms or silent tears. The story is simple and it works. A treasure hunter seeks the Heart of the Ocean and instead finds a drawing of a woman wearing the said diamond. She calls and tells her tale on the RMS Titanic. Two lovers separated by social class and ultimately, the fate of the ship. Everything about the story works and there are very few flaws. I give Titanic, an 86% awesome\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 3:\n",
            "\n",
            "De Grot is a very good film. The great plot comes from the novel by Tim Krabbé, who also adapted this story for the screen. Some really top-class acting, not only by Van Huêt, but especially by Marcel Hensema, who mostly did TV-work prior to his performance of Axel van de Graaf. The film seems to kick of as a thriller, and sets an excellent mood. Then we start to learn about Egon Wagter and Axel van de Graaf, and the story is revealed bit by bit in a very compelling flash-back structure, which adds to the more romantic aspect and the character-driven drama of the movie. In the end this all culminates into an emotional ending, that will grab audiences by their throats. Make sure you know as little as possible about the plot when you are going to see this movie. A must-see, especially if you liked 'Spoorloos' (The Vanishing's original screen adaptation).\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 4:\n",
            "\n",
            "This spectacular film is one of the most amazing movies I have ever seen. It shows a China I had never seen or imagined, and I believe it shows 1930's China in the most REAL light ever seen in a movie. It is absolutely heart-breaking in so many situations, seeing how hard life was for the characters, and yet the story and the ending are incredibly joyful. You truly see the depths and heigths of human existence in this film. The actors are all perfect, such that you feel like you have really entered a different world. <br /><br />I simply can not recommend this movie highly enough. It may just change you forever once you have seen it.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 5:\n",
            "\n",
            "There are many people in our lives that we meet only once in our lifetime, but for some reason or another we remember those persons for the rest of our lives. These once in a lifetime friendships occur between people with long distances between and there are always some natural reasons for why we don't meet these people anymore. We don't always even know their names, as we are never presented to each other, and sometimes we even forget to ask what their names are. It's funny how common humanity makes occasional friends and we like to keep it as such, because reuniting might spoil fond memories, or we don't know do they. We are too afraid to check that out.<br /><br />The movie 'Before Sunrise' just caught me watching it. I never had intention to watch it through, but because the discussion between the couple seemed interesting, I gave a look for the rest of the film. I didn't know what to expect from it, but nor did the young couple. They had time to discuss with each other until the sunrise and anything could happen before they had to separate. I believe this film has had good reviews because the situation is something that everybody on this planet has at least once or twice lived through. It makes us all think about all those people we have met only once in our lives.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 6:\n",
            "\n",
            "Remember these two stories fondly and in the first, set in the not too distant future, we see a young boy preparing for examination day, the state i.q test. The boy is slightly puzzled as to his parents anxiety as some of his friends have already done it already and eventually goes off to do the test. Upon arriving he is given an injection and is curious as to why. The examiner smiles and tells him that it is just to make sure he tells the truth. The boy then asks, puzzled again, why wouldn't he? It is later and the parents are sitting waiting worriedly by the screen when a message appears and declares that the state are sorry, but their son's i.q level has exceeded the national quotient and ask politely would they like a private burial. A corker of a concluding scene! A Message From Charity was a heart warming story about a fluke mental connection between a girl from the past and a guy from the present. Which pans out into a weird story of witchcraft accusations in the past and delving into the history pages in the present. A nice story with a heartwarming conclusion.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 7:\n",
            "\n",
            "Hello Dave Burning Paradise is a film for anyone who likes Jackie Chan and Indiana Jones. The films main protagonist is most definitely the bastard son of these two strange fathers. As for the other characters well they are familiar transformations of similar action film stereotypes. Where this film is original is in the blending of the traditional Hong Kong movie style with the Hollywood action adventure. Sadly this has not been true of the films he has made in Hollywood.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Number of samples to be printed\n",
        "n_samples = 7\n",
        "\n",
        "# Randomly sample indices from \"data_train\"\n",
        "sample_indices = random.sample(range(len(data_train)), n_samples)\n",
        "\n",
        "# Load and print each sample using the \"load_text\" function\n",
        "for i, id in enumerate(sample_indices, start=1):\n",
        "    # Load text from the file path specified in 'path' column\n",
        "    text = load_text(data_train.loc[id, 'path'])\n",
        "    print(f\"Sample {i}:\\n\")\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWw3AZ8SumVn"
      },
      "source": [
        "## Limitations/Issues Captured from the Above Text Samples\n",
        "\n",
        "- **HTML tags and special characters**\n",
        "- **Punctuation and symbols** (e.g., `&`)\n",
        "- **Contractions** (e.g., \"isn't\", \"I'll\", \"I'm\")\n",
        "- **Parentheses and annotations** (e.g., \"(Crouching Tiger)\")\n",
        "- **Informal formatting** (e.g., \"my rating is ****\")\n",
        "- **Ambiguity and polysemy** (e.g., \"dictators\", \"nuts\")\n",
        "- **Long and complex sentences**\n",
        "- **Informal language** (e.g., \"what can be so bad about that?\")\n",
        "- **Quotation marks** (e.g., \"dictators\", \"sin\")\n",
        "\n",
        "We will elaborate on these issues and add our conclusions in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "666GKq5Cug-Z"
      },
      "source": [
        "### 2. Preprocessing: Simplify the Text\n",
        "\n",
        "This section focuses on preprocessing the texts to make them more suitable for text mining. Identified constraints from the analysis step will be addressed and, as far as possible, eliminated to improve processing accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Zgm5pNuGJNM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    # Regex to match HTML tags\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZR8XyEHiGb94"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation_and_symbols(text):\n",
        "    # Remove punctuation and special symbols using regex\n",
        "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eYMTaZhZHOlZ"
      },
      "outputs": [],
      "source": [
        "def remove_parentheses(text):\n",
        "    # Remove text inside parentheses along with parentheses\n",
        "    clean_text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9PoskZmug-b"
      },
      "source": [
        "#### Expand the contractions\n",
        "There are python libraries that focus on expanding contractions. For easier loading (if these packages are not available) we decided to create our own replacement list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NaTBJ0qfG5uy"
      },
      "outputs": [],
      "source": [
        "# Dictionary of common contractions and their expanded forms\n",
        "contractions_dict = {\n",
        "    \"i'm\": \"I am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"i've\": \"I have\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"i'll\": \"I will\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"here's\": \"here is\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"shan't\": \"shall not\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b', flags=re.IGNORECASE)\n",
        "    def replace(match):\n",
        "        return contractions_dict.get(match.group(0).lower(), match.group(0))\n",
        "    return contractions_pattern.sub(replace, text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zrIoCD2IHqLy"
      },
      "outputs": [],
      "source": [
        "# Define a set of informal tokens to remove\n",
        "informal_tokens = {\n",
        "    \"u\": \"you\",\n",
        "    \"r\": \"are\",\n",
        "    \"lmao\": \"\",  # Remove\n",
        "    \"lol\": \"\",   # Remove\n",
        "    \"btw\": \"by the way\",\n",
        "    \"idk\": \"I do not know\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"gonna\": \"going to\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"gotta\": \"got to\",\n",
        "    \"b/c\": \"because\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"pls\": \"please\",\n",
        "    \"cuz\": \"because\",\n",
        "    \"wut\": \"what\",\n",
        "    \"smh\": \"\",  # Remove\n",
        "    \"k\": \"okay\",\n",
        "    \"ttyl\": \"talk to you later\"\n",
        "}\n",
        "\n",
        "def remove_informal_tokens(text):\n",
        "    # Replace informal tokens\n",
        "    for token, replacement in informal_tokens.items():\n",
        "        # Use regex to match whole words and replace them\n",
        "        text = re.sub(r'\\b' + re.escape(token) + r'\\b', replacement, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def fix_punctuation(text):\n",
        "    # Ensure proper spacing after punctuation\n",
        "    text = re.sub(r'\\s*([.,;:!?()])\\s*', r'\\1 ', text)  # Ensure space after punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = text.strip()  # Trim leading and trailing spaces\n",
        "    return text\n",
        "\n",
        "# Combined function to clean the text\n",
        "def clean_text(text):\n",
        "    text = remove_informal_tokens(text)  # Remove informal tokens\n",
        "    text = fix_punctuation(text)  # Fix punctuation\n",
        "    return text\n",
        "\n",
        "def preprocess_ellipses(text):\n",
        "    # Replace occurrences of three or more dots with actual ellipses\n",
        "    text = re.sub(r'(\\.\\s*){3,}', '...', text)\n",
        "    return text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81A7y_1sug-c",
        "outputId": "34d3e3d4-c1a0-4a0b-b421-28cfe52439f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kentf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1:\n",
            "\n",
            "Well, least theater group did, . So course I remember watching Grease since I little girl, never favorite musical story, still hold little special place heart since still lot fun watch. I heard horrible things Grease 2 I decided never watch it, boyfriend said really bad friend agreed, I decided give shot, I called laughed. First plot totally stolen first one really clever, mention used characters, different names actors. Tell me, Pink Ladies T-Birds continue years former gangs left? Not mention creator face motor cycle enemy, gee, striking resemblance guys first film well T-Birds stupid ridiculous. <br /><br />Another year Rydell music dancing stopped. But new student Sandy's cousin comes scene, love struck pink lady, Stephanie. But must stick code Pink Ladies must stick T-Birds, new student, decides train T-Bird win heart. So dresses rebel motor cycle bandit ride well defeat evil bikers easily kicking T-Bird's butts. But tell Stephanie really find own? Well, find yourself. <br /><br />Grease 2 like silly TV show sort work. The gang click well first Grease did, mention Frenchy coming back bit silly unbelievable, I thought graduated Rydell, apparently not. The songs really catchy; I glad Michelle able bounce back fast, probably one talent silly little sequel, I would really recommend film, curious, I warned you, pathetic attempt money famous musical. <br /><br />2/10\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 2:\n",
            "\n",
            "Homelessness( or Houselessness George Carlin stated) issue years never plan help street considered human everything going school, work, vote matter. Most people think homeless lost cause worrying things racism, war Iraq, pressuring kids succeed, technology, elections, inflation, worrying next end streets. <br /><br />But given bet live streets month without luxuries home, entertainment sets, bathroom, pictures wall, computer, everything treasure see like homeless? That Goddard Bolt's lesson. <br /><br />Mel Brooks( who directs) stars Bolt plays rich man everything world deciding make bet sissy rival( Jeffery Tambor) see live streets thirty days without luxuries; Bolt succeeds, wants future project making buildings. The bet's Bolt thrown street bracelet leg monitor every move cannot step sidewalk. given nickname Pepto vagrant written forehead Bolt meets characters including woman name Molly( Lesley Ann Warren) ex-dancer got divorce losing home, pals Sailor( Howard Morris) Fumes( Teddy Wilson) already used streets. survivors. Bolt not. used reaching mutual agreements like rich fight flight, kill killed. <br /><br />While love connection Molly Bolt necessary plot, I found \"Life Stinks\" one Mel Brooks' observant films prior comedy, shows tender side compared slapstick work Blazing Saddles, Young Frankenstein, Spaceballs matter, show like something valuable losing next day hand making stupid bet like rich people know money. Maybe give homeless instead using like Monopoly money. <br /><br />Or maybe film inspire help others.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 3:\n",
            "\n",
            "SLASHERS( 2 outta 5 stars) <br /><br />Not really good movie...I like idea behind it...filmmakers make look pretty good considering tiny budget work with. The movie ostensibly \"episode\" live Japanese reality show sends several contestants sealed \"danger zone\" three costumed creeps sent kill them. The survivor, one, wins fame fortune...everyone else winds dead. The main drawback movie acting pretty bad. None \"real\" people seem real all. The actors playing killers kind fun...portraying cheesy over-the-top caricatures popular modern horror movie types...exactly would done actual show. The movie pretends done one take...one cameraman follows contestants around \"danger zone\" everything seen point view camera...lights keep flickering constantly( to hide \"cuts\" one take another, I would imagine) .\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 4:\n",
            "\n",
            "Here is. . first EVER episode Friends, Where get introduced Control Freak Monica Gellar( Courtney Cox) , Newly divorced Ross Gellar( David Schimmer) , Hippy Pheobe Buffay( Lisa Kudrow) , unknown actor ladies man( Matt Le Blanc sarcastic Chandler Bing( Matthew Perry) . This scene starts introduced 6th final friend Spoilt kid Rachel Green( Jennifer Aniston) . <br /><br />The Episode better people give credit for, like new sitcom first episode always fantastic. The acting episode great cast cannot identify arnt really believable new characters( apart Kudrow Perry- shine) . <br /><br />Matt Le Blanc- Man, acting right dreadful later, gets confident, think tries funny fails. <br /><br />David Schimmer- Why pronounce EVERY word? cannot speak normally! became one funniest characters later seasons, confident. cannot sympathise Jennifer Aniston- Looks hot, good job Rachel Green, see real Rachel later 1st season, Courtney Cox- Looks quite anorexic episode, worrying, looks totally different now, ( more healthily) , acting little sketchy everyones 20 minute pilot! Lisa Kudrow Matthew Perry- I two together comic timing acting quality superb, Lisa one first roles natural Pheobe( Pheebs) Matthew Perry Matthew Perry playing basically! The episode quality improve later, , , Sets, looks dark creepy episode makes seem unfriendly, acting OK, characters gain confidence new scene proud pilot! I hope see Friends reunite! cause always us!\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 5:\n",
            "\n",
            "Roommates Sugar Bobby Lee abducted menacing dudes shopping one day taken back secluded island girls reluctantly tell thugs last visited ten years age fortune located on. All pretty much bookends movie pretty much one long flashback girls first visit island subsequent fight cannibalistic family. <br /><br />This one extremely horribly acted everyone involved point I started feeling bad poor Hank Worden truly deserved much MUCH better. As much I like \"Barracuda\"( that DVD) I admit film makes one look like Citizen Kane. <br /><br />Eye Candy: one pair tits( they might belong Kirsten Baker) <br /><br />My Grade: F <br /><br />Dark Sky DVD Extras: Vintage ads various drive-in food; Trailers \"Bonnie's Kids\"( features nudity) , \"the Centerfold Girls\", \"Part-time Wife\"( features nudity) , \"Psychic Killer\", & \"Eaten Alive\". The DVD also comes 1978's \"Barracuda\"\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = load_text(text)              # Load text from file path\n",
        "    ##################################################################################################################\n",
        "    ### Comment the following points until return out, to simulate the bare processing without normalization steps ###\n",
        "    ##################################################################################################################\n",
        "    text = expand_contractions(text)    # Expand contractions\n",
        "    text = remove_informal_tokens(text) # Replace informal tokens\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words) # Remove stopwords\n",
        "    text = fix_punctuation(text)        # Fix punctuation\n",
        "    text = preprocess_ellipses(text)    # Handle ellipses as a single token\n",
        "    return text\n",
        "\n",
        "\n",
        "# Apply preprocessing function to training and test data\n",
        "data_train['cleaned_text'] = data_train['path'].apply(preprocess_text)\n",
        "data_test['cleaned_text'] = data_test['path'].apply(preprocess_text)\n",
        "\n",
        "# Number of samples to be printed\n",
        "n_samples = 5\n",
        "\n",
        "# Randomly sample indices from \"data_train\"\n",
        "sample_indices = random.sample(range(len(data_train)), n_samples)\n",
        "\n",
        "# Print each sample from the cleaned_text\n",
        "for i, idx in enumerate(sample_indices, start=1):\n",
        "    # Access the cleaned text directly\n",
        "    text = data_train.loc[idx, 'cleaned_text']\n",
        "    print(f\"Sample {i}:\\n\")\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHLiIMDifQTI"
      },
      "source": [
        "### A simple pipeline:\n",
        "[Do not change it!]\n",
        "\n",
        "This simple pipeline will be used to compare the newly created pipeline against, to evaluate the performance increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOYzJ1-SfQTI"
      },
      "source": [
        "**White-Space tokenization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GJjcIUkHfQTI"
      },
      "outputs": [],
      "source": [
        "def tokenize(text:str):\n",
        "    ''' An example tokenization function. '''\n",
        "\n",
        "    # simple white-space tokenization:\n",
        "    return text.lower().split()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GddqcHfvfQTJ"
      },
      "source": [
        "**Bag-of-words Embedding:**\n",
        "\n",
        "See documentation of [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SGnHx6ThfQTJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create a simple bag of words embedding:\n",
        "bow = CountVectorizer(\n",
        "\n",
        "    # the next line converts the filepaths to the actual texts:\n",
        "    #preprocessor = load_text, # Commented, to check if the preprocessing from above changes the results\n",
        "\n",
        "    # tokenization function from above:\n",
        "    tokenizer = tokenize,\n",
        "\n",
        "    # Set token_pattern to None since we're using a custom tokenizer\n",
        "    token_pattern=None\n",
        "\n",
        ")\n",
        "\n",
        "# Train the embedding on cleaned training data\n",
        "embeddings_train = bow.fit_transform(data_train['cleaned_text'].values)\n",
        "\n",
        "# Vectorize the cleaned test data\n",
        "embeddings_test = bow.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "# These are the original lines\n",
        "# train the embedding:\n",
        "#embeddings_train = bow.fit_transform(data_train['path'].values)\n",
        "\n",
        "# vectorize test data:\n",
        "#embeddings_test = bow.transform(data_test['path'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qbhJp3tfQTJ"
      },
      "source": [
        "**Classification with a linear SVM**\n",
        "\n",
        "See documentation of [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUb6hTYVfQTJ",
        "outputId": "141d9bb7-db67-48e7-aeb1-ed8d4e82dd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.789\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# train classifier:\n",
        "svm.fit(embeddings_train, data_train['label'].values)\n",
        "\n",
        "# test classifier:\n",
        "predictions = svm.predict(embeddings_test)\n",
        "\n",
        "# Calculate Accuracy:\n",
        "print('Accuracy:', accuracy_score(data_test['label'].values, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7n0tlJZug-j"
      },
      "source": [
        "### Own text mining pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9AfjxXX1AQf"
      },
      "source": [
        "Tokenize a few sample texts with our chosen tokenizer BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2LtJwsVeG6",
        "outputId": "9aab73cf-43b6-4f89-df74-98296e8a2179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original string: Bromwell High cartoon comedy. It ran time programs school life, \"Teachers\". My 35 years teaching profession lead believe Bromwell High's satire much closer reality \"Teachers\". The scramble survive financially, insightful students see right pathetic teachers' pomp, pettiness whole situation, remind schools I knew students. When I saw episode student repeatedly tried burn school, I immediately recalled...High. A classic line: INSPECTOR: I sack one teachers. STUDENT: Welcome Bromwell High. I expect many adults age think Bromwell High far fetched. What pity not!\n",
            "Encoded tokens: [9292, 2821, 2991, 842, 18, 281, 3024, 276, 10904, 1697, 433, 16, 6, 13968, 754, 926, 6438, 511, 14379, 6966, 693, 778, 9292, 2821, 11, 84, 8971, 347, 7116, 2558, 6, 13968, 754, 162, 26502, 4999, 11248, 16, 9178, 4921, 269, 534, 2502, 9169, 11, 8118, 16, 21478, 845, 1732, 16, 1785, 13923, 45, 2660, 4921, 18, 915, 45, 641, 702, 4311, 6366, 2097, 2256, 1697, 16, 45, 3888, 12500, 295, 2821, 18, 37, 1360, 680, 30, 26735, 30, 45, 12370, 170, 9169, 18, 15387, 30, 15575, 9292, 2821, 18, 45, 587, 419, 2928, 350, 324, 9292, 2821, 692, 9393, 18, 639, 5446, 342, 5]\n",
            "Decoded string: Bromwell High cartoon comedy . It ran time programs school life , \" Teachers \". My 35 years teaching profession lead believe Bromwell High ' s satire much closer reality \" Teachers \". The scramble survive financially , insightful students see right pathetic teachers ' pomp , pettiness whole situation , remind schools I knew students . When I saw episode student repeatedly tried burn school , I immediately recalled ... High . A classic line : INSPECTOR : I sack one teachers . STUDENT : Welcome Bromwell High . I expect many adults age think Bromwell High far fetched . What pity not !\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Add a pre-tokenizer to handle whitespace properly\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize a BPE trainer with some default parameters\n",
        "trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"__ellipsis__\"])\n",
        "\n",
        "# Convert cleaned_text column to a list of strings for training\n",
        "training_data = data_train['cleaned_text'].tolist()\n",
        "\n",
        "# Train the tokenizer on the cleaned text data\n",
        "tokenizer.train_from_iterator(training_data, trainer)\n",
        "\n",
        "# Save the tokenizer for later use\n",
        "tokenizer.save(\"imdb_bpe_tokenizer.json\")\n",
        "\n",
        "# Encode the text data for the model\n",
        "encoded_train = [tokenizer.encode(text).ids for text in data_train['cleaned_text']]\n",
        "encoded_test = [tokenizer.encode(text).ids for text in data_test['cleaned_text']]\n",
        "\n",
        "# Select a single example to decode and print\n",
        "sample_index = 0  # Choose an index, e.g., 0 for the first item\n",
        "encoded_sample = encoded_train[sample_index]\n",
        "\n",
        "# Decode the tokenized output back to text\n",
        "decoded_text = tokenizer.decode(encoded_sample)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original string:\", data_train['cleaned_text'][sample_index])  # Original text for comparison\n",
        "print(\"Encoded tokens:\", encoded_sample)\n",
        "print(\"Decoded string:\", decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tokenization was now performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Training Data Shape: (1000, 15000)\n",
            "TF-IDF Test Data Shape: (1000, 15000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer with desired parameters\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=15000,       # You can adjust based on memory and data size\n",
        "    ngram_range=(1, 2),      # Consider unigrams and bigrams for richer context\n",
        ")\n",
        "\n",
        "# Fit on training data and transform both training and test data\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(data_train['cleaned_text'])\n",
        "tfidf_test = tfidf_vectorizer.transform(data_test['cleaned_text'])\n",
        "\n",
        "# Print the shapes of the resulting TF-IDF matrices\n",
        "print(\"TF-IDF Training Data Shape:\", tfidf_train.shape)\n",
        "print(\"TF-IDF Test Data Shape:\", tfidf_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.818\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.81      0.83      0.82       500\n",
            "         pos       0.83      0.80      0.82       500\n",
            "\n",
            "    accuracy                           0.82      1000\n",
            "   macro avg       0.82      0.82      0.82      1000\n",
            "weighted avg       0.82      0.82      0.82      1000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\kentf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize and train the SVM classifier\n",
        "svm = LinearSVC()\n",
        "svm.fit(tfidf_train, data_train['label'])  # 'label' is your target variable\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = svm.predict(tfidf_test)\n",
        "\n",
        "# Evaluate the classifier's performance\n",
        "accuracy = accuracy_score(data_test['label'], predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(data_test['label'], predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cpu\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)          # To confirm the version\n",
        "print(torch.cuda.is_available())  # This should return False as there is no GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map the labels to integers\n",
        "label_mapping = {'pos': 1, 'neg': 0}\n",
        "data_train['label'] = data_train['label'].map(label_mapping)\n",
        "data_test['label'] = data_test['label'].map(label_mapping)\n",
        "\n",
        "# Now convert to tensor\n",
        "y_train_tensor = torch.tensor(data_train['label'].values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(data_test['label'].values, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.6641243491321802\n",
            "Epoch 2/10, Loss: 0.24066265975125134\n",
            "Epoch 3/10, Loss: 0.008255663749878295\n",
            "Epoch 4/10, Loss: 0.0013952378340036375\n",
            "Epoch 5/10, Loss: 0.0006086299872549716\n",
            "Epoch 6/10, Loss: 0.0003767254838749068\n",
            "Epoch 7/10, Loss: 0.00019512378185027046\n",
            "Epoch 8/10, Loss: 0.00012972614911177516\n",
            "Epoch 9/10, Loss: 8.687810452556732e-05\n",
            "Epoch 10/10, Loss: 5.518287719041837e-05\n",
            "Test Accuracy: 0.816\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82       500\n",
            "           1       0.83      0.80      0.81       500\n",
            "\n",
            "    accuracy                           0.82      1000\n",
            "   macro avg       0.82      0.82      0.82      1000\n",
            "weighted avg       0.82      0.82      0.82      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert to dense tensors\n",
        "X_train_tensor = torch.tensor(tfidf_train.toarray(), dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(tfidf_test.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Labels as tensors\n",
        "y_train_tensor = torch.tensor(data_train['label'].values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(data_test['label'].values, dtype=torch.long)\n",
        "\n",
        "# Step 2: Define the Neural Network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)  # 2 classes: positive/negative\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = SimpleNN(input_size)\n",
        "\n",
        "# Step 3: Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 4: DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Step 5: Training Loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "# Step 6: Evaluation on Test Data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(test_outputs, 1)\n",
        "    accuracy = accuracy_score(y_test_tensor, predicted)\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test_tensor, predicted))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDx9UUI51Rqz"
      },
      "source": [
        "Further text normalization like stemming\n",
        "\n",
        "Kent: I dont think it is necessary and we should implement it into the preprocessing/normalization function at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz04Rt4eolPf",
        "outputId": "ced45d84-39ff-4d79-d8ba-40cf2bebb789"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "C:\\Users\\kentf\\AppData\\Local\\Temp\\ipykernel_10600\\946938710.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  \"\"\" import nltk\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' import nltk\\nimport re\\nfrom tokenizers import Tokenizer\\nfrom tokenizers.models import BPE\\nfrom tokenizers.trainers import BpeTrainer\\nfrom tokenizers.pre_tokenizers import Whitespace\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.stem import PorterStemmer\\n\\n# Download the \\'punkt\\' resource for tokenization\\nnltk.download(\\'punkt\\')\\n\\n# Function when further normalization is needed\\ndef further_normalization_needed(training_data):\\n    stemmer = PorterStemmer()\\n\\n    for i, text in enumerate(training_data):\\n        print(f\"\\nOriginal Text {i+1}: {text}\")\\n\\n        # Tokenize using word_tokenize for stemming analysis\\n        tokens = word_tokenize(text)\\n        stemmed_tokens = [stemmer.stem(token) for token in tokens]\\n\\n        # Custom regex to remove unwanted characters (e.g., punctuation)\\n        cleaned_text = re.sub(r\\'[^\\\\w\\\\s]\\', \\'\\', text)\\n\\n        print(\"Stemmed Tokens:\", stemmed_tokens)\\n        print(\"Cleaned Text (after regex):\", cleaned_text)\\n\\n# Check if further text normalization is needed\\nfurther_normalization_needed(training_data) '"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" import nltk\n",
        "import re\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download the 'punkt' resource for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function when further normalization is needed\n",
        "def further_normalization_needed(training_data):\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    for i, text in enumerate(training_data):\n",
        "        print(f\"\\nOriginal Text {i+1}: {text}\")\n",
        "\n",
        "        # Tokenize using word_tokenize for stemming analysis\n",
        "        tokens = word_tokenize(text)\n",
        "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        # Custom regex to remove unwanted characters (e.g., punctuation)\n",
        "        cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "        print(\"Cleaned Text (after regex):\", cleaned_text)\n",
        "\n",
        "# Check if further text normalization is needed\n",
        "further_normalization_needed(training_data) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8AKEvI4CvuR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb8QQiBEJmya",
        "outputId": "b72f2397-072c-459f-cf0f-276081014c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bag of Words with BPE Tokenization:\n",
            "[[1 0 1 3 1 1 1 0 0 3 1 1 1 0 2 1 1 1 2 0 0 1 1 0 0 1 1 0 0 0 1]\n",
            " [2 1 1 0 1 0 1 1 1 1 0 2 0 2 1 1 2 0 0 2 2 2 1 1 1 0 2 1 1 1 0]]\n",
            "[[1 0 1 1 0 1 1 0 0 3 0 1 0 0 2 1 0 0 3 1 1 2 0 1 0 0 1 0 0 0 0]\n",
            " [2 0 0 0 1 0 1 1 1 4 1 0 2 0 0 0 4 1 0 0 1 1 1 0 0 1 3 2 1 1 1]]\n",
            "\n",
            "Bag of Words with BPE Tokenization and Normalization:\n",
            "[[2 0 1 3 1 0 1 0 3 1 0 1 0 3 1 1 1 2 0 0 1 1 0 0 1 1 0 0 0]\n",
            " [3 1 1 0 1 1 1 1 0 0 1 0 1 2 2 1 0 0 2 1 2 1 1 1 0 2 1 1 1]]\n",
            "[[2 0 1 1 0 0 1 0 2 0 0 0 0 3 1 1 0 3 1 1 2 0 1 0 0 1 0 0 0]\n",
            " [2 0 0 0 1 1 1 1 2 1 0 2 0 0 1 0 1 0 0 0 0 1 0 0 1 2 2 0 1]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\kentf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample data for demonstration\n",
        "data_train = pd.DataFrame({\n",
        "    'cleaned_text': [\n",
        "        \"Hello, this is a test sentence for the tokenizer.\",\n",
        "        \"This is another sentence to improve subword merging.\"\n",
        "   ],\n",
        "    'label': [0, 1]  # Add a 'label' column to the DataFrame\n",
        "})\n",
        "\n",
        "data_test = pd.DataFrame({\n",
        "    'cleaned_text': [\n",
        "        \"Hello, this is another example sentence.\",\n",
        "        \"Subword tokenization is quite effective.\"\n",
        "    ],\n",
        "    'label': [0, 1]  # Add a 'label' column to the DataFrame\n",
        "})\n",
        "\n",
        "\n",
        "# 1. Improved Tokenization (BPE)\n",
        "# Initialize BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "trainer = BpeTrainer(vocab_size=10000, min_frequency=2)\n",
        "tokenizer.train_from_iterator(data_train['cleaned_text'], trainer=trainer)\n",
        "\n",
        "# Rename the function to avoid conflict with the 'tokens' variable\n",
        "def bpe_tokenize_func(text):\n",
        "    return tokenizer.encode(text).tokens\n",
        "\n",
        "# 2. Improved Tokenization with Normalization (BPE)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Stem each word\n",
        "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "def bpe_tokenize_normalized(text):\n",
        "    normalized_text = normalize_text(text)\n",
        "    return tokenizer.encode(normalized_text).tokens\n",
        "\n",
        "# Create CountVectorizer for each tokenization method\n",
        "\n",
        "# Bag of Words with BPE Tokenization\n",
        "# Use the renamed function here\n",
        "bow_bpe = CountVectorizer(tokenizer=bpe_tokenize_func, token_pattern=None)\n",
        "embeddings_train_bpe = bow_bpe.fit_transform(data_train['cleaned_text'].values)\n",
        "embeddings_test_bpe = bow_bpe.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "# Bag of Words with BPE Tokenization and Normalization\n",
        "bow_bpe_norm = CountVectorizer(tokenizer=bpe_tokenize_normalized, token_pattern=None)\n",
        "embeddings_train_bpe_norm = bow_bpe_norm.fit_transform(data_train['cleaned_text'].values)\n",
        "embeddings_test_bpe_norm = bow_bpe_norm.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "\n",
        "print(\"\\nBag of Words with BPE Tokenization:\")\n",
        "print(embeddings_train_bpe.toarray())\n",
        "print(embeddings_test_bpe.toarray())\n",
        "\n",
        "print(\"\\nBag of Words with BPE Tokenization and Normalization:\")\n",
        "print(embeddings_train_bpe_norm.toarray())\n",
        "print(embeddings_test_bpe_norm.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "988exFnFZ4dP",
        "outputId": "ee676a6c-012e-4081-e96d-956fbee5cdf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# train classifier:\n",
        "svm.fit(embeddings_train_bpe, data_train['label'].values)\n",
        "\n",
        "\n",
        "# test classifier:\n",
        "predictions = svm.predict(embeddings_test_bpe)\n",
        "\n",
        "\n",
        "# Calculate Accuracy:\n",
        "print('Accuracy:', accuracy_score(data_test['label'].values, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1WMtvEpdCPf",
        "outputId": "c3bb6191-6759-4064-a52e-5bd92b3eff9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with BPE Tokenization and Normalization: 0.5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "# Train and evaluate classifier using BPE embeddings\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# Train classifier\n",
        "#svm.fit(embeddings_train, data_train['label'].values)\n",
        "#svm.fit(embeddings_train_bpe, data_train['label'].values)\n",
        "svm.fit(embeddings_train_bpe_norm, data_train['label'].values) # The model is trained on embeddings_train_bpe_norm\n",
        "\n",
        "\n",
        "# Test classifier\n",
        "#predictions = svm.predict(embeddings_test_bpe) # This line causes the error because embeddings_test_bpe has a different number of features\n",
        "predictions = svm.predict(embeddings_test_bpe_norm) # Use embeddings_test_bpe_norm for prediction, which has the same number of features as the training data\n",
        "\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(data_test['label'].values, predictions)\n",
        "print('Accuracy with BPE Tokenization and Normalization:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO0d9PMR2CZr"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
