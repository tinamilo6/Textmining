{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinamilo6/Textmining/blob/main/A03_TextMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "X1D-_ww7fQTB"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas sklearn nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS1_jLQofQTD"
      },
      "source": [
        "# Assignment 3 - Text Mining\n",
        "\n",
        "Project management and tools for health informatics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufi0zQidfQTE"
      },
      "source": [
        "## 1. Download and prepare data:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "The code in this section downloads the [IMDB IMDB Large Movie Review Dataset]('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz') which is the dataset you will be working on in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "aOzKCiw4fQTE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "Sq9rCBBrfQTE"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('aclImdb'):\n",
        "    # download data:\n",
        "    urlretrieve('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', 'aclImdb.tar.gz')\n",
        "\n",
        "    # unzip data:\n",
        "    with tarfile.open('aclImdb.tar.gz') as file:\n",
        "        file.extractall('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9pzG__QfQTF"
      },
      "source": [
        "## 2. Some helper Functions:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "This section contains the code for some helper functions that will be useful for solving the assignment. Example code on how to use the functions is provided in section 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "LaM3GoopfQTF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from typing import Literal, Tuple, Iterable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxsGe7MufQTF"
      },
      "source": [
        "Function for loading data into a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "tKoAn698fQTG"
      },
      "outputs": [],
      "source": [
        "def load_data(split:Literal['train', 'test'], texts_per_class:int=500) -> pd.DataFrame:\n",
        "    ''' Loads the data into a pandas dataframe.'''\n",
        "    paths  = []\n",
        "    labels = []\n",
        "\n",
        "    for label in ('pos', 'neg'):\n",
        "        # get all files in the folder:\n",
        "        files = os.listdir(os.path.join('aclImdb', split, label))[:texts_per_class]\n",
        "\n",
        "        # append them to the lists:\n",
        "        paths.extend([os.path.join('aclImdb', split, label, f) for f in files])\n",
        "        labels.extend([label] * len(files))\n",
        "\n",
        "    return pd.DataFrame({'path':paths, 'label':labels})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C1jyAtfQTG"
      },
      "source": [
        "Function for loading a specific text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "y0QGuZMXfQTH"
      },
      "outputs": [],
      "source": [
        "def load_text(path:str) -> str:\n",
        "    ''' Reads a single text given the path. '''\n",
        "    # read file from disk:\n",
        "    with open(path, 'r', encoding='utf8') as file:\n",
        "        s = file.read()\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR52KxrvfQTH"
      },
      "source": [
        "Function for iterating through multiple texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "NImrB2NzfQTH"
      },
      "outputs": [],
      "source": [
        "def iterate_texts(data:pd.DataFrame) -> Iterable[Tuple[str, str]]:\n",
        "    ''' Iterates through a pandas dataframe. '''\n",
        "\n",
        "    for path in data['path'].values:\n",
        "        # read file from disk:\n",
        "        with open(path, 'r', encoding='utf8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        yield text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzYgen2AfQTH"
      },
      "source": [
        "## 3. Text Mining Pipeline\n",
        "\n",
        "This section will cover the text mining steps for this assignment. The following steps will be performed:\n",
        "\n",
        "1. **Analyze the Data for Difficult Parts**  \n",
        "   - Review the data to identify challenging aspects such as contractions, informal language, and complex sentence structures.\n",
        "\n",
        "2. **Replace Contractions and Informal Language**  \n",
        "   - Expand common contractions and replace informal phrases, if necessary, to standardize the text for processing.\n",
        "\n",
        "3. **Tokenize the Texts**  \n",
        "   - Apply a suitable tokenizer to break down the text into individual tokens for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2o7r5Bug-U"
      },
      "source": [
        "### Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "57LKdqhTug-V"
      },
      "outputs": [],
      "source": [
        "# Import needed libaries for the preparation of the texts\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3FnCEu6fQTH"
      },
      "source": [
        "### Load the training data\n",
        "The loaded data frpom the zip file should be saved into a `data_train` and `data_test` DataFrame.\n",
        "These can be further on be used to access the train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CeshtSl9fQTI",
        "outputId": "fa88fb56-74c9-47ca-d8fa-c3b9b6e19fe5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aclImdb\\train\\pos\\0_9.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aclImdb\\train\\pos\\10000_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aclImdb\\train\\pos\\10001_10.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aclImdb\\train\\pos\\10002_7.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aclImdb\\train\\pos\\10003_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>aclImdb\\train\\neg\\10446_2.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>aclImdb\\train\\neg\\10447_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>aclImdb\\train\\neg\\10448_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>aclImdb\\train\\neg\\10449_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>aclImdb\\train\\neg\\1044_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               path label\n",
              "0         aclImdb\\train\\pos\\0_9.txt   pos\n",
              "1     aclImdb\\train\\pos\\10000_8.txt   pos\n",
              "2    aclImdb\\train\\pos\\10001_10.txt   pos\n",
              "3     aclImdb\\train\\pos\\10002_7.txt   pos\n",
              "4     aclImdb\\train\\pos\\10003_8.txt   pos\n",
              "..                              ...   ...\n",
              "995   aclImdb\\train\\neg\\10446_2.txt   neg\n",
              "996   aclImdb\\train\\neg\\10447_1.txt   neg\n",
              "997   aclImdb\\train\\neg\\10448_1.txt   neg\n",
              "998   aclImdb\\train\\neg\\10449_4.txt   neg\n",
              "999    aclImdb\\train\\neg\\1044_4.txt   neg\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 300,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train = load_data('train')\n",
        "data_test  = load_data('test')\n",
        "data_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qXdudbDfQTI"
      },
      "source": [
        "### 1. Assess the data / texts for difficult parts\n",
        "In this part, sample texts are printed and then analyzed for diffult parts, that could affect the text mining process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soE8Qj9hsF0E",
        "outputId": "9f3f109e-dedd-4ec9-a9ce-6fb530a35fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1:\n",
            "\n",
            "Warning: This review contains minor spoilers.<br /><br />Well the writers of the first Tremors are officially out of ideas. I'm a big fan of the first movie and the first two sequels are pretty good for straight to video fare. Tremors 4: The Legends Begins, however, is a very dull movie. Where the heck are the Graboids??? <br /><br />Due to the relative lack of Graboids through the first 90 minutes I'm convinced that this entry into the series is suppose to be a \"character study\". Unfortunately there isn't one interesting character in the movie except for Billy Drago's character who is given too few lines, too little to do and in the end too little screen time. What saved the 2nd and 3rd movies was the presence of Michael Gross as Burt Gummer. Whenever there wasn't any action on the screen you could rest assured that Burt Gummer was going to be interesting to listen too and/or watch. However in this movie Gross plays Hiram Gummer a very poor and boring substitute. <br /><br />On the plus side when the Graboids (Dirt Dragons in this movie) are on the screen they do look good but that is about as good as it gets.<br /><br />I was impressed when I saw that Tremors 4 was listed at 101 minutes long. Pretty good for straight to video. But after watching it I'm sure that this movie is a good 15 minutes too long. There are long stretches of dialogue that is boring and doesn't further the plot any. Was there a rush to get this movie made? I think not, more time could have and should have been spent on the script.<br /><br />I thought I had hit a gold mine when I saw Tremors 4 packaged for sale with....Tremors!!! What luck I thought, pay for #4 get #1 for free. Well after watching Tremors 4 I like to think I paid for the original and got this mess for free, I can't imagine paying a single dime for Tremors 4. For fans of the series it's best to forget that Tremors 4: The Legend Begins even exists.<br /><br />Tremors 4: The Legend Begins rates a 3 out of 10.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 2:\n",
            "\n",
            "This movie is a joke. I mean a \"ha ha\" funny joke. Why? Because the only redeeming thing about it was the good laugh I got at the sheer ridiculousness of nonsensical, inane plot and horrible acting. Wow!<br /><br />Within this movie there are so many unanswered questions... for example; why do these women become zombies and how? Why are there four black women who are zombie's \"caretakers\" and what is their purpose? Since when does 6 people make up a \"nation\" of Zombies? And is smeared black eye mascara \"scary\" to anyone, anywhere? Even a 2 year old?<br /><br />And lastly; Why was this movie made at all? Why? why? why? No answer? That's what I thought.<br /><br />On the demand channel they actually issued this comment after the synopsis of the movie: We apologize for this movie in advance\" LOL. At least they had the decency to do this much!\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 3:\n",
            "\n",
            "From director Barbet Schroder (Reversal of Fortune), I think I saw a bit of this in my Media Studies class, and I recognised the leading actress, so I tried it, despite the rating by the critics. Basically cool kid Richard Haywood (Half Nelson's Ryan Gosling) and Justin Pendleton (Bully's Michael Pitt) team up to murder a random girl to challenge themselves and see if they can get away with it without the police finding them. Investigating the murder is homicide detective Cassie 'The Hyena' Mayweather (Sandra Bullock) with new partner Sam Kennedy (Ben Chaplin), who are pretty baffled by the evidence found on the scene, e.g. non-relating hairs. The plan doesn't seem to be completely going well because Cassie and Sam do quite quickly have Richard or Justin as suspects, it is just a question of if they can sway them away. Also starring Agnes Bruckner as Lisa Mills, Chris Penn as Ray Feathers, R.D. Call as Captain Rod Cody and Tom Verica as Asst. D.A. Al Swanson. I can see now the same concept as Sir Alfred Hitchcock's Rope with the murdering for a challenge thing, but this film does it in a very silly way, and not even a reasonably good Bullock can save it from being dull and predictable. Adequate!\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 4:\n",
            "\n",
            "Once again Canadian TV outdoes itself and creates another show that will go unwatched after its premiere episode. <br /><br />Last time I remember sitcoms were supposed to induce a reaction we in the business call laughter. How funny is it to beat the stereotype of all white people thinking that all Muslims are terrorists? OK maybe one joke just to stick it to the masses. But not 30 minutes. It's called beating a dead horse. Even SNL would know to give up after a commercial break.<br /><br />Also, let's have a little conflict in these scripts. Will she or won't she be able to serve cucumber sandwiches to break the fast on Ramadan? When will Ramadan start? Ohhhhh this is Emmy winning stuff here. <br /><br />And the characters! What characters?! They are all cardboard cut-outs without anything interesting to make us want to follow them from one situation to the next. That's the point of the situation comedy. We need to have strong, interesting, dynamic characters so that we are constantly drawn to the TV set each week. We have to care about these characters to worry about what trouble they're going to get into next week. If I never see these characters it'll be too soon. Thankfully I can't remember any of their names (note to CBC - that's not a good sign).<br /><br />And the acting is so bland. It's more so a problem in casting than in the actors. None of these people actually embody the characters they play. They just seem to act their part as though they were working on a movie of the week. Sitcoms require actors who live and breathe that character - make us fall in love with them - where they become inseparable from the character the portray. Watch any American sitcom and you'll see how easily identifiable characters are. Part of the problem is that the actors seem to treat this project as though it might be a platform to bigger and better things instead of being their one big character of a lifetime for whom they will spend the next 8 years portraying. That level of disinterest in the characters and the project shows. But to be honest, considering the lame concept and the horrible writing, there's not much for the actors to do but say their lines and try not to bump into any furniture. As another commenter mentions, this seems like a TV movie and not a sitcom.<br /><br />And the directing or lack there of! What can I say, Canada has so much talent, look at what the Comedy Channel is doing with Puppets Who Kill and Punched Up. Look at the Trailer Park Boys (not the movie cause it bit the big helium dog). Look at any American show to see the potentials our talent as that's where many of our stars go to find decent work.<br /><br />Give credit to the CBC, they really know how to build publicity for a non-event. Remember \"The One\"? No - well don't even try to learn any characters names in this show, as it's sure to go the way of the dodo.<br /><br />Let's all hope for a full blown ACTRA strike so that nothing like this emerges from the Ceeb for a good long while.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 5:\n",
            "\n",
            "James Cameron's 'Titanic' is essentially a romantic adventure with visual grandeur and magnificence, a timeless tragic love story set against the background of this major historical event... It's an astonishing movie that exemplifies hope, love and humanity... <br /><br />Leonardo DiCaprio is terrific on screen with big charisma... Conveying passion, trust, insouciance and ingenuity, he's a free-spirited wanderer with artistic pretensions, and a zest for life... <br /><br />Kate Winslet is absolutely lovely as the confused upper-class teen engaged to a nasty rich guy who finds herself, one night, plunged to the depths of despair...<br /><br />Billy Zane is an arrogant racist, abusive and ultra rich who would lie, cheat, steal, bribe with money or even use an innocent young child to escape defeat... He keeps a 56 carat blue diamond worn by Louis XVI...<br /><br />Kathy Bates is the legendary unsinkable Molly Brown, the richest woman in Denver, who is a lot less uptight than the other rich folk on the ship...<br /><br />Frances Fisheris is the impecunious cold snobbish mother who, deathly afraid of losing her social stature, forces her daughter to become engaged to marry a rich, supercilious snob...<br /><br />Victor Garber is the master shipbuilder, the real-life character who attempts to fix time, to measure it, in a sense, to make it into history... <br /><br />Jonathan Hyde is the White Star Chairman who wants the Titanic to break the Trans-Atlantic speed record, in spite of warnings that icebergs may have floated into the hazardous northern crossing...<br /><br />Bill Paxton is the opportunistic undersea explorer in search for a very rare diamond called the \"Heart of the Ocean.\" <br /><br />Gloria Stuart is the 101-year old woman who reveals a never-before told love story... The nightmare, the horror and the shock are imprinted upon her deeply lined face... <br /><br />'Titanic' is loaded with luminous photography and sweeping visuals as the footage of the shipwrecked Ocean liner lying motionless on the ocean floor; the incredible transformation of the bow of the sunken 'Titanic' that takes the viewer back to 1912, revealing the meticulously re-created interiors; the first sight of the Titanic steamed steadily toward her date with destiny; the Titanic, leaving the Southampton dock, and some dolphins appear jumping, racing along in front of the luxurious ship; DeCaprio and Winslet flying at the ship's front rail in a gorgeous magic moment; the intertwining of past and present as Jack was drawing Rose on his paper, the camera zooms closely on young Rose's eye, only to transform its shape into Gloria Stuart's aged eye...<br /><br />Chilling scenes: Titanic's inevitable collision with destiny; James Cameronin one of the most terrifying sequences ever put on film takes us down with the Titanic, finally leaving us floundering in the icy water, screaming for help that never comes...<br /><br />Winner of 11 Academy Awards, including Best Picture, James Cameron's \"Titanic\" is a gigantic epic where you don't just watch the film, you experience it! The visual effects are amazing, like no other film's... The decor is overwhelming... James Horner's music intensifies the emotions... The whole movie is hunting and involving, filled with a wide range of deep feelings... <br /><br />It's truly a moving tribute to those who lost their lives on that unfortunate ship...\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 6:\n",
            "\n",
            "\"House of Games\" is a flawlessly constructed film, and one of the few films I have seen that had me gaping at the screen in astonishment at how cleverly and unexpectedly it ends. I first saw it on video a few years back after reading Roger Ebert's review, which proclaimed it the best film of 1987. I had my doubts, mainly because it is not quite as well known as other films from that year. Boy, was I in for a surprise. This was one of the smartest, most well-written movies I had ever seen.<br /><br />The screenplay is quite a piece of work, not only in terms of the plot (which twists and turns and pulls the rug out from under you just when you think you have it all figured out), but also in terms of character development. On my second viewing, I began to realize that Mamet's screenplay succeeds not only as a clever suspense film, but that each plot development contributes to our understanding of the characters and their motivations. The climax of the movie is particularly effective, because it is absolutely inevitable. It stems naturally from what we know about the characters, and it is therefore much more than just an arbitrary twist ending. The performances by Lindsay Crouse and Joe Mantegna also add enormously to the film. I cannot picture any other actor besides Mantegna playing the role of Mike, and Crouse plays her role with just the right amount of restraint to suggest a repressed criminal mindset. Their work, plus Mamet's extraordinary screenplay, combine to create one of the greatest films of the 1980's. It is truly a must-see.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 7:\n",
            "\n",
            "Being a giant monster fan, me seeing \"Yeti\" was an absolute must, especially after hearing so much about it. Thanks to the good 'ol bootleg market I was able to find a copy pretty easily, and was happily surprised upon watching that this flick was actually, dare I say, decent.<br /><br />Decent for what it is, actually, namely a cheesy giant-monster flick. It kicks in pretty quickly as Yeti is found pretty much immediately, and we get introduced to various characters. They consist of some sleazy ones, some good ones, and a girl who is pretty much one of the most downright strikingly beautiful girls in any cheesy sci-fi film, by far.<br /><br />Yeti looks like a long-haired guy straight out of the original Woodstock concert, and really, he's not that bad of a dude, especially after being introduced to the world in some kind of funky cage-like thing. Godzilla he is not - despite his rude awakening, he doesn't even rampage (actually he rarely destroys anything in the whole picture), but kinda just looks puzzled while trying to figure things out. Yeti seems to understand English pretty nicely (my copy was dubbed in English) and he knows who the good guys and bad guys are.<br /><br />However, we want to see the giant Yeti do his thing, and he's pretty much in the whole movie, and in typical low-budget fashion, he seems to change size a lot depending on the scene and there's even a bunch of the \"fake legs\" shots of him just standing there.<br /><br />Yes, the special effects aren't the greatest, but there are definitely some good ones here. A scene where Yeti smashes through a warehouse is done very well, and in another, he uses the windows of a building as \"ladder steps\" to climb down from the top of it - shattering each window with his foot and often shocking the occupants inside - in one sequence that really looks much, much better than it should in such a \"bad\" movie.<br /><br />\"Yeti\" never stoops as low as say, \"A.P.E.\" does. Actually the only time it even comes close to genuine silliness is when the beautiful girl causes Yeti's nipple to become erect and he lifts his eyebrow in an \"oh yeah baby\" manner. But even this isn't that bad, and kinda even gets a laugh out of the viewer.<br /><br />The movie is pretty long for this kind of thing, but surprisingly enough it doesn't get boring - the story is actually good, and just watching this utterly gorgeous actress on screen will make any male viewer happy.<br /><br />\"Yeti\" may not be in the upper echelon of giant monster flicks, but it is definitely better than other King Kong '76 rip-offs like \"A.P.E.\" and \"Queen Kong\" by very far.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Number of samples to be printed\n",
        "n_samples = 7\n",
        "\n",
        "# Randomly sample indices from \"data_train\"\n",
        "sample_indices = random.sample(range(len(data_train)), n_samples)\n",
        "\n",
        "# Load and print each sample using the \"load_text\" function\n",
        "for i, id in enumerate(sample_indices, start=1):\n",
        "    # Load text from the file path specified in 'path' column\n",
        "    text = load_text(data_train.loc[id, 'path'])\n",
        "    print(f\"Sample {i}:\\n\")\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWw3AZ8SumVn"
      },
      "source": [
        "## Limitations/Issues Captured from the Above Text Samples\n",
        "\n",
        "- **HTML tags and special characters**\n",
        "- **Punctuation and symbols** (e.g., `&`)\n",
        "- **Contractions** (e.g., \"isn't\", \"I'll\", \"I'm\")\n",
        "- **Parentheses and annotations** (e.g., \"(Crouching Tiger)\")\n",
        "- **Informal formatting** (e.g., \"my rating is ****\")\n",
        "- **Ambiguity and polysemy** (e.g., \"dictators\", \"nuts\")\n",
        "- **Long and complex sentences**\n",
        "- **Informal language** (e.g., \"what can be so bad about that?\")\n",
        "- **Quotation marks** (e.g., \"dictators\", \"sin\")\n",
        "\n",
        "We will elaborate on these issues and add our conclusions in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "666GKq5Cug-Z"
      },
      "source": [
        "### 2. Preprocessing: Simplify the Text\n",
        "\n",
        "This section focuses on preprocessing the texts to make them more suitable for text mining. Identified constraints from the analysis step will be addressed and, as far as possible, eliminated to improve processing accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "_Zgm5pNuGJNM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    # Regex to match HTML tags\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "id": "ZR8XyEHiGb94"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation_and_symbols(text):\n",
        "    # Remove punctuation and special symbols using regex\n",
        "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "eYMTaZhZHOlZ"
      },
      "outputs": [],
      "source": [
        "def remove_parentheses(text):\n",
        "    # Remove text inside parentheses along with parentheses\n",
        "    clean_text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9PoskZmug-b"
      },
      "source": [
        "#### Expand the contractions\n",
        "There are python libraries that focus on expanding contractions. For easier loading (if these packages are not available) we decided to create our own replacement list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "NaTBJ0qfG5uy"
      },
      "outputs": [],
      "source": [
        "# Dictionary of common contractions and their expanded forms\n",
        "contractions_dict = {\n",
        "    \"i'm\": \"I am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"i've\": \"I have\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"i'll\": \"I will\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"here's\": \"here is\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"shan't\": \"shall not\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b', flags=re.IGNORECASE)\n",
        "    def replace(match):\n",
        "        return contractions_dict.get(match.group(0).lower(), match.group(0))\n",
        "    return contractions_pattern.sub(replace, text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "zrIoCD2IHqLy"
      },
      "outputs": [],
      "source": [
        "# Define a set of informal tokens to remove\n",
        "informal_tokens = {\n",
        "    \"u\": \"you\",\n",
        "    \"r\": \"are\",\n",
        "    \"lmao\": \"\",  # Remove\n",
        "    \"lol\": \"\",   # Remove\n",
        "    \"btw\": \"by the way\",\n",
        "    \"idk\": \"I do not know\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"gonna\": \"going to\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"gotta\": \"got to\",\n",
        "    \"b/c\": \"because\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"pls\": \"please\",\n",
        "    \"cuz\": \"because\",\n",
        "    \"wut\": \"what\",\n",
        "    \"smh\": \"\",  # Remove\n",
        "    \"k\": \"okay\",\n",
        "    \"ttyl\": \"talk to you later\"\n",
        "}\n",
        "\n",
        "def remove_informal_tokens(text):\n",
        "    # Replace informal tokens\n",
        "    for token, replacement in informal_tokens.items():\n",
        "        # Use regex to match whole words and replace them\n",
        "        text = re.sub(r'\\b' + re.escape(token) + r'\\b', replacement, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def fix_punctuation(text):\n",
        "    # Ensure proper spacing after punctuation\n",
        "    text = re.sub(r'\\s*([.,;:!?()])\\s*', r'\\1 ', text)  # Ensure space after punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = text.strip()  # Trim leading and trailing spaces\n",
        "    return text\n",
        "\n",
        "# Combined function to clean the text\n",
        "def clean_text(text):\n",
        "    text = remove_informal_tokens(text)  # Remove informal tokens\n",
        "    text = fix_punctuation(text)  # Fix punctuation\n",
        "    return text\n",
        "\n",
        "def preprocess_ellipses(text):\n",
        "    # Replace occurrences of three or more dots with actual ellipses\n",
        "    text = re.sub(r'(\\.\\s*){3,}', '...', text)\n",
        "    return text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81A7y_1sug-c",
        "outputId": "34d3e3d4-c1a0-4a0b-b421-28cfe52439f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1:\n",
            "\n",
            "What a bad, bad film!!! I can't believe all the hype that has been lavished on this pretentious, amateurish excuse for a real movie!! I left the theater before the end, stunned by how bad the direction and camera-work of that movie were!! And to read adoring paeans that claim there is truth and reality in this film when all it is in reality is a brazen attempt at pulling the wool over the eyes of reviewers and festivals by being cheap and tawdry.<br /><br />At least this film showed me once and for all that the Sundance Festival has become a complete joke and that being shown here is more a label of bad film-making than anything else.<br /><br />Avoid at all costs. You'll want your time back! I know I did.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 2:\n",
            "\n",
            "I love the frequently misnomered \"Masters of Horror\" series. Horror fans live in a constant lack of nourishment. Projects like this (and the similar \"Greenlight Project\" with gave us \"Feast\" - like it or lump it) are breeding grounds for wonderful thought bubbles in the minds of directors with a horror bent to develop and bring to maturation food for we who love to dine on horror.<br /><br />This one began with a kernel of really-kool-idea and ran ... right off the edge of \"where in the world am I going with this?!!!\".<br /><br />I don't know how to spoil the spoiled but \"SPOILER AHEAD\" All of a sudden ... no, there was that light drifting across the night sky earlier ... we have long haired luminescent aliens (huh? ... HUH?) brain drilling males and ... yeah, I get it but ... well ... the worst curse of storytelling - a rousing and promising set up without a rewarding denouement.<br /><br />Cue to storytellers ... your build up has to have a payoff that exceeds build up. Not the other way around. Storytelling math 101.<br /><br />End of Spoilers - Big Oops!\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 3:\n",
            "\n",
            "There are many illnesses born in the mind of man which have been given life in modern times. Constant vigilance or accrued information in the realm of Pyschosis, have kept psychologists, counselors and psychiatrists busy with enough work to last them decades. Occasionally, some of these mental phenomenon are discover by those with no knowledge of their remedy or even of their existence. That is the premise of the film entitled \" The Night Listner.\" It tells the story of a popular radio host called Gabriel Noon (Robin Williams) who spends his evenings enthralling his audiences with vivid stories about Gay lifestyles. Perhaps its because his show is losing it's authentic veneer which causes Noon to admit he is no longer himself. Feeling abandoned by both his lover Jess (Bobby Cannavale) and his and best friend (Joe Morton), he seeks shelter in his deepening despair and isolation. It is here, a mysterious voice in the night asks him for help. Noon needs to feel useful and reaches out to the desperate voice which belongs to a 14 year old boy called Peter (Rory Culkin). In reading the boy's harrowing manuscript which depicts the early life and sexual abuse at the hands of his brutal parents, Noon is captivated and wants to help. However, things are not what they seem and Noon soon finds himself en-wrapped in an elusive and bizarre tale torn right out of a medical nightmare. This movie is pure Robin Williams and were it not for Toni Collette who plays Donna D. Logand, Sandra Oh as Anna and John Cullum as pop, this might be comical. Instead, this may prove to be one of William's more serious performances. ***\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 4:\n",
            "\n",
            "how can this movie have a 5.5 this movie was a piece of skunk s**t. first the actors were really bad i mean chainsaw Charlie was so retarded. because in the very beginning when he pokes his head into the wooden hut (that happened to be about oh 1 quarter of an inch thick (that really cheap as* flimsy piece of wood) and he did not even think he could cut threw it)second the person who did the set sucks as* at supplying things for them to build with. the only good thing about this movie is the idea of this t.v. show. bottom line DO NOT waste your hard earned cash on this hunk of s**t they call a movie.<br /><br />rating:0.3\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 5:\n",
            "\n",
            "Like the Arabian Nights this film plays with storytelling conventions in order to make us feel that there's plot, plot and more plot: it opens with what appears to be the frame device of a blind man telling the story of his life, then plunges into a flashback which takes us right up to the blind man's present, where we discover that about half of the story is yet to come. (It must be admitted that the second half doesn't quite live up to the promise of the first.) Like the Arabian Nights it tries to cram as many Middle-Eastern folk motiffs as possible into the one work. A freed genie, a beautiful princess, a flying carpet, fantastic mechanical toys, sea voyages, a crowded marketplace, a wicked vizier, jewels ... I don't know why it all works, but it does. Everything is just so beautiful. The sets are beautiful. June Duprez is beautiful. Rozsa's score is especially beautiful. As usual, it sounds Hungarian; but somehow he manages to convince us that he's being Hungarian in a Persian way.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kentf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = load_text(text)              # Load text from file path\n",
        "    ##################################################################################################################\n",
        "    ### Comment the following points until return out, to simulate the bare processing without normalization steps ###\n",
        "    ##################################################################################################################\n",
        "    #text = expand_contractions(text)    # Expand contractions\n",
        "    #text = remove_informal_tokens(text) # Replace informal tokens\n",
        "    #text = ' '.join(word for word in text.split() if word not in stop_words) # Remove stopwords\n",
        "    #text = fix_punctuation(text)        # Fix punctuation\n",
        "    #text = preprocess_ellipses(text)    # Handle ellipses as a single token\n",
        "    return text\n",
        "\n",
        "\n",
        "# Apply preprocessing function to training and test data\n",
        "data_train['cleaned_text'] = data_train['path'].apply(preprocess_text)\n",
        "data_test['cleaned_text'] = data_test['path'].apply(preprocess_text)\n",
        "\n",
        "# Number of samples to be printed\n",
        "n_samples = 5\n",
        "\n",
        "# Randomly sample indices from \"data_train\"\n",
        "sample_indices = random.sample(range(len(data_train)), n_samples)\n",
        "\n",
        "# Print each sample from the cleaned_text\n",
        "for i, idx in enumerate(sample_indices, start=1):\n",
        "    # Access the cleaned text directly\n",
        "    text = data_train.loc[idx, 'cleaned_text']\n",
        "    print(f\"Sample {i}:\\n\")\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHLiIMDifQTI"
      },
      "source": [
        "### A simple pipeline:\n",
        "[Do not change it!]\n",
        "\n",
        "This simple pipeline will be used to compare the newly created pipeline against, to evaluate the performance increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOYzJ1-SfQTI"
      },
      "source": [
        "**White-Space tokenization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "GJjcIUkHfQTI"
      },
      "outputs": [],
      "source": [
        "def tokenize(text:str):\n",
        "    ''' An example tokenization function. '''\n",
        "\n",
        "    # simple white-space tokenization:\n",
        "    return text.lower().split()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GddqcHfvfQTJ"
      },
      "source": [
        "**Bag-of-words Embedding:**\n",
        "\n",
        "See documentation of [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "SGnHx6ThfQTJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create a simple bag of words embedding:\n",
        "bow = CountVectorizer(\n",
        "\n",
        "    # the next line converts the filepaths to the actual texts:\n",
        "    #preprocessor = load_text, # Commented, to check if the preprocessing from above changes the results\n",
        "\n",
        "    # tokenization function from above:\n",
        "    tokenizer = tokenize,\n",
        "\n",
        "    # Set token_pattern to None since we're using a custom tokenizer\n",
        "    token_pattern=None\n",
        "\n",
        ")\n",
        "\n",
        "# Train the embedding on cleaned training data\n",
        "embeddings_train = bow.fit_transform(data_train['cleaned_text'].values)\n",
        "\n",
        "# Vectorize the cleaned test data\n",
        "embeddings_test = bow.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "# These are the original lines\n",
        "# train the embedding:\n",
        "#embeddings_train = bow.fit_transform(data_train['path'].values)\n",
        "\n",
        "# vectorize test data:\n",
        "#embeddings_test = bow.transform(data_test['path'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qbhJp3tfQTJ"
      },
      "source": [
        "**Classification with a linear SVM**\n",
        "\n",
        "See documentation of [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUb6hTYVfQTJ",
        "outputId": "141d9bb7-db67-48e7-aeb1-ed8d4e82dd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.761\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# train classifier:\n",
        "svm.fit(embeddings_train, data_train['label'].values)\n",
        "\n",
        "# test classifier:\n",
        "predictions = svm.predict(embeddings_test)\n",
        "\n",
        "# Calculate Accuracy:\n",
        "print('Accuracy:', accuracy_score(data_test['label'].values, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7n0tlJZug-j"
      },
      "source": [
        "### Own text mining pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9AfjxXX1AQf"
      },
      "source": [
        "Tokenize a few sample texts with our chosen tokenizer BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2LtJwsVeG6",
        "outputId": "9aab73cf-43b6-4f89-df74-98296e8a2179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original string: Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
            "Encoded tokens: [9362, 2897, 121, 66, 3078, 901, 18, 267, 3612, 125, 118, 613, 313, 130, 251, 297, 11684, 286, 1745, 504, 16, 568, 130, 6, 15371, 960, 997, 6525, 581, 117, 118, 14232, 7037, 769, 168, 132, 842, 156, 9362, 2897, 11, 84, 9018, 121, 408, 6917, 132, 2630, 389, 121, 6, 15371, 960, 175, 23018, 132, 4882, 11284, 16, 118, 9245, 4970, 238, 291, 263, 605, 542, 352, 2583, 9155, 11, 8157, 16, 118, 26612, 135, 118, 896, 1780, 16, 204, 1818, 168, 135, 118, 13998, 45, 2734, 134, 352, 4970, 18, 988, 45, 712, 118, 778, 117, 349, 66, 4386, 6449, 2165, 132, 2446, 616, 118, 1745, 16, 45, 3958, 12497, 20555, 125, 14557, 2897, 18, 37, 1410, 757, 30, 26913, 30, 45, 11, 78, 344, 132, 12358, 190, 135, 517, 9155, 18, 15461, 30, 15640, 132, 9362, 2897, 18, 45, 657, 156, 484, 3074, 135, 303, 415, 379, 156, 9362, 2897, 121, 663, 8661, 18, 694, 66, 5512, 156, 133, 854, 11, 85, 5]\n",
            "Decoded string: Bromwell High is a cartoon comedy . It ran at the same time as some other programs about school life , such as \" Teachers \". My 35 years in the teaching profession lead me to believe that Bromwell High ' s satire is much closer to reality than is \" Teachers \". The scramble to survive financially , the insightful students who can see right through their pathetic teachers ' pomp , the pettiness of the whole situation , all remind me of the schools I knew and their students . When I saw the episode in which a student repeatedly tried to burn down the school , I immediately recalled ......... at .......... High . A classic line : INSPECTOR : I ' m here to sack one of your teachers . STUDENT : Welcome to Bromwell High . I expect that many adults of my age think that Bromwell High is far fetched . What a pity that it isn ' t !\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Add a pre-tokenizer to handle whitespace properly\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize a BPE trainer with some default parameters\n",
        "trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"__ellipsis__\"])\n",
        "\n",
        "# Convert cleaned_text column to a list of strings for training\n",
        "training_data = data_train['cleaned_text'].tolist()\n",
        "\n",
        "# Train the tokenizer on the cleaned text data\n",
        "tokenizer.train_from_iterator(training_data, trainer)\n",
        "\n",
        "# Save the tokenizer for later use\n",
        "tokenizer.save(\"imdb_bpe_tokenizer.json\")\n",
        "\n",
        "# Encode the text data for the model\n",
        "encoded_train = [tokenizer.encode(text).ids for text in data_train['cleaned_text']]\n",
        "encoded_test = [tokenizer.encode(text).ids for text in data_test['cleaned_text']]\n",
        "\n",
        "# Select a single example to decode and print\n",
        "sample_index = 0  # Choose an index, e.g., 0 for the first item\n",
        "encoded_sample = encoded_train[sample_index]\n",
        "\n",
        "# Decode the tokenized output back to text\n",
        "decoded_text = tokenizer.decode(encoded_sample)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original string:\", data_train['cleaned_text'][sample_index])  # Original text for comparison\n",
        "print(\"Encoded tokens:\", encoded_sample)\n",
        "print(\"Decoded string:\", decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tokenization was now performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Training Data Shape: (1000, 15000)\n",
            "TF-IDF Test Data Shape: (1000, 15000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer with desired parameters\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=15000,       # You can adjust based on memory and data size\n",
        "    ngram_range=(1, 2),      # Consider unigrams and bigrams for richer context\n",
        ")\n",
        "\n",
        "# Fit on training data and transform both training and test data\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(data_train['cleaned_text'])\n",
        "tfidf_test = tfidf_vectorizer.transform(data_test['cleaned_text'])\n",
        "\n",
        "# Print the shapes of the resulting TF-IDF matrices\n",
        "print(\"TF-IDF Training Data Shape:\", tfidf_train.shape)\n",
        "print(\"TF-IDF Test Data Shape:\", tfidf_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.824\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.83      0.82      0.82       500\n",
            "         pos       0.82      0.83      0.83       500\n",
            "\n",
            "    accuracy                           0.82      1000\n",
            "   macro avg       0.82      0.82      0.82      1000\n",
            "weighted avg       0.82      0.82      0.82      1000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\kentf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize and train the SVM classifier\n",
        "svm = LinearSVC()\n",
        "svm.fit(tfidf_train, data_train['label'])  # 'label' is your target variable\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = svm.predict(tfidf_test)\n",
        "\n",
        "# Evaluate the classifier's performance\n",
        "accuracy = accuracy_score(data_test['label'], predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(data_test['label'], predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cpu\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)          # To confirm the version\n",
        "print(torch.cuda.is_available())  # This should return False as there is no GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map the labels to integers\n",
        "label_mapping = {'pos': 1, 'neg': 0}\n",
        "data_train['label'] = data_train['label'].map(label_mapping)\n",
        "data_test['label'] = data_test['label'].map(label_mapping)\n",
        "\n",
        "# Now convert to tensor\n",
        "y_train_tensor = torch.tensor(data_train['label'].values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(data_test['label'].values, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2, Loss: 0.6064429935067892\n",
            "Epoch 2/2, Loss: 0.04579201550404832\n",
            "Test Accuracy: 0.827\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83       500\n",
            "           1       0.84      0.81      0.82       500\n",
            "\n",
            "    accuracy                           0.83      1000\n",
            "   macro avg       0.83      0.83      0.83      1000\n",
            "weighted avg       0.83      0.83      0.83      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert to dense tensors\n",
        "X_train_tensor = torch.tensor(tfidf_train.toarray(), dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(tfidf_test.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Labels as tensors\n",
        "y_train_tensor = torch.tensor(data_train['label'].values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(data_test['label'].values, dtype=torch.long)\n",
        "\n",
        "# Define the Neural Network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = SimpleNN(input_size)\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "# Evaluation on Test Data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(test_outputs, 1)\n",
        "    accuracy = accuracy_score(y_test_tensor, predicted)\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test_tensor, predicted))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDx9UUI51Rqz"
      },
      "source": [
        "Further text normalization like stemming\n",
        "\n",
        "Kent: I dont think it is necessary and we should implement it into the preprocessing/normalization function at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz04Rt4eolPf",
        "outputId": "ced45d84-39ff-4d79-d8ba-40cf2bebb789"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "C:\\Users\\kentf\\AppData\\Local\\Temp\\ipykernel_4560\\946938710.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  \"\"\" import nltk\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' import nltk\\nimport re\\nfrom tokenizers import Tokenizer\\nfrom tokenizers.models import BPE\\nfrom tokenizers.trainers import BpeTrainer\\nfrom tokenizers.pre_tokenizers import Whitespace\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.stem import PorterStemmer\\n\\n# Download the \\'punkt\\' resource for tokenization\\nnltk.download(\\'punkt\\')\\n\\n# Function when further normalization is needed\\ndef further_normalization_needed(training_data):\\n    stemmer = PorterStemmer()\\n\\n    for i, text in enumerate(training_data):\\n        print(f\"\\nOriginal Text {i+1}: {text}\")\\n\\n        # Tokenize using word_tokenize for stemming analysis\\n        tokens = word_tokenize(text)\\n        stemmed_tokens = [stemmer.stem(token) for token in tokens]\\n\\n        # Custom regex to remove unwanted characters (e.g., punctuation)\\n        cleaned_text = re.sub(r\\'[^\\\\w\\\\s]\\', \\'\\', text)\\n\\n        print(\"Stemmed Tokens:\", stemmed_tokens)\\n        print(\"Cleaned Text (after regex):\", cleaned_text)\\n\\n# Check if further text normalization is needed\\nfurther_normalization_needed(training_data) '"
            ]
          },
          "execution_count": 317,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" import nltk\n",
        "import re\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download the 'punkt' resource for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function when further normalization is needed\n",
        "def further_normalization_needed(training_data):\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    for i, text in enumerate(training_data):\n",
        "        print(f\"\\nOriginal Text {i+1}: {text}\")\n",
        "\n",
        "        # Tokenize using word_tokenize for stemming analysis\n",
        "        tokens = word_tokenize(text)\n",
        "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        # Custom regex to remove unwanted characters (e.g., punctuation)\n",
        "        cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "        print(\"Cleaned Text (after regex):\", cleaned_text)\n",
        "\n",
        "# Check if further text normalization is needed\n",
        "further_normalization_needed(training_data) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8AKEvI4CvuR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb8QQiBEJmya",
        "outputId": "b72f2397-072c-459f-cf0f-276081014c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bag of Words with BPE Tokenization:\n",
            "[[1 0 1 3 1 1 1 0 0 3 1 1 1 0 2 1 1 1 2 0 0 1 1 0 0 1 1 0 0 0 1]\n",
            " [2 1 1 0 1 0 1 1 1 1 0 2 0 2 1 1 2 0 0 2 2 2 1 1 1 0 2 1 1 1 0]]\n",
            "[[1 0 1 1 0 1 1 0 0 3 0 1 0 0 2 1 0 0 3 1 1 2 0 1 0 0 1 0 0 0 0]\n",
            " [2 0 0 0 1 0 1 1 1 4 1 0 2 0 0 0 4 1 0 0 1 1 1 0 0 1 3 2 1 1 1]]\n",
            "\n",
            "Bag of Words with BPE Tokenization and Normalization:\n",
            "[[2 0 1 3 1 0 1 0 3 1 0 1 0 3 1 1 1 2 0 0 1 1 0 0 1 1 0 0 0]\n",
            " [3 1 1 0 1 1 1 1 0 0 1 0 1 2 2 1 0 0 2 1 2 1 1 1 0 2 1 1 1]]\n",
            "[[2 0 1 1 0 0 1 0 2 0 0 0 0 3 1 1 0 3 1 1 2 0 1 0 0 1 0 0 0]\n",
            " [2 0 0 0 1 1 1 1 2 1 0 2 0 0 1 0 1 0 0 0 0 1 0 0 1 2 2 0 1]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\kentf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample data for demonstration\n",
        "data_train = pd.DataFrame({\n",
        "    'cleaned_text': [\n",
        "        \"Hello, this is a test sentence for the tokenizer.\",\n",
        "        \"This is another sentence to improve subword merging.\"\n",
        "   ],\n",
        "    'label': [0, 1]  # Add a 'label' column to the DataFrame\n",
        "})\n",
        "\n",
        "data_test = pd.DataFrame({\n",
        "    'cleaned_text': [\n",
        "        \"Hello, this is another example sentence.\",\n",
        "        \"Subword tokenization is quite effective.\"\n",
        "    ],\n",
        "    'label': [0, 1]  # Add a 'label' column to the DataFrame\n",
        "})\n",
        "\n",
        "\n",
        "# 1. Improved Tokenization (BPE)\n",
        "# Initialize BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "trainer = BpeTrainer(vocab_size=10000, min_frequency=2)\n",
        "tokenizer.train_from_iterator(data_train['cleaned_text'], trainer=trainer)\n",
        "\n",
        "# Rename the function to avoid conflict with the 'tokens' variable\n",
        "def bpe_tokenize_func(text):\n",
        "    return tokenizer.encode(text).tokens\n",
        "\n",
        "# 2. Improved Tokenization with Normalization (BPE)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Stem each word\n",
        "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "def bpe_tokenize_normalized(text):\n",
        "    normalized_text = normalize_text(text)\n",
        "    return tokenizer.encode(normalized_text).tokens\n",
        "\n",
        "# Create CountVectorizer for each tokenization method\n",
        "\n",
        "# Bag of Words with BPE Tokenization\n",
        "# Use the renamed function here\n",
        "bow_bpe = CountVectorizer(tokenizer=bpe_tokenize_func, token_pattern=None)\n",
        "embeddings_train_bpe = bow_bpe.fit_transform(data_train['cleaned_text'].values)\n",
        "embeddings_test_bpe = bow_bpe.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "# Bag of Words with BPE Tokenization and Normalization\n",
        "bow_bpe_norm = CountVectorizer(tokenizer=bpe_tokenize_normalized, token_pattern=None)\n",
        "embeddings_train_bpe_norm = bow_bpe_norm.fit_transform(data_train['cleaned_text'].values)\n",
        "embeddings_test_bpe_norm = bow_bpe_norm.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "\n",
        "print(\"\\nBag of Words with BPE Tokenization:\")\n",
        "print(embeddings_train_bpe.toarray())\n",
        "print(embeddings_test_bpe.toarray())\n",
        "\n",
        "print(\"\\nBag of Words with BPE Tokenization and Normalization:\")\n",
        "print(embeddings_train_bpe_norm.toarray())\n",
        "print(embeddings_test_bpe_norm.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "988exFnFZ4dP",
        "outputId": "ee676a6c-012e-4081-e96d-956fbee5cdf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# train classifier:\n",
        "svm.fit(embeddings_train_bpe, data_train['label'].values)\n",
        "\n",
        "\n",
        "# test classifier:\n",
        "predictions = svm.predict(embeddings_test_bpe)\n",
        "\n",
        "\n",
        "# Calculate Accuracy:\n",
        "print('Accuracy:', accuracy_score(data_test['label'].values, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1WMtvEpdCPf",
        "outputId": "c3bb6191-6759-4064-a52e-5bd92b3eff9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with BPE Tokenization and Normalization: 0.5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "# Train and evaluate classifier using BPE embeddings\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# Train classifier\n",
        "#svm.fit(embeddings_train, data_train['label'].values)\n",
        "#svm.fit(embeddings_train_bpe, data_train['label'].values)\n",
        "svm.fit(embeddings_train_bpe_norm, data_train['label'].values) # The model is trained on embeddings_train_bpe_norm\n",
        "\n",
        "\n",
        "# Test classifier\n",
        "#predictions = svm.predict(embeddings_test_bpe) # This line causes the error because embeddings_test_bpe has a different number of features\n",
        "predictions = svm.predict(embeddings_test_bpe_norm) # Use embeddings_test_bpe_norm for prediction, which has the same number of features as the training data\n",
        "\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(data_test['label'].values, predictions)\n",
        "print('Accuracy with BPE Tokenization and Normalization:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO0d9PMR2CZr"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
