{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinamilo6/Textmining/blob/main/A03_TextMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1D-_ww7fQTB",
        "outputId": "b3b6c476-4e27-4b42-aca9-73fad6976ded"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas sklearn nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS1_jLQofQTD"
      },
      "source": [
        "# Assignment 3 - Text Mining\n",
        "\n",
        "Project management and tools for health informatics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufi0zQidfQTE"
      },
      "source": [
        "## 1. Download and prepare data:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "The code in this section downloads the [IMDB IMDB Large Movie Review Dataset]('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz') which is the dataset you will be working on in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aOzKCiw4fQTE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sq9rCBBrfQTE"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('aclImdb'):\n",
        "    # download data:\n",
        "    urlretrieve('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', 'aclImdb.tar.gz')\n",
        "\n",
        "    # unzip data:\n",
        "    with tarfile.open('aclImdb.tar.gz') as file:\n",
        "        file.extractall('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9pzG__QfQTF"
      },
      "source": [
        "## 2. Some helper Functions:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "This section contains the code for some helper functions that will be useful for solving the assignment. Example code on how to use the functions is provided in section 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LaM3GoopfQTF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from typing import Literal, Tuple, Iterable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxsGe7MufQTF"
      },
      "source": [
        "Function for loading data into a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tKoAn698fQTG"
      },
      "outputs": [],
      "source": [
        "def load_data(split:Literal['train', 'test'], texts_per_class:int=500) -> pd.DataFrame:\n",
        "    ''' Loads the data into a pandas dataframe.'''\n",
        "    paths  = []\n",
        "    labels = []\n",
        "\n",
        "    for label in ('pos', 'neg'):\n",
        "        # get all files in the folder:\n",
        "        files = os.listdir(os.path.join('aclImdb', split, label))[:texts_per_class]\n",
        "\n",
        "        # append them to the lists:\n",
        "        paths.extend([os.path.join('aclImdb', split, label, f) for f in files])\n",
        "        labels.extend([label] * len(files))\n",
        "\n",
        "    return pd.DataFrame({'path':paths, 'label':labels})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C1jyAtfQTG"
      },
      "source": [
        "Function for loading a specific text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y0QGuZMXfQTH"
      },
      "outputs": [],
      "source": [
        "def load_text(path:str) -> str:\n",
        "    ''' Reads a single text given the path. '''\n",
        "    # read file from disk:\n",
        "    with open(path, 'r', encoding='utf8') as file:\n",
        "        s = file.read()\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR52KxrvfQTH"
      },
      "source": [
        "Function for iterating through multiple texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NImrB2NzfQTH"
      },
      "outputs": [],
      "source": [
        "def iterate_texts(data:pd.DataFrame) -> Iterable[Tuple[str, str]]:\n",
        "    ''' Iterates through a pandas dataframe. '''\n",
        "\n",
        "    for path in data['path'].values:\n",
        "        # read file from disk:\n",
        "        with open(path, 'r', encoding='utf8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        yield text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzYgen2AfQTH"
      },
      "source": [
        "## 3. Text Mining Pipeline\n",
        "\n",
        "This section will cover the text mining steps for this assignment. The following steps will be performed:\n",
        "\n",
        "1. **Analyze the Data for Difficult Parts**  \n",
        "   - Review the data to identify challenging aspects such as contractions, informal language, and complex sentence structures.\n",
        "\n",
        "2. **Replace Contractions and Informal Language**  \n",
        "   - Expand common contractions and replace informal phrases, if necessary, to standardize the text for processing.\n",
        "\n",
        "3. **Tokenize the Texts**  \n",
        "   - Apply a suitable tokenizer to break down the text into individual tokens for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import needed libaries for the preparation of the texts\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3FnCEu6fQTH"
      },
      "source": [
        "### Load the training data\n",
        "The loaded data frpom the zip file should be saved into a `data_train` and `data_test` DataFrame.\n",
        "These can be further on be used to access the train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CeshtSl9fQTI",
        "outputId": "820af184-b4a1-4ff7-c2aa-baa64153e918"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aclImdb\\train\\pos\\0_9.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aclImdb\\train\\pos\\10000_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aclImdb\\train\\pos\\10001_10.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aclImdb\\train\\pos\\10002_7.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aclImdb\\train\\pos\\10003_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>aclImdb\\train\\neg\\10446_2.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>aclImdb\\train\\neg\\10447_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>aclImdb\\train\\neg\\10448_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>aclImdb\\train\\neg\\10449_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>aclImdb\\train\\neg\\1044_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               path label\n",
              "0         aclImdb\\train\\pos\\0_9.txt   pos\n",
              "1     aclImdb\\train\\pos\\10000_8.txt   pos\n",
              "2    aclImdb\\train\\pos\\10001_10.txt   pos\n",
              "3     aclImdb\\train\\pos\\10002_7.txt   pos\n",
              "4     aclImdb\\train\\pos\\10003_8.txt   pos\n",
              "..                              ...   ...\n",
              "995   aclImdb\\train\\neg\\10446_2.txt   neg\n",
              "996   aclImdb\\train\\neg\\10447_1.txt   neg\n",
              "997   aclImdb\\train\\neg\\10448_1.txt   neg\n",
              "998   aclImdb\\train\\neg\\10449_4.txt   neg\n",
              "999    aclImdb\\train\\neg\\1044_4.txt   neg\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train = load_data('train')\n",
        "data_test  = load_data('test')\n",
        "data_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qXdudbDfQTI"
      },
      "source": [
        "### 1. Assess the data / texts for difficult parts\n",
        "In this part, sample texts are printed and then analyzed for diffult parts, that could affect the text mining process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soE8Qj9hsF0E",
        "outputId": "62c10086-d43f-4f1d-a24e-c88460cb43b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1:\n",
            "\n",
            "Made after QUARTET was, TRIO continued the quality of the earlier film versions of the short stories by Maugham. Here the three stories are THE VERGER, MR. KNOW-IT-ALL, and SANITORIUM. The first two are comic (THE VERGER is like a prolonged joke, but one with a good pay-off), and the last more serious (as health issues are involved). Again the author introduces the film and the stories.<br /><br />James Hayter, soon to have his signature role as Samuel Pickwick, is the hero in THE VERGER. He holds this small custodial-type job in a church, but the new Vicar (Michael Hordern) is an intellectual snob. When he hears Hayter has no schooling he fires him. Hayter has saved some money, so he tells his wife (Kathleen Harrison) he fancies buying a small news and tobacco shop. He has a good eye, and his store thrives. Soon he has a whole chain of stores. When his grandchild is christened by Hordern, the latter is amazed to see how prosperous his ex-Verger. The payoff is when bank manager Felix Aylmer meets with Hayter about diversifying his investments. I'll leave it to you to hear the unintentional but ironic coda of the meeting.<br /><br />According to Maugham he met a man like Max Kelada (Nigel Patrick) on a cruise. In MR. KNOW-IT-ALL Kelada is a splashy, friendly, and slightly overbearing type from the Middle East who is on a business trip (regarding jewelry) by steamship. His state-room mate is Mr. Grey (the ever quiet and proper Wilfred Hyde-White) who is somewhat, silently disapproving of Max. Max likes to enliven things, and soon is heavily involved in the ship's entertainment. At this point the story actually resembles part of the plot of the non-Maugham story and film CHINA SEAS (1935), as Max makes a bet that he can tell a real piece of jewelry from a fake (after insisting that a piece of jewelry he spotted is real). I won't describe the way Max rises to the occasion.<br /><br />SANITORIUM is the longest segment. Roland Culver plays \"Ashenden\" (the fictional alter-ego of Maugham - a writer and one time spy as in Hitchcock's THE SECRET AGENT). Here he has to use a sanitorium for a couple of months for his health. He finds a remarkable crew of people, including Jean Simmons as a frail but beautiful young woman, Finlay Currie as an irascible Scotsman, John Laurie as a second irascible Scotsman who is \"at war\" with Currie, Raymond Huntley as a quiet patient who only shows his internal anger at his situation when his wife shows up, and Michael Rennie as a young man who has a serious life threatening illness. Culver watches as three stories among these characters play out to their conclusions. The last, dealing with Simmons and Rennie, is ironic but deeply moving.<br /><br />It was a dandy follow-up to the earlier QUARTET, and well worth watching.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 2:\n",
            "\n",
            "BTK Killer, Green River Killer, Zodiac Killer; the man keeps putting out absolute garbage and the ironic thing is, he loves his crap.<br /><br />I've never seen a Ulli Lommel film but I was so amazed on how everyone thinks his stuff is so awful. Like the movies I said in the beginning don't even equal a six when added together! After reading the comments I was curious to see how bad this guy really is. He is the worst out there.<br /><br />The credits wouldn't end as the pathetic movie started and quickly I noticed that the audio was incredibly badly dubbed in. The acting was incredibly awful and same to the camera shots. The editing is easily the worst. This movie made no sense and I unbearably couldn't take it anymore as it wouldn't end and I was only 45 minutes in the movie. I couldn't take it anymore. I wasted 45 minutes of my life.<br /><br />DO NOT WATCH THIS CRAP!\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 3:\n",
            "\n",
            "In this \"critically acclaimed psychological thriller based on true events, Gabriel (Robin Williams), a celebrated writer and late-night talk show host, becomes captivated by the harrowing story of a young listener and his adoptive mother (Toni Collette). When troubling questions arise about this boy's (story), however, Gabriel finds himself drawn into a widening mystery that hides a deadly secret\" according to film's official synopsis.<br /><br />You really should STOP reading these comments, and watch the film NOW...<br /><br />The \"How did he lose his leg?\" ending, with Ms. Collette planning her new life, should be chopped off, and sent to \"deleted scenes\" land. It's overkill. The true nature of her physical and mental ailments should be obvious, by the time Mr. Williams returns to New York. Possibly, her blindness could be in question - but a revelation could have be made certain in either the \"highway\" or \"video tape\" scenes. The film would benefit from a re-editing - how about a \"director's cut\"? <br /><br />Williams and Bobby Cannavale (as Jess) don't seem, initially, believable as a couple. A scene or two establishing their relationship might have helped set the stage. Otherwise, the cast is exemplary. Williams offers an exceptionally strong characterization, and not a \"gay impersonation\". Sandra Oh (as Anna), Joe Morton (as Ashe), and Rory Culkin (Pete Logand) are all perfect.<br /><br />Best of all, Collette's \"Donna\" belongs in the creepy hall of fame. Ms. Oh is correct in saying Collette might be, \"you know, like that guy from 'Psycho'.\" There have been several years when organizations giving acting awards seemed to reach for women, due to a slighter dispersion of roles; certainly, they could have noticed Collette with some award consideration. She is that good. And, director Patrick Stettner definitely evokes Hitchcock - he even makes getting a sandwich from a vending machine suspenseful.<br /><br />Finally, writers Stettner, Armistead Maupin, and Terry Anderson deserve gratitude from flight attendants everywhere.<br /><br />******* The Night Listener (1/21/06) Patrick Stettner ~ Robin Williams, Toni Collette, Sandra Oh, Rory Culkin\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 4:\n",
            "\n",
            "Admirably odd, though mean-spirited comedy-drama about a strange young man who hopes to fly like a bird through the Houston Astrodome. Robert Altman-directed quasi-comedy with eccentric characters is so overloaded with weirdos that it starts to creak early on from the weight. Some of the cinematography is evocative, Shelley Duvall is a stitch in her debut as a tour guide, and Sally Kellerman looks every inch the glamourpuss as Bud Cort's vision of a \"mother bird\" (imagine Altman and producer Lou Adler explaining that role to her!). In the lead, Bud Cort is--once again, after \"Harold & Maude\"--a true original; not off-putting like, say, Michael J. Pollard, Cort manages to be geeky, wacky and inoffensive, a tough act to pull off. Unfortunately, this is one of Altman's misfires. He can put together a cast and a showpiece like no one else, but let him get fired up with some misguided inspiration and he spirals downward. ** from ****\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 5:\n",
            "\n",
            "The fact that this film was put out on DVD still formatted-to TV and with a fuzzy picture really annoyed a lot of film purists......and rightly so. This deserves a lot better treatment.<br /><br />The story is about a street performer who needs a son to pass on his craft (the rules of the day) and winds up with a little girl instead (not the conventional way) ....and the problems that ensue afterward. The old man had bought the kid at a slave auction and soon discovers the kid is not a boy, which he obviously thought was the case.<br /><br />The old man \"Bianlian Wang (Xu Zhu)is kind of funny-looking with a missing front tooth and an infectious grin. The little girl \"Doggie\" (Zhou Renying) is a cutie. The rest of the story is how the two manage after that. I usually like a nice sentimental ending but this gets a bit carried away in the final 15 minutes.<br /><br />Overall, it's involving story complete with drama, suspense, humor and sadness. Just don't expect a good quality picture for the money you are spending on the DVD. Until it comes out on widescreen, rent it.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 6:\n",
            "\n",
            "More wide-eyed, hysterical 50s hyper-cheerfulness that gives new meaning to anti-social, pathological behaviour. Danza and Grayson will leave you begging for mercy.<br /><br />It's a shame that all the people involved in the making of this movie are now dead (or in nursing homes). I kinda thought about suing them for torture. As this movie started unleashing its shamelessly aggressive operatic assault onto my poor, defenseless ear-drums, I felt instant, strong pain envelop my entire being. That damn muscular vibrato can shatter Soviet tanks into tiny bits, nevermind glass.<br /><br />\"Why didn't you switch the channel if you didn't like it?\", you might ask angrily. Fair point, fair point... The answer is that I wanted to, but the pain was so sudden and excruciating that I fell to the floor, writhing in agony. With my last ounces of energy, I tried to reach the remote but couldn't.<br /><br />A silly little fisherman with the questionable talent of singing with an annoying opera voice is discovered by Niven, who then proceeds to \"pigmalionize\" him. Lanza is in love with asymmetrical Grayson, but she predictably treats him with contempt until they finally hook up. This may seem like a rather thin plot, but this noisy movie is so chock-full of singing and music that there is barely any dialogue at all. This movie is RELENTLESS. Forget about torturing hippies and war prisoners with Slayer's \"Reign In Blood\" (as in a South Park episode). Whatever little conversation there is amongst the silly adults that infest this strange 50s musical world, it's all infantile - as if they were all 6 year-olds impersonating grown-ups. I can only envy people who find movies like this funny. It must be great being easy-to-please: what a world of wonder would open up to me if only I could enjoy any silly old gag as hilarious, gut-busting comedy. <br /><br />But let's examine this phenomenon, the 50s musical. My best guess is that 50s musicals offered the more day-dreaming idealists among us a glimpse into Utopia or Heaven (depending on whether you're church-going or Lenin's-tomb-going), or at least very cheesy version of these fantasy-inspired places. TTONO is more akin to a representation of Hell, but that's just me. I don't seem to \"get\" musicals. People talk, there is a story - but then all-of-a-sudden everyone starts singing for about 4 minutes after which they abruptly calm down and then pretend as if nothing unusual happened! When you think about it, musicals are stranger than any science-fiction film.<br /><br />Worse yet, TTONO (my favourite type of pizza, btw) is not just a 50s musical, but one with opera squealing. Opera is proof that there is such a thing as over-training a voice - to the point where it becomes an ear-piercing weapon rather than a means of bringing the listener pleasure. The clearest example of this travesty is when Lanza and Grayson unite their Dark Side vocal powers for a truly unbearable duet. I tried lowering the volume. I lowered it from 18 to 14. Then from 14 to 10. Then 8. I ended up lowering it to a 1, which is usually so low that it's only heard by specially-trained dogs and certain types of marsupials, and yet I STILL could hear those two braying like donkeys!<br /><br />Take the scene in the small boat in the river. Danza starts off with one of his deafening, brain-killing tunes, and then... nothing. No animals anywhere to be seen. Even the crocodiles, who are mostly deaf, have all but left. If you look carefully, you might even see the trees change colour, from green to yellow, in a matter of minutes. No, this was not a continuity error, it was plain old torture of the flora. And those trees were just matte paintings! Imagine how real trees would have reacted.<br /><br />The reason glass breaks when a high C is belched out of the overweight belly of an operatic screamer is not due to any laws of physics relating to waves and frequency, but because glass is only human - hence can take only so much pain before committing suicide through spontaneous self-explosion. I can listen to the loudest, least friendly death metal band for hours, but give me just a minute of a soprano and I get a splitting headache.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 7:\n",
            "\n",
            "Dull, cheap sci-fi thriller, made with an almost total lack of conviction (a control room full of computers and other devices used to receive and decipher messages from outer space is run by only ONE MAN, and is VERY poorly guarded at night), and full of campy sound effects. Christopher Lee is not only wasted, but he also gives one of his few \"I'm here strictly for the money\" performances. (*1/2)\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Number of samples to be printed\n",
        "n_samples = 7\n",
        "\n",
        "# Randomly sample indices from \"data_train\"\n",
        "sample_indices = random.sample(range(len(data_train)), n_samples)\n",
        "\n",
        "# Load and print each sample using the \"load_text\" function\n",
        "for i, id in enumerate(sample_indices, start=1):\n",
        "    # Load text from the file path specified in 'path' column\n",
        "    text = load_text(data_train.loc[id, 'path'])\n",
        "    print(f\"Sample {i}:\\n\")\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWw3AZ8SumVn"
      },
      "source": [
        "## Limitations/Issues Captured from the Above Text Samples\n",
        "\n",
        "- **HTML tags and special characters**\n",
        "- **Punctuation and symbols** (e.g., `&`)\n",
        "- **Contractions** (e.g., \"isn't\", \"I'll\", \"I'm\")\n",
        "- **Parentheses and annotations** (e.g., \"(Crouching Tiger)\")\n",
        "- **Informal formatting** (e.g., \"my rating is ****\")\n",
        "- **Ambiguity and polysemy** (e.g., \"dictators\", \"nuts\")\n",
        "- **Long and complex sentences**\n",
        "- **Informal language** (e.g., \"what can be so bad about that?\")\n",
        "- **Quotation marks** (e.g., \"dictators\", \"sin\")\n",
        "\n",
        "We will elaborate on these issues and add our conclusions in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Preprocessing: Simplify the Text\n",
        "\n",
        "This section focuses on preprocessing the texts to make them more suitable for text mining. Identified constraints from the analysis step will be addressed and, as far as possible, eliminated to improve processing accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Zgm5pNuGJNM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    # Regex to match HTML tags\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZR8XyEHiGb94"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation_and_symbols(text):\n",
        "    # Remove punctuation and special symbols using regex\n",
        "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eYMTaZhZHOlZ"
      },
      "outputs": [],
      "source": [
        "def remove_parentheses(text):\n",
        "    # Remove text inside parentheses along with parentheses\n",
        "    clean_text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Expand the contractions\n",
        "There are python libraries that focus on expanding contractions. For easier loading (if these packages are not available) we decided to create our own replacement list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaTBJ0qfG5uy",
        "outputId": "b24e2271-de35-47af-a921-4e15a526d162"
      },
      "outputs": [],
      "source": [
        "# Dictionary of common contractions and their expanded forms\n",
        "contractions_dict = {\n",
        "    \"i'm\": \"i am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"here's\": \"here is\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"shan't\": \"shall not\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b', flags=re.IGNORECASE)\n",
        "    def replace(match):\n",
        "        return contractions_dict.get(match.group(0).lower(), match.group(0))\n",
        "    return contractions_pattern.sub(replace, text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zrIoCD2IHqLy"
      },
      "outputs": [],
      "source": [
        "# Define a set of informal tokens to remove\n",
        "informal_tokens = {\n",
        "    \"u\": \"you\",\n",
        "    \"r\": \"are\",\n",
        "    \"lmao\": \"\",  # Remove\n",
        "    \"lol\": \"\",   # Remove\n",
        "    \"btw\": \"by the way\",\n",
        "    \"idk\": \"I do not know\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"gonna\": \"going to\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"gotta\": \"got to\",\n",
        "    \"b/c\": \"because\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"pls\": \"please\",\n",
        "    \"cuz\": \"because\",\n",
        "    \"wut\": \"what\",\n",
        "    \"smh\": \"\",  # Remove\n",
        "    \"k\": \"okay\",\n",
        "    \"ttyl\": \"talk to you later\"\n",
        "}\n",
        "\n",
        "def remove_informal_tokens(text):\n",
        "    # Replace informal tokens\n",
        "    for token, replacement in informal_tokens.items():\n",
        "        # Use regex to match whole words and replace them\n",
        "        text = re.sub(r'\\b' + re.escape(token) + r'\\b', replacement, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def fix_punctuation(text):\n",
        "    # Ensure proper spacing after punctuation\n",
        "    text = re.sub(r'\\s*([.,;:!?()])\\s*', r'\\1 ', text)  # Ensure space after punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = text.strip()  # Trim leading and trailing spaces\n",
        "    return text\n",
        "\n",
        "# Combined function to clean the text\n",
        "def clean_text(text):\n",
        "    text = remove_informal_tokens(text)  # Remove informal tokens\n",
        "    text = fix_punctuation(text)  # Fix punctuation\n",
        "    return text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             path  \\\n",
            "0       aclImdb\\train\\pos\\0_9.txt   \n",
            "1   aclImdb\\train\\pos\\10000_8.txt   \n",
            "2  aclImdb\\train\\pos\\10001_10.txt   \n",
            "3   aclImdb\\train\\pos\\10002_7.txt   \n",
            "4   aclImdb\\train\\pos\\10003_8.txt   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  Bromwell High is a cartoon comedy. It ran at t...  \n",
            "1  Homelessness( or Houselessness as George Carli...  \n",
            "2  Brilliant over-acting by Lesley Ann Warren. Be...  \n",
            "3  This is easily the most underrated film inn th...  \n",
            "4  This is not the typical Mel Brooks film. It wa...  \n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    text = load_text(text)              # Load text from file path\n",
        "    text = expand_contractions(text)    # Expand contractions\n",
        "    text = remove_informal_tokens(text) # Replace informal tokens\n",
        "    text = fix_punctuation(text)        # Fix punctuation\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing function to training and test data\n",
        "data_train['cleaned_text'] = data_train['path'].apply(preprocess_text)\n",
        "data_test['cleaned_text'] = data_test['path'].apply(preprocess_text)\n",
        "\n",
        "# Now, you can use 'cleaned_text' for further processing or model training\n",
        "print(data_train[['path', 'cleaned_text']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHLiIMDifQTI"
      },
      "source": [
        "### A simple pipeline:\n",
        "[Do not change it!]\n",
        "\n",
        "This simple pipeline will be used to compare the newly created pipeline against, to evaluate the performance increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOYzJ1-SfQTI"
      },
      "source": [
        "**White-Space tokenization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GJjcIUkHfQTI"
      },
      "outputs": [],
      "source": [
        "def tokenize(text:str):\n",
        "    ''' An example tokenization function. '''\n",
        "\n",
        "    # simple white-space tokenization:\n",
        "    return text.lower().split()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GddqcHfvfQTJ"
      },
      "source": [
        "**Bag-of-words Embedding:**\n",
        "\n",
        "See documentation of [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGnHx6ThfQTJ",
        "outputId": "0a44343e-9849-4dcd-cb25-68b4579c78a6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create a simple bag of words embedding:\n",
        "bow = CountVectorizer(\n",
        "\n",
        "    # the next line converts the filepaths to the actual texts:\n",
        "    #preprocessor = load_text, # Commented, to check if the preprocessing from above changes the results\n",
        "\n",
        "    # tokenization function from above:\n",
        "    tokenizer = tokenize,\n",
        "\n",
        "    # Set token_pattern to None since we're using a custom tokenizer\n",
        "    token_pattern=None\n",
        "\n",
        ")\n",
        "\n",
        "# Train the embedding on cleaned training data\n",
        "embeddings_train = bow.fit_transform(data_train['cleaned_text'].values)\n",
        "\n",
        "# Vectorize the cleaned test data\n",
        "embeddings_test = bow.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "# These are the original lines\n",
        "# train the embedding:\n",
        "#embeddings_train = bow.fit_transform(data_train['path'].values)\n",
        "\n",
        "# vectorize test data:\n",
        "#embeddings_test = bow.transform(data_test['path'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qbhJp3tfQTJ"
      },
      "source": [
        "**Classification with a linear SVM**\n",
        "\n",
        "See documentation of [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUb6hTYVfQTJ",
        "outputId": "09b4dc39-1cc1-4b39-cdd2-b13a468385ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.767\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# train classifier:\n",
        "svm.fit(embeddings_train, data_train['label'].values)\n",
        "\n",
        "# test classifier:\n",
        "predictions = svm.predict(embeddings_test)\n",
        "\n",
        "# Calculate Accuracy:\n",
        "print('Accuracy:', accuracy_score(data_test['label'].values, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Own text mining pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2LtJwsVeG6",
        "outputId": "fcac9f39-e48f-4c97-e81b-c1eee2711e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original string: Hello, this is a test sentence for the tokenizer.\n",
            "Encoded tokens: ['Hello', 't', 'h', 'is', 'is', 'a', 'test', 's', 'en', 't', 'en', 'e', 'or', 'the', 'tokenizer']\n",
            "Decoded string: Hello t h is is a test s en t en e or the tokenizer\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Example sentence to encode\n",
        "s = \"Hello, this is a test sentence for the tokenizer.\"\n",
        "\n",
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Add a pre-tokenizer to handle whitespace properly\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize a BPE trainer with some default parameters\n",
        "trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"])\n",
        "\n",
        "# Instead of reading files, let's provide training data directly as strings\n",
        "# Simulate a dataset by giving it a list of strings (or paths to actual files)\n",
        "training_data = [\"Hello world\", \"This is a test\", \"We are training the tokenizer\"]\n",
        "\n",
        "# The trainer expects file paths, but we can create files dynamically (or mock them)\n",
        "# For now, let's assume you have access to these files or use in-memory data.\n",
        "tokenizer.train_from_iterator(training_data, trainer)\n",
        "\n",
        "# Encode the string\n",
        "t = tokenizer.encode(s)\n",
        "\n",
        "# Decode the tokenized output\n",
        "decoded_s = tokenizer.decode(t.ids)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original string:\", s)\n",
        "print(\"Encoded tokens:\", t.tokens)\n",
        "print(\"Decoded string:\", decoded_s)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
