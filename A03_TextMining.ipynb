{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinamilo6/Textmining/blob/main/A03_TextMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X1D-_ww7fQTB"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas sklearn nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS1_jLQofQTD"
      },
      "source": [
        "# Assignment 3 - Text Mining\n",
        "\n",
        "Project management and tools for health informatics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufi0zQidfQTE"
      },
      "source": [
        "## 1. Download and prepare data:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "The code in this section downloads the [IMDB IMDB Large Movie Review Dataset]('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz') which is the dataset you will be working on in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aOzKCiw4fQTE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sq9rCBBrfQTE"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('aclImdb'):\n",
        "    # download data:\n",
        "    urlretrieve('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', 'aclImdb.tar.gz')\n",
        "\n",
        "    # unzip data:\n",
        "    with tarfile.open('aclImdb.tar.gz') as file:\n",
        "        file.extractall('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9pzG__QfQTF"
      },
      "source": [
        "## 2. Some helper Functions:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "This section contains the code for some helper functions that will be useful for solving the assignment. Example code on how to use the functions is provided in section 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LaM3GoopfQTF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from typing import Literal, Tuple, Iterable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxsGe7MufQTF"
      },
      "source": [
        "Function for loading data into a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tKoAn698fQTG"
      },
      "outputs": [],
      "source": [
        "def load_data(split:Literal['train', 'test'], texts_per_class:int=500) -> pd.DataFrame:\n",
        "    ''' Loads the data into a pandas dataframe.'''\n",
        "    paths  = []\n",
        "    labels = []\n",
        "\n",
        "    for label in ('pos', 'neg'):\n",
        "        # get all files in the folder:\n",
        "        files = os.listdir(os.path.join('aclImdb', split, label))[:texts_per_class]\n",
        "\n",
        "        # append them to the lists:\n",
        "        paths.extend([os.path.join('aclImdb', split, label, f) for f in files])\n",
        "        labels.extend([label] * len(files))\n",
        "\n",
        "    return pd.DataFrame({'path':paths, 'label':labels})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C1jyAtfQTG"
      },
      "source": [
        "Function for loading a specific text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y0QGuZMXfQTH"
      },
      "outputs": [],
      "source": [
        "def load_text(path:str) -> str:\n",
        "    ''' Reads a single text given the path. '''\n",
        "    # read file from disk:\n",
        "    with open(path, 'r', encoding='utf8') as file:\n",
        "        s = file.read()\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR52KxrvfQTH"
      },
      "source": [
        "Function for iterating through multiple texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NImrB2NzfQTH"
      },
      "outputs": [],
      "source": [
        "def iterate_texts(data:pd.DataFrame) -> Iterable[Tuple[str, str]]:\n",
        "    ''' Iterates through a pandas dataframe. '''\n",
        "\n",
        "    for path in data['path'].values:\n",
        "        # read file from disk:\n",
        "        with open(path, 'r', encoding='utf8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        yield text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzYgen2AfQTH"
      },
      "source": [
        "## 3. Text Mining Pipeline\n",
        "\n",
        "This section will cover the text mining steps for this assignment. The following steps will be performed:\n",
        "\n",
        "1. **Analyze the Data for Difficult Parts**  \n",
        "   - Reviewing the data to identify challenging aspects such as contractions, informal language, and complex sentence structures.\n",
        "\n",
        "2. **Replace Contractions and Informal Language**  \n",
        "   - Expanding common contractions and replacing informal phrases to standardize the text for processing.\n",
        "   - This part is optinal and it should be later evaluated with normalization and without.\n",
        "\n",
        "3. **Tokenize the Texts**  \n",
        "   - Applying a tokenizer to break down the text into individual tokens for analysis.\n",
        "\n",
        "4. **Training and Predicting with ML**\n",
        "   - Training different ML models with the tokenized data and predicting the test data to evaluate the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2o7r5Bug-U"
      },
      "source": [
        "### Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "57LKdqhTug-V"
      },
      "outputs": [],
      "source": [
        "# Import needed libaries for the preparation of the texts\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3FnCEu6fQTH"
      },
      "source": [
        "### Load the training data\n",
        "The loaded data frpom the zip file should be saved into a `data_train` and `data_test` DataFrame.\n",
        "These can be further on be used to access the train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CeshtSl9fQTI",
        "outputId": "fa88fb56-74c9-47ca-d8fa-c3b9b6e19fe5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aclImdb\\train\\pos\\0_9.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aclImdb\\train\\pos\\10000_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aclImdb\\train\\pos\\10001_10.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aclImdb\\train\\pos\\10002_7.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aclImdb\\train\\pos\\10003_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>aclImdb\\train\\neg\\10446_2.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>aclImdb\\train\\neg\\10447_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>aclImdb\\train\\neg\\10448_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>aclImdb\\train\\neg\\10449_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>aclImdb\\train\\neg\\1044_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               path label\n",
              "0         aclImdb\\train\\pos\\0_9.txt   pos\n",
              "1     aclImdb\\train\\pos\\10000_8.txt   pos\n",
              "2    aclImdb\\train\\pos\\10001_10.txt   pos\n",
              "3     aclImdb\\train\\pos\\10002_7.txt   pos\n",
              "4     aclImdb\\train\\pos\\10003_8.txt   pos\n",
              "..                              ...   ...\n",
              "995   aclImdb\\train\\neg\\10446_2.txt   neg\n",
              "996   aclImdb\\train\\neg\\10447_1.txt   neg\n",
              "997   aclImdb\\train\\neg\\10448_1.txt   neg\n",
              "998   aclImdb\\train\\neg\\10449_4.txt   neg\n",
              "999    aclImdb\\train\\neg\\1044_4.txt   neg\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train = load_data('train')\n",
        "data_test  = load_data('test')\n",
        "data_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qXdudbDfQTI"
      },
      "source": [
        "### 1. Assess the data / texts for difficult parts\n",
        "In this part, sample texts are printed and then analyzed for diffult parts, that could affect the text mining process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soE8Qj9hsF0E",
        "outputId": "9f3f109e-dedd-4ec9-a9ce-6fb530a35fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1:\n",
            "\n",
            "The Haunted World of Edward D. Wood, Jr. isn't a particularly good documentary. Aesthetically, it's lackluster and cheap looking, the people in it go off on tangents which make it very unfocused and in-cohesive, but this adds to it's charm. I say this because it's a documentary about an oddball that made oddball pictures and surrounded himself with fellow oddballs and, as such, there's really no other way to document the life and career of the man and his crew of misfits. There are some glimpses of insight into both the genius and the ineptness of Wood, and the portrayal of both qualities is a credit to the genuineness of the documentary. Overall, it's worth a watch for the Wood fan and those of cinema in general, but don't expect brilliance here. Expect a documentary made after Wood's own heart.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 2:\n",
            "\n",
            "Lorenzo Lamas stars as some type of CIA agent, who captures some exotic beauty named Alexa, kidnaps her daughter and forces her to fight her former employers. O.J Simpson is also on board to provide a dash of acting credibility for the not so talented ensemble. I must admit i'm not a fan of Lorenzo Lamas, or his movies. He stinks. However when compared to O.J Simpson and Lamas' comatose wife Kinmont, Lamas seems like ah, Jean-Claude Van Damme. I only saw CIA because of the renewed interest around the O.J Simpson trial, you see because if your parents had cable and the extra channels, you couldn't escape this movie. in 1994 you could go to an Amish community and some moron would have this playing in their portable TV. The movie itself is a collection of lame action sequences and would be intrigue although the shock value of O.J Simpson jumping after fireballs and exchanging would be one liners do provide some unintentional humor. Also where was Bobby Knight and Kobe Bryant to make this a complete camp classic? <br /><br />* out of 4-(Bad)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 3:\n",
            "\n",
            "If there is one thing to recommend about this film is that it is intriguing. The premise certainly draws the audience in because it is a mystery, and throughout the film there are hints that there is something dark lurking about. However, there is not much tension, and Williams' mild mannered portrayal doesn't do much to makes us relate to his obsession with the boy.<br /><br />Collete fares much better as the woman whose true nature and intentions are not very clear. The production felt rushed and holes are apparent. It certainly feels like a preview for a much more complete and better effort. The book is probably better.<br /><br />One thing is certain: Taupin must have written something truly good to have inspired at least one commendable effort.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 4:\n",
            "\n",
            "Up until this new season I have been a big 'Little Mosque' fan. However, the new season had absolutely RUINED it.<br /><br />The new Christian vicar has destroyed the entire intent of the show. It has always been about living together to overcome prejudice. The new vicar ruins that premise and shows Christians in a very bad light.<br /><br />I am neither Christian or Muslim, but loved watching the show and seeing the camaraderie between Amar and the Reverend. Not any more.<br /><br />Just cancel it and be done with it. It's not worth watching any more.<br /><br />It might still be saved, but a lot of change would need to be made.<br /><br />Bring back the old format.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 5:\n",
            "\n",
            "Without wishing to be a killjoy, Brad Sykes is responsible for at least two of the most dull and clichéd films i've ever seen - this being one of them, and Camp Blood being another. <br /><br />The acting is terrible, the print is shoddy, and everything about this film screams \"seriously, you could do better yourself\". Maybe this is a challenge to everyone to saturate youtube with our own zombie related crap?<br /><br />I bought this for £1, but remember, you can't put a price on 71 minutes of your life. You'd do well to avoid this turkey, even at a bargain basement price.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 6:\n",
            "\n",
            "This can be one of the most enjoyable movies ever if you don't take it seriously. It is a bit dated and the effects are lame, but it is so enjoyable. There are giant crabs that attack a girl. oh, and the crabs sing Japanese. It is amazingly bad. And the ending, which has been telegraphed throughout the entire film is hideously awesome. Predictable, but seeing the final fight will leave you rolling in your seat. Don't even give this film a chance and you will love it. Susan George is fun to watch and yes, she does appear naked. Her daughter isn't quite worth putting up with, but she does get attacked by giant crabs. They are the size of large cats. This is a 2, but I love it. As a movie, my God, but for entertainment, I give it a 7. Did I mention there are giant crabs?\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 7:\n",
            "\n",
            "We watched this in my Women's Health Issues class to point out how women are treated inferior to men in many societies, and I absolutely loved this movie. I plan on trying to get a copy of it myself to watch. The story is very touching and I would recommend it to anyone. I am a fan of different cultures and this movie was just what I needed. This is a movie for the whole family despite its rating. This is a movie I will show to my children. The professor of our class meant for the movie to primarily be a too to educate about women, but this movie was more than that. It is one of those movies that will forever stick out in my mind and will be a favorite.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Number of samples to be printed\n",
        "n_samples = 7\n",
        "\n",
        "# Randomly sample indices from \"data_train\"\n",
        "sample_indices = random.sample(range(len(data_train)), n_samples)\n",
        "\n",
        "# Load and print each sample using the \"load_text\" function\n",
        "for i, id in enumerate(sample_indices, start=1):\n",
        "    # Load text from the file path specified in 'path' column\n",
        "    text = load_text(data_train.loc[id, 'path'])\n",
        "    print(f\"Sample {i}:\\n\")\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWw3AZ8SumVn"
      },
      "source": [
        "## Limitations/Issues Captured from the Above Text Samples\n",
        "\n",
        "- **HTML tags and special characters**\n",
        "- **Punctuation and symbols** (e.g., `&`)\n",
        "- **Contractions** (e.g., \"isn't\", \"I'll\", \"I'm\")\n",
        "- **Parentheses and annotations** (e.g., \"(Crouching Tiger)\")\n",
        "- **Informal formatting** (e.g., \"my rating is ****\")\n",
        "- **Ambiguity and polysemy** (e.g., \"dictators\", \"nuts\")\n",
        "- **Long and complex sentences**\n",
        "- **Informal language** (e.g., \"what can be so bad about that?\")\n",
        "- **Quotation marks** (e.g., \"dictators\", \"sin\")\n",
        "\n",
        "We will elaborate on these issues and add our conclusions in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "666GKq5Cug-Z"
      },
      "source": [
        "### 2. Preprocessing: Simplify the Text\n",
        "\n",
        "This section focuses on preprocessing the texts to make them more suitable for text mining. Identified constraints from the analysis step will be addressed and, as far as possible, eliminated to improve processing accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Zgm5pNuGJNM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    # Regex to match HTML tags\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZR8XyEHiGb94"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation_and_symbols(text):\n",
        "    # Remove punctuation and special symbols using regex\n",
        "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eYMTaZhZHOlZ"
      },
      "outputs": [],
      "source": [
        "def remove_parentheses(text):\n",
        "    # Remove text inside parentheses along with parentheses\n",
        "    clean_text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9PoskZmug-b"
      },
      "source": [
        "#### Expand the contractions\n",
        "There are python libraries that focus on expanding contractions. For easier loading (if these packages are not available) we decided to create our own replacement list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NaTBJ0qfG5uy"
      },
      "outputs": [],
      "source": [
        "# Dictionary of common contractions and their expanded forms\n",
        "contractions_dict = {\n",
        "    \"i'm\": \"I am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"i've\": \"I have\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"i'll\": \"I will\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"here's\": \"here is\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"shan't\": \"shall not\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b', flags=re.IGNORECASE)\n",
        "    def replace(match):\n",
        "        return contractions_dict.get(match.group(0).lower(), match.group(0))\n",
        "    return contractions_pattern.sub(replace, text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zrIoCD2IHqLy"
      },
      "outputs": [],
      "source": [
        "# Define a set of informal tokens to remove\n",
        "informal_tokens = {\n",
        "    \"u\": \"you\",\n",
        "    \"r\": \"are\",\n",
        "    \"lmao\": \"\",  # Remove\n",
        "    \"lol\": \"\",   # Remove\n",
        "    \"btw\": \"by the way\",\n",
        "    \"idk\": \"I do not know\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"gonna\": \"going to\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"gotta\": \"got to\",\n",
        "    \"b/c\": \"because\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"pls\": \"please\",\n",
        "    \"cuz\": \"because\",\n",
        "    \"wut\": \"what\",\n",
        "    \"smh\": \"\",  # Remove\n",
        "    \"k\": \"okay\",\n",
        "    \"ttyl\": \"talk to you later\"\n",
        "}\n",
        "\n",
        "def remove_informal_tokens(text):\n",
        "    # Replace informal tokens\n",
        "    for token, replacement in informal_tokens.items():\n",
        "        # Use regex to match whole words and replace them\n",
        "        text = re.sub(r'\\b' + re.escape(token) + r'\\b', replacement, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def fix_punctuation(text):\n",
        "    # Ensure proper spacing after punctuation\n",
        "    text = re.sub(r'\\s*([.,;:!?()])\\s*', r'\\1 ', text)  # Ensure space after punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = text.strip()  # Trim leading and trailing spaces\n",
        "    return text\n",
        "\n",
        "# Combined function to clean the text\n",
        "def clean_text(text):\n",
        "    text = remove_informal_tokens(text)  # Remove informal tokens\n",
        "    text = fix_punctuation(text)  # Fix punctuation\n",
        "    return text\n",
        "\n",
        "def preprocess_ellipses(text):\n",
        "    # Replace occurrences of three or more dots with actual ellipses\n",
        "    text = re.sub(r'(\\.\\s*){3,}', '...', text)\n",
        "    return text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81A7y_1sug-c",
        "outputId": "34d3e3d4-c1a0-4a0b-b421-28cfe52439f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kentf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1:\n",
            "\n",
            "This one worst films I ever seen. I looked mainly morbid curiosity since I loved novel, I wish I not. I turned little less hour, though I wanted turn five minutes. I wish I had. It disregards novel lot changes sorts factors. Unless film managed redeem last 50 minutes( which would impossible) I would way recommend this. Its insult one greatest writers 20th century. I think, many people say is, \"The Bell Jar\" necessarily unfilmable, particular rendition could done without. I'd almost like see one day hands director screenwriter justice.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 2:\n",
            "\n",
            "This film renewed interest French cinema. The story enchanting, acting flawless Audrey Tautou absolutely beautiful. I imagine seeing lot States upcoming role Amelie.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 3:\n",
            "\n",
            "Superman II - The Richard Donner Cut fan's dream come true. At long last, footage seen photos scenes existed printed page would finally come life. A director unable complete vision would opportunity vision restored. It seems like winning situation. And start watching assembly footage realize \"esoteric dream\" real nightmare sloppiness incompetence. While entirely possible movie could compete finished perfect version us imagined years really thrill finally see project. And times. You know things shaky first bit text screen looks like home brew computer graphics. But start seeing new footage( alternates Superman - The Movie trial) first bit hesitation fades away. Hey, pretty neat! Things alright fleeting moments see footage STM intermixed new effects project, convince all. And point on, never ever lets up. probably right judge movie bad visual effects, supposedly direct follow movie whose tag line \"you Believe A Man Can Fly\" difficult believe anything shown screen here. The best effects original productions. Another issue re-cut. A lot make sense. The reason really works seen theatrical version Superman II, movie make sense. Lester's Superman II fills holes assembly. Part could Donner get complete shooting, part could makers project intent using little Lester material possible. What end assembly footage makes Superman IV look airtight coherent. After viewing this, one gets sense Lester faithful comfortable using Donner material, Michael Thau team extremely disrespectful towards anything filmed Lester. The best scenes The Donner Cut ones lifted relatively intact released version Superman II. That includes moon sequence diner sequence, ironically, filmed Donner. But anything else movie filmed Lester re-edited hasty fashion, makes Lester seem like ham fisted know nothing. While Lester honored Donner material, Lester thrown bus. So anything good release? Well Marlon Brando it, neat see. In fact watching material shot Donner neat since filmed time Superman - The Movie. But highlights problems release. Any major scenes( really Lois jumping scenes Marlon Brando) would better served completed scenes deleted scenes section. Instead shoe horned nonsensical narrative inferior performances( many alternate takes familiar scenes used) sloppy edits bad decisions. Watch opening scene Daily Planet. Why looking Jackie Cooper's back calls Lois Clark? At end Lois walking dark apartment followed Jackie Cooper walking dark bathroom turning lights on? I initially confused this, I expected see Lois. The entire assembly filled questionable choices like this. Battle scenes mess too, geography cuts. random action. Of course, major action scenes shot Lester material used bridge next set Donner outtakes alternates. They used Lester's footage, probably much pride admit that. The sloppiness extends military missile well. As noted elsewhere, missile shown The Donner Cut bears designation \"XK 10\" know \"XK 101\"! A blind man STM knows that! The producers assembly, tried hard honor original film, dropped ball less five minutes mistake indicative quality entire production. For supposed care put this, final product air shoddiness inescapable. The entire affair would probably easier digest Warner's make separate release states. As is, expected pay essentially bonus disc deleted scenes \"Play All\" option. really worth one viewing finally see legendary cut scenes, initial viewing, I expect excellent magnet dust little else. I know experience watching this, I new respect Lester's version. means perfect, Lester realized deficiencies script stand bold relief. He managed make movie entertained many years continue so, new re-cut likely remembered footnote films history.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 4:\n",
            "\n",
            "The unlikely duo Zero Mostel Harry Belafonte team give us interesting performances subject matter The Angel Levine. one interesting twist themes A Wonderful Life. Zero married Ida Kaminsky two belong special class elderly Jewish poor New York. Mostel used tailor proud trade, back arthritis prevented working. Kaminsky mostly bedridden. reduced applying welfare. In desperation like Jimmy Stewart, cries God help. Now maybe gotten someone like Henry Travers things might worked differently, even Stewart trouble accepting Travers. But Travers one thing going him, 100 years mortal coil ties earthly things gone. God sent Mostel something quite different, recently deceased Harry Belafonte least given basic training angels given assignment. Belafonte accepted moved life, still got lot issues. He also wife, Gloria Foster, know passed on, hit car right beginning film. You put issues Mostel's issues got good conflict, starting fact Mostel cannot believe black Jew named Levine. This farewell performance Polish/Jewish actress Ida Kaminsky got nomination Best Actress The Shop Main Street years back. The prominent role Irish actor Milo O'Shea playing nice Jewish doctor. Remembering O'Shea's brogue The Verdict, I really surprised see hear carry part doctor. The Angel Levine raises interesting disturbing questions faith race society. brought stellar cast course created acclaimed writer Bernard Malamud. Make sure catch broadcast.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 5:\n",
            "\n",
            "The film opens peaceful shot traditional Japanese house complete thatched roof sits side small hill screen caption appears reads 'KUSHIATA KYOTO, JAPAN 1840'. A young Japanese trainee Samurai named Masanori( Toshiya Maruyama) walks winding path house, inside waits Otami( Mako Hattori) affair behind husband, Shugoro's( Tsuiyuki Sasaki Toshiyuki Sasaki) back happens Masanori's teacher. Shugoro unexpectedly arrives home find wife student intimate relations other. His honour destroyed enraged Samurai brutally murders Otami Masanori committing suicide. Over century later Ted Fletcher( Edward Albert) arrives Japan working holiday wife Laura( Susan George) young daughter Amy( Amy Barrett) . Their close friend Alex Curtis( Doug McClure) works American consulate helps finding place stay, need where! He jokingly says going cheap haunted, Ted Laura laugh obviously believe ghosts, least time is. Almost immediately film goes cliché mode. Lights turn themselves, Laura uneasy feeling place local Zen Monk( Henry Mitowa) gives ominous warning leave late ignore, course. The spirits Otami, Masanori Shugoro doomed eternity remain within walls house Majyo witches( Tsuyako Olajima) curse put upon them. But may way break curse, unfortunately Fletcher family could potentially cost marriage, daughter possibly even lives. Directed Kevin Connor I thought pretty average film, OK watch got nothing better day two probably completely forgotten it. Nothing sticks memory particularly bad hand nothing particularly good film either. The script Robert Suhosky novel James Hardiman little dull side strictly by-the-numbers, lot ghostly goings happen throughout film none interesting exciting flat characters direction help things. There couple silly sequences like giant plastic crabs try get Amy babysitter, Noriko( Mayumi Umeda) . And scene Zen Monk exorcises house ghosts banished outside unable get back in, however Ted simply opens door walk right back in, exorcism! One thing, I think bad idea Doug McClure 47 made this, try hand Kung-Fu oriental sword fighting! George gets ample breasts couple times including unerotic sex scene McClure, although great pains couple bed sheets stick like super glue taken ensure waist nudity present. Apart couple mostly screen decapitations blood, gore violence speak about. The 'transparent' ghost effects OK ain't going impress many people days. professionally enough made looks quite nice potential Japanese setting myths squandered film could set America, England Western country without change thing. An OK time waster.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = load_text(text)              # Load text from file path\n",
        "    ##################################################################################################################\n",
        "    ### Comment the following points until return out, to simulate the bare processing without normalization steps ###\n",
        "    ##################################################################################################################\n",
        "    text = expand_contractions(text)    # Expand contractions\n",
        "    text = remove_informal_tokens(text) # Replace informal tokens\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words) # Remove stopwords\n",
        "    text = fix_punctuation(text)        # Fix punctuation\n",
        "    text = preprocess_ellipses(text)    # Handle ellipses as a single token\n",
        "    text = re.sub(r'<.*?>', '', text)   # Remove HTML tags\n",
        "    return text\n",
        "\n",
        "\n",
        "# Apply preprocessing function to training and test data\n",
        "data_train['cleaned_text'] = data_train['path'].apply(preprocess_text)\n",
        "data_test['cleaned_text'] = data_test['path'].apply(preprocess_text)\n",
        "\n",
        "# Number of samples to be printed\n",
        "n_samples = 5\n",
        "\n",
        "# Randomly sample indices from \"data_train\"\n",
        "sample_indices = random.sample(range(len(data_train)), n_samples)\n",
        "\n",
        "# Print each sample from the cleaned_text\n",
        "for i, idx in enumerate(sample_indices, start=1):\n",
        "    # Access the cleaned text directly\n",
        "    text = data_train.loc[idx, 'cleaned_text']\n",
        "    print(f\"Sample {i}:\\n\")\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHLiIMDifQTI"
      },
      "source": [
        "### A simple pipeline:\n",
        "[Do not change it!]\n",
        "\n",
        "This simple pipeline will be used to compare the newly created pipeline against, to evaluate the performance increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOYzJ1-SfQTI"
      },
      "source": [
        "**White-Space tokenization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GJjcIUkHfQTI"
      },
      "outputs": [],
      "source": [
        "def tokenize(text:str):\n",
        "    ''' An example tokenization function. '''\n",
        "\n",
        "    # simple white-space tokenization:\n",
        "    return text.lower().split()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GddqcHfvfQTJ"
      },
      "source": [
        "**Bag-of-words Embedding:**\n",
        "\n",
        "See documentation of [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SGnHx6ThfQTJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create a simple bag of words embedding:\n",
        "bow = CountVectorizer(\n",
        "\n",
        "    # the next line converts the filepaths to the actual texts:\n",
        "    #preprocessor = load_text, # Commented, to check if the preprocessing from above changes the results\n",
        "\n",
        "    # tokenization function from above:\n",
        "    tokenizer = tokenize,\n",
        "\n",
        "    # Set token_pattern to None since we're using a custom tokenizer\n",
        "    token_pattern=None\n",
        "\n",
        ")\n",
        "\n",
        "# Train the embedding on cleaned training data\n",
        "embeddings_train = bow.fit_transform(data_train['cleaned_text'].values)\n",
        "\n",
        "# Vectorize the cleaned test data\n",
        "embeddings_test = bow.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "# These are the original lines\n",
        "# train the embedding:\n",
        "#embeddings_train = bow.fit_transform(data_train['path'].values)\n",
        "\n",
        "# vectorize test data:\n",
        "#embeddings_test = bow.transform(data_test['path'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qbhJp3tfQTJ"
      },
      "source": [
        "**Classification with a linear SVM**\n",
        "\n",
        "See documentation of [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUb6hTYVfQTJ",
        "outputId": "141d9bb7-db67-48e7-aeb1-ed8d4e82dd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.792\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# train classifier:\n",
        "svm.fit(embeddings_train, data_train['label'].values)\n",
        "\n",
        "# test classifier:\n",
        "predictions = svm.predict(embeddings_test)\n",
        "\n",
        "# Calculate Accuracy:\n",
        "print('Accuracy:', accuracy_score(data_test['label'].values, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7n0tlJZug-j"
      },
      "source": [
        "### Own text mining pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9AfjxXX1AQf"
      },
      "source": [
        "As the text was previously normalized / or not in the other case, the next step is to tokenize the text.\n",
        "For this, we are employing the tokenizer with BPE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2LtJwsVeG6",
        "outputId": "9aab73cf-43b6-4f89-df74-98296e8a2179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1 Original string: Bromwell High cartoon comedy. It ran time programs school life, \"Teachers\". My 35 years teaching profession lead believe Bromwell High's satire much closer reality \"Teachers\". The scramble survive financially, insightful students see right pathetic teachers' pomp, pettiness whole situation, remind schools I knew students. When I saw episode student repeatedly tried burn school, I immediately recalled...High. A classic line: INSPECTOR: I sack one teachers. STUDENT: Welcome Bromwell High. I expect many adults age think Bromwell High far fetched. What pity not!\n",
            "Sample 1 Encoded tokens (token IDs): [9284, 2818, 2984, 834, 18, 278, 2906, 272, 11627, 1693, 431, 16, 6, 13956, 751, 919, 6442, 509, 14376, 6967, 689, 773, 9284, 2818, 11, 82, 8960, 344, 7050, 2554, 6, 13956, 751, 157, 26485, 4983, 11223, 16, 9165, 4904, 263, 511, 2502, 9156, 11, 8109, 16, 21461, 837, 1726, 16, 1777, 13916, 43, 2656, 4904, 18, 908, 43, 633, 698, 4305, 6370, 2094, 2258, 1693, 16, 43, 3873, 12474, 289, 2818, 18, 35, 1353, 675, 30, 26720, 30, 43, 12345, 165, 9156, 18, 15373, 30, 15564, 9284, 2818, 18, 43, 583, 417, 2922, 347, 321, 9284, 2818, 688, 9384, 18, 636, 5434, 338, 5]\n",
            "Sample 1 Tokenized string (tokens): ['Bromwell', 'High', 'cartoon', 'comedy', '.', 'It', 'ran', 'time', 'programs', 'school', 'life', ',', '\"', 'Teachers', '\".', 'My', '35', 'years', 'teaching', 'profession', 'lead', 'believe', 'Bromwell', 'High', \"'\", 's', 'satire', 'much', 'closer', 'reality', '\"', 'Teachers', '\".', 'The', 'scramble', 'survive', 'financially', ',', 'insightful', 'students', 'see', 'right', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'pettiness', 'whole', 'situation', ',', 'remind', 'schools', 'I', 'knew', 'students', '.', 'When', 'I', 'saw', 'episode', 'student', 'repeatedly', 'tried', 'burn', 'school', ',', 'I', 'immediately', 'recalled', '...', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', 'sack', 'one', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'Bromwell', 'High', '.', 'I', 'expect', 'many', 'adults', 'age', 'think', 'Bromwell', 'High', 'far', 'fetched', '.', 'What', 'pity', 'not', '!']\n",
            "Sample 1 Decoded string: Bromwell High cartoon comedy . It ran time programs school life , \" Teachers \". My 35 years teaching profession lead believe Bromwell High ' s satire much closer reality \" Teachers \". The scramble survive financially , insightful students see right pathetic teachers ' pomp , pettiness whole situation , remind schools I knew students . When I saw episode student repeatedly tried burn school , I immediately recalled ... High . A classic line : INSPECTOR : I sack one teachers . STUDENT : Welcome Bromwell High . I expect many adults age think Bromwell High far fetched . What pity not !\n",
            "================================================================================\n",
            "Sample 2 Original string: Yes art...successfully make slow paced thriller. The story unfolds nice volumes even notice happening. Fine performance Robin Williams. The sexuality angles film seem unnecessary probably affect much enjoy film. However, core plot engaging. The movie rush onto still grips enough keep wondering. The direction good. Use lights achieve desired affects suspense unexpectedness good. Very nice 1 time watch looking lay back hear thrilling short story!\n",
            "Sample 2 Encoded tokens (token IDs): [1875, 213, 289, 5238, 379, 1365, 3393, 1785, 18, 157, 259, 6364, 1026, 20216, 275, 2859, 3633, 18, 9567, 836, 2242, 1454, 18, 157, 5274, 6773, 164, 433, 4757, 780, 3458, 344, 562, 164, 18, 1071, 16, 2522, 424, 3068, 18, 157, 162, 6783, 3147, 518, 8028, 711, 924, 3316, 18, 157, 1291, 251, 18, 9722, 5607, 5727, 6787, 9166, 1635, 22100, 251, 18, 3719, 1026, 21, 272, 268, 1066, 3549, 423, 2409, 6886, 1098, 259, 5]\n",
            "Sample 2 Tokenized string (tokens): ['Yes', 'art', '...', 'successfully', 'make', 'slow', 'paced', 'thriller', '.', 'The', 'story', 'unfolds', 'nice', 'volumes', 'even', 'notice', 'happening', '.', 'Fine', 'performance', 'Robin', 'Williams', '.', 'The', 'sexuality', 'angles', 'film', 'seem', 'unnecessary', 'probably', 'affect', 'much', 'enjoy', 'film', '.', 'However', ',', 'core', 'plot', 'engaging', '.', 'The', 'movie', 'rush', 'onto', 'still', 'grips', 'enough', 'keep', 'wondering', '.', 'The', 'direction', 'good', '.', 'Use', 'lights', 'achieve', 'desired', 'affects', 'suspense', 'unexpectedness', 'good', '.', 'Very', 'nice', '1', 'time', 'watch', 'looking', 'lay', 'back', 'hear', 'thrilling', 'short', 'story', '!']\n",
            "Sample 2 Decoded string: Yes art ... successfully make slow paced thriller . The story unfolds nice volumes even notice happening . Fine performance Robin Williams . The sexuality angles film seem unnecessary probably affect much enjoy film . However , core plot engaging . The movie rush onto still grips enough keep wondering . The direction good . Use lights achieve desired affects suspense unexpectedness good . Very nice 1 time watch looking lay back hear thrilling short story !\n",
            "================================================================================\n",
            "Sample 3 Original string: There many illnesses born mind man given life modern times. Constant vigilance accrued information realm Pyschosis, kept psychologists, counselors psychiatrists busy enough work last decades. Occasionally, mental phenomenon discover knowledge remedy even existence. That premise film entitled \" The Night Listner. \" It tells story popular radio host called Gabriel Noon( Robin Williams) spends evenings enthralling audiences vivid stories Gay lifestyles. Perhaps show losing authentic veneer causes Noon admit longer himself. Feeling abandoned lover Jess( Bobby Cannavale) best friend( Joe Morton) , seeks shelter deepening despair isolation. It here, mysterious voice night asks help. Noon needs feel useful reaches desperate voice belongs 14 year old boy called Peter( Rory Culkin) . In reading boy's harrowing manuscript depicts early life sexual abuse hands brutal parents, Noon captivated wants help. However, things seem Noon soon finds en-wrapped elusive bizarre tale torn right medical nightmare. This movie pure Robin Williams Toni Collette plays Donna D. Logand, Sandra Oh Anna John Cullum pop, might comical. Instead, may prove one William's serious performances. ***\n",
            "Sample 3 Encoded tokens (token IDs): [582, 417, 24289, 6194, 786, 196, 1029, 431, 3102, 790, 18, 21084, 15018, 26349, 3935, 5580, 24501, 16, 1816, 22121, 16, 25394, 22800, 21600, 711, 396, 703, 6823, 18, 25907, 16, 6743, 8073, 1931, 4502, 13272, 275, 3493, 18, 1085, 2366, 164, 8080, 6, 157, 1946, 22507, 18, 6, 278, 1560, 259, 2496, 3488, 2453, 1091, 4076, 6856, 12, 2242, 1454, 13, 6738, 18919, 24244, 2782, 5256, 1514, 9575, 19518, 18, 3483, 303, 5264, 6729, 19980, 4885, 6856, 1831, 3196, 3066, 18, 22819, 9347, 3046, 2517, 12, 4363, 5787, 13, 457, 592, 12, 2778, 4674, 13, 16, 7551, 23957, 21290, 8725, 14856, 18, 278, 450, 16, 4905, 1865, 981, 1925, 934, 18, 6856, 1935, 526, 7704, 7888, 2798, 1865, 6337, 2891, 384, 368, 827, 1091, 2068, 12, 5389, 4954, 13, 18, 339, 2140, 827, 11, 82, 15010, 14950, 9051, 1769, 431, 2426, 5132, 3063, 3869, 2015, 16, 6856, 10695, 1652, 934, 18, 1071, 16, 627, 433, 6856, 1768, 2486, 116, 17, 6133, 12615, 4337, 2187, 6374, 511, 7677, 8865, 18, 257, 162, 1993, 2242, 1454, 4631, 3088, 931, 4883, 38, 18, 9307, 16, 4318, 1605, 6118, 779, 15443, 1122, 16, 640, 7485, 18, 3048, 16, 504, 3315, 165, 3828, 11, 82, 1008, 1126, 18, 3056]\n",
            "Sample 3 Tokenized string (tokens): ['There', 'many', 'illnesses', 'born', 'mind', 'man', 'given', 'life', 'modern', 'times', '.', 'Constant', 'vigilance', 'accrued', 'information', 'realm', 'Pyschosis', ',', 'kept', 'psychologists', ',', 'counselors', 'psychiatrists', 'busy', 'enough', 'work', 'last', 'decades', '.', 'Occasionally', ',', 'mental', 'phenomenon', 'discover', 'knowledge', 'remedy', 'even', 'existence', '.', 'That', 'premise', 'film', 'entitled', '\"', 'The', 'Night', 'Listner', '.', '\"', 'It', 'tells', 'story', 'popular', 'radio', 'host', 'called', 'Gabriel', 'Noon', '(', 'Robin', 'Williams', ')', 'spends', 'evenings', 'enthralling', 'audiences', 'vivid', 'stories', 'Gay', 'lifestyles', '.', 'Perhaps', 'show', 'losing', 'authentic', 'veneer', 'causes', 'Noon', 'admit', 'longer', 'himself', '.', 'Feeling', 'abandoned', 'lover', 'Jess', '(', 'Bobby', 'Cannavale', ')', 'best', 'friend', '(', 'Joe', 'Morton', ')', ',', 'seeks', 'shelter', 'deepening', 'despair', 'isolation', '.', 'It', 'here', ',', 'mysterious', 'voice', 'night', 'asks', 'help', '.', 'Noon', 'needs', 'feel', 'useful', 'reaches', 'desperate', 'voice', 'belongs', '14', 'year', 'old', 'boy', 'called', 'Peter', '(', 'Rory', 'Culkin', ')', '.', 'In', 'reading', 'boy', \"'\", 's', 'harrowing', 'manuscript', 'depicts', 'early', 'life', 'sexual', 'abuse', 'hands', 'brutal', 'parents', ',', 'Noon', 'captivated', 'wants', 'help', '.', 'However', ',', 'things', 'seem', 'Noon', 'soon', 'finds', 'en', '-', 'wrapped', 'elusive', 'bizarre', 'tale', 'torn', 'right', 'medical', 'nightmare', '.', 'This', 'movie', 'pure', 'Robin', 'Williams', 'Toni', 'Collette', 'plays', 'Donna', 'D', '.', 'Logand', ',', 'Sandra', 'Oh', 'Anna', 'John', 'Cullum', 'pop', ',', 'might', 'comical', '.', 'Instead', ',', 'may', 'prove', 'one', 'William', \"'\", 's', 'serious', 'performances', '.', '***']\n",
            "Sample 3 Decoded string: There many illnesses born mind man given life modern times . Constant vigilance accrued information realm Pyschosis , kept psychologists , counselors psychiatrists busy enough work last decades . Occasionally , mental phenomenon discover knowledge remedy even existence . That premise film entitled \" The Night Listner . \" It tells story popular radio host called Gabriel Noon ( Robin Williams ) spends evenings enthralling audiences vivid stories Gay lifestyles . Perhaps show losing authentic veneer causes Noon admit longer himself . Feeling abandoned lover Jess ( Bobby Cannavale ) best friend ( Joe Morton ) , seeks shelter deepening despair isolation . It here , mysterious voice night asks help . Noon needs feel useful reaches desperate voice belongs 14 year old boy called Peter ( Rory Culkin ) . In reading boy ' s harrowing manuscript depicts early life sexual abuse hands brutal parents , Noon captivated wants help . However , things seem Noon soon finds en - wrapped elusive bizarre tale torn right medical nightmare . This movie pure Robin Williams Toni Collette plays Donna D . Logand , Sandra Oh Anna John Cullum pop , might comical . Instead , may prove one William ' s serious performances . ***\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Add a pre-tokenizer to handle whitespace properly\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize a BPE trainer with some default parameters\n",
        "trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"__ellipsis__\"])\n",
        "\n",
        "# Convert cleaned_text column to a list of strings for training\n",
        "training_data = data_train['cleaned_text'].tolist()\n",
        "\n",
        "# Train the tokenizer on the cleaned text data\n",
        "tokenizer.train_from_iterator(training_data, trainer)\n",
        "\n",
        "# Save the tokenizer for later use\n",
        "tokenizer.save(\"imdb_bpe_tokenizer.json\")\n",
        "\n",
        "# Encode the text data for the model\n",
        "encoded_train = [tokenizer.encode(text).ids for text in data_train['cleaned_text']]\n",
        "encoded_test = [tokenizer.encode(text).ids for text in data_test['cleaned_text']]\n",
        "\n",
        "# Select a single example to decode and print\n",
        "sample_index = 0  # Choose an index, e.g., 0 for the first item\n",
        "encoded_sample = encoded_train[sample_index]\n",
        "\n",
        "# Decode the tokenized output back to text\n",
        "decoded_text = tokenizer.decode(encoded_sample)\n",
        "\n",
        "# Select additional examples to decode and print\n",
        "sample_indices = [0, 6, 12]  # You can adjust these indices as needed\n",
        "\n",
        "for i, sample_index in enumerate(sample_indices):\n",
        "    encoded_sample = tokenizer.encode(data_train['cleaned_text'][sample_index])\n",
        "    decoded_text = tokenizer.decode(encoded_sample.ids)\n",
        "    \n",
        "    print(f\"Sample {i+1} Original string:\", data_train['cleaned_text'][sample_index])  # Original text for comparison\n",
        "    print(f\"Sample {i+1} Encoded tokens (token IDs):\", encoded_sample.ids)  # Encoded token IDs\n",
        "    print(f\"Sample {i+1} Tokenized string (tokens):\", encoded_sample.tokens)  # Encoded tokens as text\n",
        "    print(f\"Sample {i+1} Decoded string:\", decoded_text)  # Decoded string\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Analysis of the results\n",
        "\n",
        "To be discussed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Training Data Shape: (1000, 15000)\n",
            "TF-IDF Test Data Shape: (1000, 15000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer with desired parameters\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=15000,    \n",
        "    ngram_range=(1, 2),\n",
        ")\n",
        "\n",
        "# Fit on training data and transform both training and test data\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(data_train['cleaned_text'])\n",
        "tfidf_test = tfidf_vectorizer.transform(data_test['cleaned_text'])\n",
        "\n",
        "# Print the shapes of the resulting TF-IDF matrices\n",
        "print(\"TF-IDF Training Data Shape:\", tfidf_train.shape)\n",
        "print(\"TF-IDF Test Data Shape:\", tfidf_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.819\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.81      0.83      0.82       500\n",
            "         pos       0.83      0.80      0.82       500\n",
            "\n",
            "    accuracy                           0.82      1000\n",
            "   macro avg       0.82      0.82      0.82      1000\n",
            "weighted avg       0.82      0.82      0.82      1000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\kentf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize and train the SVM classifier\n",
        "svm = LinearSVC()\n",
        "svm.fit(tfidf_train, data_train['label'])  # 'label' is your target variable\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = svm.predict(tfidf_test)\n",
        "\n",
        "# Evaluate the classifier's performance\n",
        "accuracy = accuracy_score(data_test['label'], predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(data_test['label'], predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Try another Predicition Model on the same tokenized (and normalized) data.\n",
        "\n",
        "A neural network is trained with the same data as above to see if that might perform better on the same preprocessed vectors than the svm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cpu\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)          # To confirm the version\n",
        "print(torch.cuda.is_available())  # This should return False as there is no GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map the labels to integers\n",
        "label_mapping = {'pos': 1, 'neg': 0}\n",
        "data_train['label'] = data_train['label'].map(label_mapping)\n",
        "data_test['label'] = data_test['label'].map(label_mapping)\n",
        "\n",
        "# Now convert to tensor\n",
        "y_train_tensor = torch.tensor(data_train['label'].values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(data_test['label'].values, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2, Loss: 0.6120333722792566\n",
            "Epoch 2/2, Loss: 0.06782460274916957\n",
            "Test Accuracy: 0.813\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.83      0.82       500\n",
            "           1       0.82      0.80      0.81       500\n",
            "\n",
            "    accuracy                           0.81      1000\n",
            "   macro avg       0.81      0.81      0.81      1000\n",
            "weighted avg       0.81      0.81      0.81      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert to dense tensors\n",
        "X_train_tensor = torch.tensor(tfidf_train.toarray(), dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(tfidf_test.toarray(), dtype=torch.float32)\n",
        "\n",
        "# Labels as tensors\n",
        "y_train_tensor = torch.tensor(data_train['label'].values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(data_test['label'].values, dtype=torch.long)\n",
        "\n",
        "# Define the Neural Network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = SimpleNN(input_size)\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "# Evaluation on Test Data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(test_outputs, 1)\n",
        "    accuracy = accuracy_score(y_test_tensor, predicted)\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test_tensor, predicted))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
