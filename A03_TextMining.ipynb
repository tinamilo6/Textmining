{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinamilo6/Textmining/blob/main/A03_TextMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X1D-_ww7fQTB"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas sklearn nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS1_jLQofQTD"
      },
      "source": [
        "# Assignment 3 - Text Mining\n",
        "\n",
        "Project management and tools for health informatics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufi0zQidfQTE"
      },
      "source": [
        "## 1. Download and prepare data:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "The code in this section downloads the [IMDB IMDB Large Movie Review Dataset]('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz') which is the dataset you will be working on in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aOzKCiw4fQTE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sq9rCBBrfQTE"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('aclImdb'):\n",
        "    # download data:\n",
        "    urlretrieve('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', 'aclImdb.tar.gz')\n",
        "\n",
        "    # unzip data:\n",
        "    with tarfile.open('aclImdb.tar.gz') as file:\n",
        "        file.extractall('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9pzG__QfQTF"
      },
      "source": [
        "## 2. Some helper Functions:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "This section contains the code for some helper functions that will be useful for solving the assignment. Example code on how to use the functions is provided in section 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LaM3GoopfQTF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from typing import Literal, Tuple, Iterable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxsGe7MufQTF"
      },
      "source": [
        "Function for loading data into a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tKoAn698fQTG"
      },
      "outputs": [],
      "source": [
        "def load_data(split:Literal['train', 'test'], texts_per_class:int=500) -> pd.DataFrame:\n",
        "    ''' Loads the data into a pandas dataframe.'''\n",
        "    paths  = []\n",
        "    labels = []\n",
        "\n",
        "    for label in ('pos', 'neg'):\n",
        "        # get all files in the folder:\n",
        "        files = os.listdir(os.path.join('aclImdb', split, label))[:texts_per_class]\n",
        "\n",
        "        # append them to the lists:\n",
        "        paths.extend([os.path.join('aclImdb', split, label, f) for f in files])\n",
        "        labels.extend([label] * len(files))\n",
        "\n",
        "    return pd.DataFrame({'path':paths, 'label':labels})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C1jyAtfQTG"
      },
      "source": [
        "Function for loading a specific text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y0QGuZMXfQTH"
      },
      "outputs": [],
      "source": [
        "def load_text(path:str) -> str:\n",
        "    ''' Reads a single text given the path. '''\n",
        "    # read file from disk:\n",
        "    with open(path, 'r', encoding='utf8') as file:\n",
        "        s = file.read()\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR52KxrvfQTH"
      },
      "source": [
        "Function for iterating through multiple texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NImrB2NzfQTH"
      },
      "outputs": [],
      "source": [
        "def iterate_texts(data:pd.DataFrame) -> Iterable[Tuple[str, str]]:\n",
        "    ''' Iterates through a pandas dataframe. '''\n",
        "\n",
        "    for path in data['path'].values:\n",
        "        # read file from disk:\n",
        "        with open(path, 'r', encoding='utf8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        yield text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzYgen2AfQTH"
      },
      "source": [
        "## 3. Text Mining Pipeline\n",
        "\n",
        "This section will cover the text mining steps for this assignment. The following steps will be performed:\n",
        "\n",
        "1. **Analyze the Data for Difficult Parts**  \n",
        "   - Review the data to identify challenging aspects such as contractions, informal language, and complex sentence structures.\n",
        "\n",
        "2. **Replace Contractions and Informal Language**  \n",
        "   - Expand common contractions and replace informal phrases, if necessary, to standardize the text for processing.\n",
        "\n",
        "3. **Tokenize the Texts**  \n",
        "   - Apply a suitable tokenizer to break down the text into individual tokens for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2o7r5Bug-U"
      },
      "source": [
        "### Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "57LKdqhTug-V"
      },
      "outputs": [],
      "source": [
        "# Import needed libaries for the preparation of the texts\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3FnCEu6fQTH"
      },
      "source": [
        "### Load the training data\n",
        "The loaded data frpom the zip file should be saved into a `data_train` and `data_test` DataFrame.\n",
        "These can be further on be used to access the train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CeshtSl9fQTI",
        "outputId": "fa88fb56-74c9-47ca-d8fa-c3b9b6e19fe5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              path label\n",
              "0     aclImdb/train/pos/8252_9.txt   pos\n",
              "1     aclImdb/train/pos/3803_9.txt   pos\n",
              "2     aclImdb/train/pos/606_10.txt   pos\n",
              "3     aclImdb/train/pos/1865_8.txt   pos\n",
              "4    aclImdb/train/pos/10303_7.txt   pos\n",
              "..                             ...   ...\n",
              "995   aclImdb/train/neg/9307_1.txt   neg\n",
              "996   aclImdb/train/neg/3197_3.txt   neg\n",
              "997   aclImdb/train/neg/8804_4.txt   neg\n",
              "998    aclImdb/train/neg/588_2.txt   neg\n",
              "999   aclImdb/train/neg/8061_2.txt   neg\n",
              "\n",
              "[1000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-694a3085-f0df-4e5a-9887-8a16afc138be\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aclImdb/train/pos/8252_9.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aclImdb/train/pos/3803_9.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aclImdb/train/pos/606_10.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aclImdb/train/pos/1865_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aclImdb/train/pos/10303_7.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>aclImdb/train/neg/9307_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>aclImdb/train/neg/3197_3.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>aclImdb/train/neg/8804_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>aclImdb/train/neg/588_2.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>aclImdb/train/neg/8061_2.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-694a3085-f0df-4e5a-9887-8a16afc138be')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-694a3085-f0df-4e5a-9887-8a16afc138be button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-694a3085-f0df-4e5a-9887-8a16afc138be');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0fe63914-4a07-42be-b0f1-da85857265f3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0fe63914-4a07-42be-b0f1-da85857265f3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0fe63914-4a07-42be-b0f1-da85857265f3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6d521895-441e-45aa-9ca5-52e3575f899e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6d521895-441e-45aa-9ca5-52e3575f899e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_train",
              "summary": "{\n  \"name\": \"data_train\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"aclImdb/train/neg/1065_1.txt\",\n          \"aclImdb/train/neg/11005_1.txt\",\n          \"aclImdb/train/neg/11199_1.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neg\",\n          \"pos\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data_train = load_data('train')\n",
        "data_test  = load_data('test')\n",
        "data_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qXdudbDfQTI"
      },
      "source": [
        "### 1. Assess the data / texts for difficult parts\n",
        "In this part, sample texts are printed and then analyzed for diffult parts, that could affect the text mining process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soE8Qj9hsF0E",
        "outputId": "9f3f109e-dedd-4ec9-a9ce-6fb530a35fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "\n",
            "Saw a trailer for this on another video, and decided to rent when it came out. Boy, was I disappointed! The story is extremely boring, the acting (aside from Christopher Walken) is bad, and I couldn't care less about the characters, aside from really wanting to see Nora's husband get thrashed. Christopher Walken's role is such a throw-away, what a tease!\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 2:\n",
            "\n",
            "This very strange movie is unlike anything made in the west at the time. With its tumultuous emotions and net of visions, dreams, and startling images, its effect is both beautiful and unsettling. The actors are choreographed more like dance than acting. It contains the only dream sequence I know of that actually resembles a real nightmare (sorry, Dali fans).\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 3:\n",
            "\n",
            "The Dereks did seem to struggle to find rolls for Bo after \"10\".<br /><br />I used to work for a marine park in the Florida Keys. One day, the script for \"Ghosts Can't Do It\" was circulating among the trainers in the \"fish house\" where food was prepared for the dolphins. There was one scene where a -dolphin- supposedly propositions Bo (or Bo the dolphin), asking to \"go make eggs.\" Reading the script, we -lauuughed-...<br /><br />We did not end up doing any portion of this movie at our facility, although our dolphins -were- in \"The Big Blue!\"<br /><br />This must have been very close to the end of Anthony Quinn's life. I hope he had fun in this film, as it certainly didn't do anything for his legacy.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 4:\n",
            "\n",
            "Note: These comments are for people who have seen the movie.<br /><br />Vanilla Sky is a brilliant, complex, and thrilling movie that existentially explores exactly what the tag-line says: LoveHateDreamsLifeWorkPlayFriends. Maybe the movie plot can come into focus for confused movie-goers if one looks at it from a different angle.<br /><br />Considering the following:<br /><br />Now, I have not painstakingly gone through the film scene by scene, so I will have to further examine my assertions, (and I welcome your thoughts) but give this a try and see if the movie doesn't fall into place: Where exactly does the debatable 'splice' occur?<br /><br />Now, I'm not talking about the splice as it is explained by the L.E. 'technician', since that sequence itself could be actually interpreted as a rationalization inside of David Aames's mind/dream/coma state, but the true splice between reality and dream.<br /><br />It seems to me that the reality of the car crash, the way that it is filmed (no explosion, for example) is a likely 'splice' point, and that any particular sequence containing an existential/dream/coma/non-reality feel to it -- whether it's shown onscreen before or after the crash -- is actually a part of Aames' personal journey toward self-realization inside of his own mind.<br /><br />In that respect, then, we are left with two questions at the very end(if you know of more, let me know): is Aames actually disfigured, and where does he wake up?<br /><br />If you don't get entirely wrapped up in the exact sequence of details in the plot, or at what particular point his dreams are scattered throughout, this movie becomes a fascinating exploration of a human on a journey to find himself and what that means in today's pop-culture society.<br /><br />\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 5:\n",
            "\n",
            "I saw this at \"Dances with Films\", and it was awesome. I really felt for Jake. Talk about adding insult to injury! Not only are your parents getting divorced, but there's a monster after you. <br /><br />It was both heartfelt and scary -- there were several moments where the audience screamed in genuine fright. It kind of reminded me of a Japanese horror film, except that the story was actually good.<br /><br />And that's what separated \"Jake's Closet\" from the usual indy film pabulum -- an excellent script with compelling characters. Also, by mixing elements of the horror film with family drama, the movie gets the best out of both genres, and avoids the clichés of both.<br /><br />If it's not coming out in theaters, definitely get the DVD.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 6:\n",
            "\n",
            "Imagine pulling back the mask of a lethal assassin and finding Barbara Cartland there... that's what happens with this film.<br /><br />The opening showed promise, but soon it drops all pretenses of being a thriller (or even an imaginative love story) and the only reason they made this story becomes abundantly clear: to fill a gap in their female viewing market by creating yet another re-hash of 'mis-understood, brooding bad-boy' (Andrei) meets 'innocent, whimsical beauty' (Paula). <br /><br />Rather than waste any time in creating an original premise, the filmmakers went straight for the money-shot: the bad boy being tamed by said whimsical beauty. Thence follows a string of insincere and heavily-clichéd love scenes sprinkled with pseudo philosophical/poetic fluff. Andrei's admission of being (eponymously) a 'poet' is levered in to round out the perceived qualities a Byronic hero should have - but even when we're told in heavy, underlined writing who and what he is, it's still difficult to believe it - or care.<br /><br />For a Byronic hero/antihero to work, the story needs subtlety, style and innovation - all of which are utterly absent here. This is not a modern day Phantom of the Opera, it's just what happens when a weak and rather silly woman (with loose knicker elastic) dates a bad man, who, after meeting her, seems as dangerous as bunny slippers.<br /><br />The performances might have saved this film, had they been any good: the female lead is preoccupied with looking sexy and 'otherworldly', no matter how forced or ridiculous; and poor Dougray Scott appears to have been drugged as he shambles through his part. This is not his best work. The glimmers of interest were brought by Jürgen Prochnow as 'Vashon', and Andrew Lee Potts as the young photographer/brother. A better movie would have offed the sister and kept the brother instead.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 7:\n",
            "\n",
            "FORGET CREDIBILITY<br /><br />You must not expect credibility with action movies where the superhero has to perform an endless string of unbelievable feats, being trodden upon in the process but recovering at lightning speed, and transforming innocuous gadgets in lethal weapons... especially when Renny Harlin is directing.<br /><br />\"CLIFFHANGER \" is no exception. But the movie has numerous assets : breathtaking scenery gorgeously photographed, stunning special and visual effects ( the first five minutes are gripping and give the tone of the film ), excellent musical score, welcome attempts at levity to relieve some of the tension, and a solid cast : two heroes ( Stallone, star and cowriter, has the lion's share of the footage, but the excellent Michael Rooker more than stands his ground ), a charming heroin ( Janine Turner ), and one of the most darstardy bunch of villains ever ( priceless John Lithgow and deceivingly feminine Caroline Goodall, but also Rex Linn - in a longer than usual part and who makes the most of it, Leon, Craig Fairbrass ) Good, solid entertainment then , if no credibility.As Roger Ebert wrote ( about another film )\"It's the kind of movie you can sit back and enjoy as long as you don't make the mistake of thinking too much.\"<br /><br />\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Number of samples to be printed\n",
        "n_samples = 7\n",
        "\n",
        "# Randomly sample indices from \"data_train\"\n",
        "sample_indices = random.sample(range(len(data_train)), n_samples)\n",
        "\n",
        "# Load and print each sample using the \"load_text\" function\n",
        "for i, id in enumerate(sample_indices, start=1):\n",
        "    # Load text from the file path specified in 'path' column\n",
        "    text = load_text(data_train.loc[id, 'path'])\n",
        "    print(f\"Sample {i}:\\n\")\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWw3AZ8SumVn"
      },
      "source": [
        "## Limitations/Issues Captured from the Above Text Samples\n",
        "\n",
        "- **HTML tags and special characters**\n",
        "- **Punctuation and symbols** (e.g., `&`)\n",
        "- **Contractions** (e.g., \"isn't\", \"I'll\", \"I'm\")\n",
        "- **Parentheses and annotations** (e.g., \"(Crouching Tiger)\")\n",
        "- **Informal formatting** (e.g., \"my rating is ****\")\n",
        "- **Ambiguity and polysemy** (e.g., \"dictators\", \"nuts\")\n",
        "- **Long and complex sentences**\n",
        "- **Informal language** (e.g., \"what can be so bad about that?\")\n",
        "- **Quotation marks** (e.g., \"dictators\", \"sin\")\n",
        "\n",
        "We will elaborate on these issues and add our conclusions in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "666GKq5Cug-Z"
      },
      "source": [
        "### 2. Preprocessing: Simplify the Text\n",
        "\n",
        "This section focuses on preprocessing the texts to make them more suitable for text mining. Identified constraints from the analysis step will be addressed and, as far as possible, eliminated to improve processing accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Zgm5pNuGJNM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    # Regex to match HTML tags\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZR8XyEHiGb94"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation_and_symbols(text):\n",
        "    # Remove punctuation and special symbols using regex\n",
        "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eYMTaZhZHOlZ"
      },
      "outputs": [],
      "source": [
        "def remove_parentheses(text):\n",
        "    # Remove text inside parentheses along with parentheses\n",
        "    clean_text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9PoskZmug-b"
      },
      "source": [
        "#### Expand the contractions\n",
        "There are python libraries that focus on expanding contractions. For easier loading (if these packages are not available) we decided to create our own replacement list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NaTBJ0qfG5uy"
      },
      "outputs": [],
      "source": [
        "# Dictionary of common contractions and their expanded forms\n",
        "contractions_dict = {\n",
        "    \"i'm\": \"i am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"here's\": \"here is\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"shan't\": \"shall not\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b', flags=re.IGNORECASE)\n",
        "    def replace(match):\n",
        "        return contractions_dict.get(match.group(0).lower(), match.group(0))\n",
        "    return contractions_pattern.sub(replace, text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zrIoCD2IHqLy"
      },
      "outputs": [],
      "source": [
        "# Define a set of informal tokens to remove\n",
        "informal_tokens = {\n",
        "    \"u\": \"you\",\n",
        "    \"r\": \"are\",\n",
        "    \"lmao\": \"\",  # Remove\n",
        "    \"lol\": \"\",   # Remove\n",
        "    \"btw\": \"by the way\",\n",
        "    \"idk\": \"I do not know\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"gonna\": \"going to\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"gotta\": \"got to\",\n",
        "    \"b/c\": \"because\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"pls\": \"please\",\n",
        "    \"cuz\": \"because\",\n",
        "    \"wut\": \"what\",\n",
        "    \"smh\": \"\",  # Remove\n",
        "    \"k\": \"okay\",\n",
        "    \"ttyl\": \"talk to you later\"\n",
        "}\n",
        "\n",
        "def remove_informal_tokens(text):\n",
        "    # Replace informal tokens\n",
        "    for token, replacement in informal_tokens.items():\n",
        "        # Use regex to match whole words and replace them\n",
        "        text = re.sub(r'\\b' + re.escape(token) + r'\\b', replacement, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def fix_punctuation(text):\n",
        "    # Ensure proper spacing after punctuation\n",
        "    text = re.sub(r'\\s*([.,;:!?()])\\s*', r'\\1 ', text)  # Ensure space after punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = text.strip()  # Trim leading and trailing spaces\n",
        "    return text\n",
        "\n",
        "# Combined function to clean the text\n",
        "def clean_text(text):\n",
        "    text = remove_informal_tokens(text)  # Remove informal tokens\n",
        "    text = fix_punctuation(text)  # Fix punctuation\n",
        "    return text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81A7y_1sug-c",
        "outputId": "34d3e3d4-c1a0-4a0b-b421-28cfe52439f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            path  \\\n",
            "0   aclImdb/train/pos/8252_9.txt   \n",
            "1   aclImdb/train/pos/3803_9.txt   \n",
            "2   aclImdb/train/pos/606_10.txt   \n",
            "3   aclImdb/train/pos/1865_8.txt   \n",
            "4  aclImdb/train/pos/10303_7.txt   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  A year or so ago, I was watching the TV news w...  \n",
            "1  What people fail to understand about this movi...  \n",
            "2  I have seen a lot of Saura films and always fo...  \n",
            "3  \"And the time came when the risk to remain tig...  \n",
            "4  Bruce Almighty is the story of Bruce Nolan, an...  \n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    text = load_text(text)              # Load text from file path\n",
        "    text = expand_contractions(text)    # Expand contractions\n",
        "    text = remove_informal_tokens(text) # Replace informal tokens\n",
        "    text = fix_punctuation(text)        # Fix punctuation\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing function to training and test data\n",
        "data_train['cleaned_text'] = data_train['path'].apply(preprocess_text)\n",
        "data_test['cleaned_text'] = data_test['path'].apply(preprocess_text)\n",
        "\n",
        "# Now, you can use 'cleaned_text' for further processing or model training\n",
        "print(data_train[['path', 'cleaned_text']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHLiIMDifQTI"
      },
      "source": [
        "### A simple pipeline:\n",
        "[Do not change it!]\n",
        "\n",
        "This simple pipeline will be used to compare the newly created pipeline against, to evaluate the performance increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOYzJ1-SfQTI"
      },
      "source": [
        "**White-Space tokenization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GJjcIUkHfQTI"
      },
      "outputs": [],
      "source": [
        "def tokenize(text:str):\n",
        "    ''' An example tokenization function. '''\n",
        "\n",
        "    # simple white-space tokenization:\n",
        "    return text.lower().split()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GddqcHfvfQTJ"
      },
      "source": [
        "**Bag-of-words Embedding:**\n",
        "\n",
        "See documentation of [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SGnHx6ThfQTJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create a simple bag of words embedding:\n",
        "bow = CountVectorizer(\n",
        "\n",
        "    # the next line converts the filepaths to the actual texts:\n",
        "    #preprocessor = load_text, # Commented, to check if the preprocessing from above changes the results\n",
        "\n",
        "    # tokenization function from above:\n",
        "    tokenizer = tokenize,\n",
        "\n",
        "    # Set token_pattern to None since we're using a custom tokenizer\n",
        "    token_pattern=None\n",
        "\n",
        ")\n",
        "\n",
        "# Train the embedding on cleaned training data\n",
        "embeddings_train = bow.fit_transform(data_train['cleaned_text'].values)\n",
        "\n",
        "# Vectorize the cleaned test data\n",
        "embeddings_test = bow.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "# These are the original lines\n",
        "# train the embedding:\n",
        "#embeddings_train = bow.fit_transform(data_train['path'].values)\n",
        "\n",
        "# vectorize test data:\n",
        "#embeddings_test = bow.transform(data_test['path'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qbhJp3tfQTJ"
      },
      "source": [
        "**Classification with a linear SVM**\n",
        "\n",
        "See documentation of [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUb6hTYVfQTJ",
        "outputId": "141d9bb7-db67-48e7-aeb1-ed8d4e82dd1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.784\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# train classifier:\n",
        "svm.fit(embeddings_train, data_train['label'].values)\n",
        "\n",
        "# test classifier:\n",
        "predictions = svm.predict(embeddings_test)\n",
        "\n",
        "# Calculate Accuracy:\n",
        "print('Accuracy:', accuracy_score(data_test['label'].values, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7n0tlJZug-j"
      },
      "source": [
        "### Own text mining pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize a few sample texts with our chosen tokenizer BPE"
      ],
      "metadata": {
        "id": "x9AfjxXX1AQf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2LtJwsVeG6",
        "outputId": "9aab73cf-43b6-4f89-df74-98296e8a2179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original string: Hello, this is a test sentence for the tokenizer.\n",
            "Encoded tokens: ['Hello', 't', 'h', 'is', 'is', 'a', 'test', 's', 'en', 't', 'en', 'e', 'or', 'the', 'tokenizer']\n",
            "Decoded string: Hello t h is is a test s en t en e or the tokenizer\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Example sentence to encode\n",
        "s = \"Hello, this is a test sentence for the tokenizer.\"\n",
        "\n",
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Add a pre-tokenizer to handle whitespace properly\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize a BPE trainer with some default parameters\n",
        "trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"])\n",
        "\n",
        "# Instead of reading files, let's provide training data directly as strings\n",
        "# Simulate a dataset by giving it a list of strings (or paths to actual files)\n",
        "training_data = [\"Hello world\", \"This is a test\", \"We are training the tokenizer\"]\n",
        "\n",
        "# The trainer expects file paths, but we can create files dynamically (or mock them)\n",
        "# For now, let's assume you have access to these files or use in-memory data.\n",
        "tokenizer.train_from_iterator(training_data, trainer)\n",
        "\n",
        "# Encode the string\n",
        "t = tokenizer.encode(s)\n",
        "\n",
        "# Decode the tokenized output\n",
        "decoded_s = tokenizer.decode(t.ids)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original string:\", s)\n",
        "print(\"Encoded tokens:\", t.tokens)\n",
        "print(\"Decoded string:\", decoded_s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text normalization like expand contractions or remove unwanted punctuation using regex"
      ],
      "metadata": {
        "id": "kEHfVqe31ZJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# Initialize a BPE trainer with a larger vocab size to avoid excessive splitting\n",
        "trainer = BpeTrainer(vocab_size=200, special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"])\n",
        "\n",
        "# Sample training data to teach the tokenizer\n",
        "training_data = [\n",
        "    \"Hello, this is a test sentence for training the BPE tokenizer.\",\n",
        "    \"This is another sentence to improve subword merging.\",\n",
        "    \"BPE tokenizers split words into subwords based on frequency.\"\n",
        "]\n",
        "\n",
        "# Train the tokenizer on the sample dataset\n",
        "tokenizer.train_from_iterator(training_data, trainer)\n",
        "\n",
        "# Define a function to normalize and tokenize the text\n",
        "def normalize_text(text):\n",
        "    # Expand contractions (e.g., \"isn't\" -> \"is not\")\n",
        "    contractions = {\"isn't\": \"is not\", \"it's\": \"it is\", \"he's\": \"he is\", \"she's\": \"she is\", \"i'm\": \"i am\"}\n",
        "    for contraction, expanded in contractions.items():\n",
        "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expanded, text)\n",
        "\n",
        "    # Remove unwanted punctuation using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove all punctuation except word characters and spaces\n",
        "\n",
        "    # Tokenize and encode the string\n",
        "    encoded = tokenizer.encode(text)\n",
        "\n",
        "    # Decode the tokens back into a string\n",
        "    decoded = tokenizer.decode(encoded.ids)\n",
        "\n",
        "    return encoded.tokens, decoded\n",
        "\n",
        "# Test with a sample text\n",
        "s = \"Hello, this is a test sentence for the tokenizer.\"\n",
        "tokens, decoded_s = normalize_text(s)\n",
        "\n",
        "# Display results\n",
        "print(\"Original string:\", s)\n",
        "print(\"Encoded tokens:\", tokens)\n",
        "print(\"Decoded string:\", decoded_s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bki-n3fDzq6l",
        "outputId": "714fcd77-5c8e-4c32-acd2-54e7702d536e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original string: Hello, this is a test sentence for the tokenizer.\n",
            "Encoded tokens: ['Hell', 'o', ' t', 'his is a tes', 't sentence', ' f', 'or', ' t', 'h', 'e', ' to', 'kenizer']\n",
            "Decoded string: Hell o  t his is a tes t sentence  f or  t h e  to kenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further text normalization like stemming"
      ],
      "metadata": {
        "id": "kDx9UUI51Rqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download the 'punkt' resource for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function when further normalization is needed\n",
        "def further_normalization_needed(training_data):\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    for i, text in enumerate(training_data):\n",
        "        print(f\"\\nOriginal Text {i+1}: {text}\")\n",
        "\n",
        "        # Tokenize using word_tokenize for stemming analysis\n",
        "        tokens = word_tokenize(text)\n",
        "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        # Custom regex to remove unwanted characters (e.g., punctuation)\n",
        "        cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "        print(\"Cleaned Text (after regex):\", cleaned_text)\n",
        "\n",
        "# Check if further text normalization is needed\n",
        "further_normalization_needed(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz04Rt4eolPf",
        "outputId": "ced45d84-39ff-4d79-d8ba-40cf2bebb789"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Text 1: Hello, this is a test sentence for training the BPE tokenizer.\n",
            "Stemmed Tokens: ['hello', ',', 'thi', 'is', 'a', 'test', 'sentenc', 'for', 'train', 'the', 'bpe', 'token', '.']\n",
            "Cleaned Text (after regex): Hello this is a test sentence for training the BPE tokenizer\n",
            "\n",
            "Original Text 2: This is another sentence to improve subword merging.\n",
            "Stemmed Tokens: ['thi', 'is', 'anoth', 'sentenc', 'to', 'improv', 'subword', 'merg', '.']\n",
            "Cleaned Text (after regex): This is another sentence to improve subword merging\n",
            "\n",
            "Original Text 3: BPE tokenizers split words into subwords based on frequency.\n",
            "Stemmed Tokens: ['bpe', 'token', 'split', 'word', 'into', 'subword', 'base', 'on', 'frequenc', '.']\n",
            "Cleaned Text (after regex): BPE tokenizers split words into subwords based on frequency\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g8AKEvI4CvuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample data for demonstration\n",
        "data_train = pd.DataFrame({\n",
        "    'cleaned_text': [\n",
        "        \"Hello, this is a test sentence for the tokenizer.\",\n",
        "        \"This is another sentence to improve subword merging.\"\n",
        "   ],\n",
        "    'label': [0, 1]  # Add a 'label' column to the DataFrame\n",
        "})\n",
        "\n",
        "data_test = pd.DataFrame({\n",
        "    'cleaned_text': [\n",
        "        \"Hello, this is another example sentence.\",\n",
        "        \"Subword tokenization is quite effective.\"\n",
        "    ],\n",
        "    'label': [0, 1]  # Add a 'label' column to the DataFrame\n",
        "})\n",
        "\n",
        "\n",
        "# 1. Improved Tokenization (BPE)\n",
        "# Initialize BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "trainer = BpeTrainer(vocab_size=10000, min_frequency=2)\n",
        "tokenizer.train_from_iterator(data_train['cleaned_text'], trainer=trainer)\n",
        "\n",
        "# Rename the function to avoid conflict with the 'tokens' variable\n",
        "def bpe_tokenize_func(text):\n",
        "    return tokenizer.encode(text).tokens\n",
        "\n",
        "# 2. Improved Tokenization with Normalization (BPE)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Stem each word\n",
        "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "def bpe_tokenize_normalized(text):\n",
        "    normalized_text = normalize_text(text)\n",
        "    return tokenizer.encode(normalized_text).tokens\n",
        "\n",
        "# Create CountVectorizer for each tokenization method\n",
        "\n",
        "# Bag of Words with BPE Tokenization\n",
        "# Use the renamed function here\n",
        "bow_bpe = CountVectorizer(tokenizer=bpe_tokenize_func, token_pattern=None)\n",
        "embeddings_train_bpe = bow_bpe.fit_transform(data_train['cleaned_text'].values)\n",
        "embeddings_test_bpe = bow_bpe.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "# Bag of Words with BPE Tokenization and Normalization\n",
        "bow_bpe_norm = CountVectorizer(tokenizer=bpe_tokenize_normalized, token_pattern=None)\n",
        "embeddings_train_bpe_norm = bow_bpe_norm.fit_transform(data_train['cleaned_text'].values)\n",
        "embeddings_test_bpe_norm = bow_bpe_norm.transform(data_test['cleaned_text'].values)\n",
        "\n",
        "\n",
        "print(\"\\nBag of Words with BPE Tokenization:\")\n",
        "print(embeddings_train_bpe.toarray())\n",
        "print(embeddings_test_bpe.toarray())\n",
        "\n",
        "print(\"\\nBag of Words with BPE Tokenization and Normalization:\")\n",
        "print(embeddings_train_bpe_norm.toarray())\n",
        "print(embeddings_test_bpe_norm.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb8QQiBEJmya",
        "outputId": "b72f2397-072c-459f-cf0f-276081014c9c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag of Words with BPE Tokenization:\n",
            "[[1 0 1 3 1 1 1 0 0 3 1 1 1 0 2 1 1 1 2 0 0 1 1 0 0 1 1 0 0 0 1]\n",
            " [2 1 1 0 1 0 1 1 1 1 0 2 0 2 1 1 2 0 0 2 2 2 1 1 1 0 2 1 1 1 0]]\n",
            "[[1 0 1 1 0 1 1 0 0 3 0 1 0 0 2 1 0 0 3 1 1 2 0 1 0 0 1 0 0 0 0]\n",
            " [2 0 0 0 1 0 1 1 1 4 1 0 2 0 0 0 4 1 0 0 1 1 1 0 0 1 3 2 1 1 1]]\n",
            "\n",
            "Bag of Words with BPE Tokenization and Normalization:\n",
            "[[2 0 1 3 1 0 1 0 3 1 0 1 0 3 1 1 1 2 0 0 1 1 0 0 1 1 0 0 0]\n",
            " [3 1 1 0 1 1 1 1 0 0 1 0 1 2 2 1 0 0 2 1 2 1 1 1 0 2 1 1 1]]\n",
            "[[2 0 1 1 0 0 1 0 2 0 0 0 0 3 1 1 0 3 1 1 2 0 1 0 0 1 0 0 0]\n",
            " [2 0 0 0 1 1 1 1 2 1 0 2 0 0 1 0 1 0 0 0 0 1 0 0 1 2 2 0 1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# train classifier:\n",
        "svm.fit(embeddings_train_bpe, data_train['label'].values)\n",
        "\n",
        "\n",
        "# test classifier:\n",
        "predictions = svm.predict(embeddings_test_bpe)\n",
        "\n",
        "\n",
        "# Calculate Accuracy:\n",
        "print('Accuracy:', accuracy_score(data_test['label'].values, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "988exFnFZ4dP",
        "outputId": "ee676a6c-012e-4081-e96d-956fbee5cdf7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "# Train and evaluate classifier using BPE embeddings\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# Train classifier\n",
        "#svm.fit(embeddings_train, data_train['label'].values)\n",
        "#svm.fit(embeddings_train_bpe, data_train['label'].values)\n",
        "svm.fit(embeddings_train_bpe_norm, data_train['label'].values) # The model is trained on embeddings_train_bpe_norm\n",
        "\n",
        "\n",
        "# Test classifier\n",
        "#predictions = svm.predict(embeddings_test_bpe) # This line causes the error because embeddings_test_bpe has a different number of features\n",
        "predictions = svm.predict(embeddings_test_bpe_norm) # Use embeddings_test_bpe_norm for prediction, which has the same number of features as the training data\n",
        "\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(data_test['label'].values, predictions)\n",
        "print('Accuracy with BPE Tokenization and Normalization:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1WMtvEpdCPf",
        "outputId": "c3bb6191-6759-4064-a52e-5bd92b3eff9c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with BPE Tokenization and Normalization: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AO0d9PMR2CZr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}