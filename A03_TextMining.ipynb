{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinamilo6/Textmining/blob/main/A03_TextMining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X1D-_ww7fQTB"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas sklearn nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS1_jLQofQTD"
      },
      "source": [
        "# Assignment 3 - Text Mining\n",
        "\n",
        "Project management and tools for health informatics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufi0zQidfQTE"
      },
      "source": [
        "## 1. Download and prepare data:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "The code in this section downloads the [IMDB IMDB Large Movie Review Dataset]('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz') which is the dataset you will be working on in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aOzKCiw4fQTE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sq9rCBBrfQTE"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('aclImdb'):\n",
        "    # download data:\n",
        "    urlretrieve('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', 'aclImdb.tar.gz')\n",
        "\n",
        "    # unzip data:\n",
        "    with tarfile.open('aclImdb.tar.gz') as file:\n",
        "        file.extractall('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9pzG__QfQTF"
      },
      "source": [
        "## 2. Some helper Functions:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "This section contains the code for some helper functions that will be useful for solving the assignment. Example code on how to use the functions is provided in section 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LaM3GoopfQTF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from typing import Literal, Tuple, Iterable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxsGe7MufQTF"
      },
      "source": [
        "Function for loading data into a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tKoAn698fQTG"
      },
      "outputs": [],
      "source": [
        "def load_data(split:Literal['train', 'test'], texts_per_class:int=500) -> pd.DataFrame:\n",
        "    ''' Loads the data into a pandas dataframe.'''\n",
        "    paths  = []\n",
        "    labels = []\n",
        "\n",
        "    for label in ('pos', 'neg'):\n",
        "        # get all files in the folder:\n",
        "        files = os.listdir(os.path.join('aclImdb', split, label))[:texts_per_class]\n",
        "\n",
        "        # append them to the lists:\n",
        "        paths.extend([os.path.join('aclImdb', split, label, f) for f in files])\n",
        "        labels.extend([label] * len(files))\n",
        "\n",
        "    return pd.DataFrame({'path':paths, 'label':labels})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C1jyAtfQTG"
      },
      "source": [
        "Function for loading a specific text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y0QGuZMXfQTH"
      },
      "outputs": [],
      "source": [
        "def load_text(path:str) -> str:\n",
        "    ''' Reads a single text given the path. '''\n",
        "    # read file from disk:\n",
        "    with open(path, 'r', encoding='utf8') as file:\n",
        "        s = file.read()\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR52KxrvfQTH"
      },
      "source": [
        "Function for iterating through multiple texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NImrB2NzfQTH"
      },
      "outputs": [],
      "source": [
        "def iterate_texts(data:pd.DataFrame) -> Iterable[Tuple[str, str]]:\n",
        "    ''' Iterates through a pandas dataframe. '''\n",
        "\n",
        "    for path in data['path'].values:\n",
        "        # read file from disk:\n",
        "        with open(path, 'r', encoding='utf8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        yield text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzYgen2AfQTH"
      },
      "source": [
        "## 3. Text Mining Pipeline\n",
        "\n",
        "This section will cover the text mining steps for this assignment. The following steps will be performed:\n",
        "\n",
        "1. **Analyze the Data for Difficult Parts**  \n",
        "   - Reviewing the data to identify challenging aspects such as contractions, informal language, and complex sentence structures.\n",
        "\n",
        "2. **Replace Contractions and Informal Language**  \n",
        "   - Expanding common contractions and replacing informal phrases to standardize the text for processing.\n",
        "   - This part is optinal and it should be later evaluated with normalization and without.\n",
        "\n",
        "3. **Tokenize the Texts**  \n",
        "   - Applying a tokenizer to break down the text into individual tokens for analysis.\n",
        "\n",
        "4. **Training and Predicting with ML**\n",
        "   - Training different ML models with the tokenized data and predicting the test data to evaluate the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2o7r5Bug-U"
      },
      "source": [
        "### Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "57LKdqhTug-V"
      },
      "outputs": [],
      "source": [
        "# Import needed libaries for the preparation of the texts\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3FnCEu6fQTH"
      },
      "source": [
        "### Load the training data\n",
        "The loaded data frpom the zip file should be saved into a `data_train` and `data_test` DataFrame.\n",
        "These can be further on be used to access the train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CeshtSl9fQTI",
        "outputId": "fa88fb56-74c9-47ca-d8fa-c3b9b6e19fe5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aclImdb\\train\\pos\\0_9.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aclImdb\\train\\pos\\10000_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aclImdb\\train\\pos\\10001_10.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aclImdb\\train\\pos\\10002_7.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aclImdb\\train\\pos\\10003_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>aclImdb\\train\\neg\\10446_2.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>aclImdb\\train\\neg\\10447_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>aclImdb\\train\\neg\\10448_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>aclImdb\\train\\neg\\10449_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>aclImdb\\train\\neg\\1044_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               path label\n",
              "0         aclImdb\\train\\pos\\0_9.txt   pos\n",
              "1     aclImdb\\train\\pos\\10000_8.txt   pos\n",
              "2    aclImdb\\train\\pos\\10001_10.txt   pos\n",
              "3     aclImdb\\train\\pos\\10002_7.txt   pos\n",
              "4     aclImdb\\train\\pos\\10003_8.txt   pos\n",
              "..                              ...   ...\n",
              "995   aclImdb\\train\\neg\\10446_2.txt   neg\n",
              "996   aclImdb\\train\\neg\\10447_1.txt   neg\n",
              "997   aclImdb\\train\\neg\\10448_1.txt   neg\n",
              "998   aclImdb\\train\\neg\\10449_4.txt   neg\n",
              "999    aclImdb\\train\\neg\\1044_4.txt   neg\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train = load_data('train')\n",
        "data_test  = load_data('test')\n",
        "data_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qXdudbDfQTI"
      },
      "source": [
        "### 1. Assess the data / texts for difficult parts\n",
        "In this part, sample texts are printed and then analyzed for diffult parts, that could affect the text mining process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soE8Qj9hsF0E",
        "outputId": "9f3f109e-dedd-4ec9-a9ce-6fb530a35fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1:\n",
            "\n",
            "Fabulous costumes by Edith Head who painted them on Liz Taylor at her finest!<br /><br />The SFX are very good for a movie of its age, and the stunt doubles actually looked like the actors, even down to body type, a rarity in movies of this vintage.<br /><br />A cozy movie, with splendid panoramas -- even when chopped down to pan and scan.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 2:\n",
            "\n",
            "\"Happenstance\" is the most New York-feeling Parisian film I've seen since \"When the Cat's Away (Chacun cherche son chat). \"<br /><br />A film from last year released now to capitalize on the attention Audrey Tatou is getting for \"Amelie,\" its French title is more apt: \"Le Battement d'ailes du papillon (The Beating of the Butterfly's Wings)\" as in summarizing chaos theory as a controlling element in our lives.<br /><br />Tatou's gamine-ness is less annoying here because she only occasionally flashes that dazzling smile amidst her hapless adventures, and because she's part of a large, multi-ethnic ensemble, so large that it took me a long time to sort out the characters, especially as some of the cute guys and older women looked alike to me, and some of the characters fantasize what they should do such that I wasn't sure if they were doing that or not. <br /><br />But I loved how urban the coincidences were, from immigrants to love nests to crowded subway cars to hanging around cafés.<br /><br />The subtitles quite annoyingly gave both parts of a dialog at once.<br /><br />(originally written 12/8/2001)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 3:\n",
            "\n",
            "Wang Bianlian is an old street performer who is known as a 'King of masks' for his mastery of Sichuan change art. Liang is a famous opera performer of Sichuan art and respects Wang as an artist and as a person. Liang is worried that a precious art shouldn't die with Wang and so he sows the seed of an heir in to Wang's mind. The film is about prejudices, male domination, state of art, values and most importantly warmth.<br /><br />I can't recommend this film enough. The whole film is in loops. Everything has a significance. Its a long story which has been edited so well that the length of the film is just 91 minutes. A total satisfaction. For five minutes it is an artistic film, next five minutes its a sad film, next five minutes its a thriller. It just keeps changing its mood like its protagonist changes his face. Last scene on the rope is phenomenal. Story and script is flawless. Actors are brilliant. Both the protagonists are artists you can tell the way they have performed. Very impressive. It was not even nominated for Oscars. That year 'English patient' got the best film Oscar and in the foreign film category 'Kolya' won. 'Kolya' was just OK and about 'English patient' the lesser said the better. Watch it 9/10.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 4:\n",
            "\n",
            "I got all excited when I saw the ads for this movie because I recently read the book and really enjoyed it. The movie, however, did not meet my expectations. Having read the book recently prepared me for big let down as often happens when stories are translated into movies. The characters didn't seem to fit very well with the book. The direction was weak. I had a hard time getting into the characters. There wasn't a real connection with the viewer about what was going on. The dialog didn't explain adequately what was happening. It just seemed slapped together and rushed through. All in all I was very disappointed with the movie. I suppose if you haven't read the book, it might be ok by itself. At the very least, it might entice you to read the book, which you'll probably enjoy more.<br /><br />\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 5:\n",
            "\n",
            "Jameson Parker And Marilyn Hassett are the screen's most unbelievable couple since John Travolta and Lily Tomlin. Larry Peerce's direction wavers uncontrollably between black farce and Roman tragedy. Robert Klein certainly think it's the former and his self-centered performance in a minor role underscores the total lack of balance and chemistry between the players in the film. Normally, I don't like to let myself get so ascerbic, but The Bell Jar is one of my all-time favorite books, and to watch what they did with it makes me literally crazy.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 6:\n",
            "\n",
            "The story-line of \"The Thief of Bagdad\" is complex, owing to its being told in flashbacks and having three separate and equally important strands woven together. The screenplay by Lajo Biros and the dialogue by Miles Malleson keep the story moving skillfully at all points.The young King Ahmad of Bagdad is angry at his vizier Jaffar for executing a man for having different ideas. He discovers while in disguise that people blame him for Jaffar's deeds and hate him. He is imprisoned by Jaffar, where he meets Abu the young thief. The two escape and take a boat to the city of Basra. There the companions spy when men clear the way so none will see the Princess of the city passing by. Ahmad falls in love with her and visits her in her garden. He tells her he has come to her from beyond time and wins a kiss. Then he is captured. When Jaffar comes to win the Princess of Basra for himself, Ahmad attacks the evil vizier who blinds him and turns Abu into a dog. Jaffar then asks for the Princess's hand, and he gives the gift of a mechanical flying horse to the Sultan of Basra. The blind Ahmad then tells his tale in the marketplace, accompanied by Abu as his dog. The Prince has fallen into a sleep and nothing can wake her. So Jaffar sends his servant Halima for Ahmad and the dog, in hopes the prince can rouse her. He does awaken her. She boards a ship to find a doctor to cure Ahmad, but she is captured by Jaffar who then throws the dog overboard. She then allows Jaffar to take her in his arms, on his promise to restore Ahmad's sight and turn Abu back into a thief. The princess sees a vision of Ahmad; he is in a boat; Jaffar sends a storm to beset him and Abu is shipwrecked on a deserted island. Abu finds a genie or djinn who wants to kill him now that he is free after many centuries spent imprisoned in a bottle. Abu tricks him into proving he really came from so small a vessel, then corks him in again. For freeing him, he gets three wishes. His first is for sausages. In the meanwhile, the Princess pleads with her father to refuse Jaffar; but Jaffar shows the Sultan a new mechanical toy, one of whose six arms stabs him to death. Abu makes a second wish, to find Ahmad. The cunning genie flies him to the goddess of the All-Seeing eye. Abu has to climb a great web to get to the gem that is the eye, battling a giant spider, then scaling the goddess's statue. Abu gazes into the 'eye' and sees Ahmad in a canyon. He has the genie take him to Ahmad. Ahmad uses the eye to see the princess. She smells a flower and forgets everything at once. Abu wishes they were in Bagdad, but the genie laughs and leaves; Jaffar tells the Princess that she is in love with him, omitting mention of Ahmad. Ahmad tries to fight his way to the Princess, but Jaffar smashes the 'eye'. Abu finds himself in the \"Land of Legend\", where the old men who rule want to make him their king. He steals a bow and a magic carpet and escapes instead, to hurry to save Ahmad and the princess. The thief arrives in time to save the young king from the executioner, using his bow from the flying carpet, to the wonder of the throng who had come to watch the execution. Jaffar tries to flee on the mechanical flying horse, but another shot from the bow finishes him. Ahmad is ruler again and plans to wed his Princess; but when he tries to make Abu his vizier, the young thief refuses, saying that what he wants is adventure, not hard work and confinement in a palace however grand it may be. This fantastic story was given a sumptuous production by producer Alexander Korda. The production was designed by Vincent Korda who was also art director, while Georges Perinal did the colorful cinematography. The directors credited are Ludwig Berger and Michael Powell, with Tim Whelan, Alexander Korda, William Cameron Menzies and Zoltan Korda participating. The extraordinary and numerous costumes designs were the work of John Armstrong, Oliver Messel and Marcel Vertes. The production, apart from its gorgeous and expensive-looking visual splendors, I claim is dominated by two other elements, the choral music of Miklos Rozsa and the performance by Conrad Veidt as the evil Jaffar. Rex Ingram plays the genie with a curious accent, plus his usual intelligence and power. June Duprez is lovely and effective as the Princess Mary Morris is a sad and beautiful Halima, and Miles Malleson a properly bumbling and avaricious Sultan. As Ahmad, John Justin appears to do most of what can be done with the part of a young prince in love and then some; he is memorably good in his winning role. This film has a spaciousness about it that is found, I assert, in other Korda works also. Its imaginative content stands in contrast to very-strong realistic sets, costumes and set-design elements. This is one of the most memorable idea-level fantasies of all time, worthy to be enjoyed over and over.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 7:\n",
            "\n",
            "The \"Amazing Mr. Williams\" stars Melvyn Douglas, who did five films in 1939, one of which was Ninotchka with Garbo. His co-star was Joan Blondell (Maxine), who ALSO did five films that year, THREE of which they made together! Douglas is Lt. Williams, and he and his co-horts are presented with a dead body, and they must figure out what really happened. Viewers will recognize his co-workers - the actors (Clarence Kolb, Donald MacBride, Don Beddoe) always played positions of authority... senators, bank presidents, policemen. This who-dunnit has a flair of comedy to it -- the policemen are always throwing jabs at each other, and even Williams and his girlfriend are battling verbally. Some fun gags - Williams even takes the man they arrested along on a date with his girlfriend. There's a lot of fun stuff in here, so get past the slow beginning and wait for the funnier stuff later on. Don't want to give away any spoilers, so you'll have to catch it on Turner Classic Movies. Director Alexander Hall made mostly comedies, and was reportedly engaged to Lucy at some point.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Number of samples to be printed\n",
        "n_samples = 7\n",
        "\n",
        "# Randomly sample indices from \"data_train\"\n",
        "sample_indices = random.sample(range(len(data_train)), n_samples)\n",
        "\n",
        "# Load and print each sample using the \"load_text\" function\n",
        "for i, id in enumerate(sample_indices, start=1):\n",
        "    # Load text from the file path specified in 'path' column\n",
        "    text = load_text(data_train.loc[id, 'path'])\n",
        "    print(f\"Sample {i}:\\n\")\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWw3AZ8SumVn"
      },
      "source": [
        "## Limitations/Issues Captured from the Above Text Samples\n",
        "\n",
        "- **HTML tags and special characters**\n",
        "- **Punctuation and symbols** (e.g., `&`)\n",
        "- **Contractions** (e.g., \"isn't\", \"I'll\", \"I'm\")\n",
        "- **Parentheses and annotations** (e.g., \"(Crouching Tiger)\")\n",
        "- **Informal formatting** (e.g., \"my rating is ****\")\n",
        "- **Ambiguity and polysemy** (e.g., \"dictators\", \"nuts\")\n",
        "- **Long and complex sentences**\n",
        "- **Informal language** (e.g., \"what can be so bad about that?\")\n",
        "- **Quotation marks** (e.g., \"dictators\", \"sin\")\n",
        "\n",
        "We will elaborate on these issues and add our conclusions in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHLiIMDifQTI"
      },
      "source": [
        "### A simple pipeline:\n",
        "[Do not change it!]\n",
        "\n",
        "This simple pipeline will be used to compare the newly created pipeline against, to evaluate the performance increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOYzJ1-SfQTI"
      },
      "source": [
        "**White-Space tokenization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GJjcIUkHfQTI"
      },
      "outputs": [],
      "source": [
        "def tokenize(text:str):\n",
        "    ''' An example tokenization function. '''\n",
        "\n",
        "    # simple white-space tokenization:\n",
        "    return text.lower().split()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GddqcHfvfQTJ"
      },
      "source": [
        "**Bag-of-words Embedding:**\n",
        "\n",
        "See documentation of [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SGnHx6ThfQTJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create a simple bag of words embedding:\n",
        "bow = CountVectorizer(\n",
        "\n",
        "    # the next line converts the filepaths to the actual texts:\n",
        "    preprocessor = load_text,\n",
        "\n",
        "    # tokenization function from above:\n",
        "    tokenizer = tokenize,\n",
        "\n",
        "    # Set token_pattern to None since we're using a custom tokenizer\n",
        "    token_pattern=None\n",
        "\n",
        ")\n",
        "\n",
        "# train the embedding:\n",
        "embeddings_train = bow.fit_transform(data_train['path'].values)\n",
        "\n",
        "# vectorize test data:\n",
        "embeddings_test = bow.transform(data_test['path'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qbhJp3tfQTJ"
      },
      "source": [
        "**Classification with a linear SVM**\n",
        "\n",
        "See documentation of [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUb6hTYVfQTJ",
        "outputId": "141d9bb7-db67-48e7-aeb1-ed8d4e82dd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.761\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# train classifier:\n",
        "svm.fit(embeddings_train, data_train['label'].values)\n",
        "\n",
        "# test classifier:\n",
        "predictions = svm.predict(embeddings_test)\n",
        "\n",
        "# Calculate Accuracy:\n",
        "print('Accuracy:', accuracy_score(data_test['label'].values, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2b) Implement the new Tokenizer\n",
        "We decided to implement the BPE Tokenizer and will start to evaluate the Output of the BPE Tokenizer without normaliation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1 Original string: Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
            "Sample 1 Encoded tokens (token IDs): [9362, 2897, 121, 66, 3078, 901, 18, 267, 3612, 125, 118, 613, 313, 130, 251, 297, 11684, 286, 1745, 504, 16, 568, 130, 6, 15371, 960, 997, 6525, 581, 117, 118, 14232, 7037, 769, 168, 132, 842, 156, 9362, 2897, 11, 84, 9018, 121, 408, 6917, 132, 2630, 389, 121, 6, 15371, 960, 175, 23018, 132, 4882, 11284, 16, 118, 9245, 4970, 238, 291, 263, 605, 542, 352, 2583, 9155, 11, 8157, 16, 118, 26612, 135, 118, 896, 1780, 16, 204, 1818, 168, 135, 118, 13998, 45, 2734, 134, 352, 4970, 18, 988, 45, 712, 118, 778, 117, 349, 66, 4386, 6449, 2165, 132, 2446, 616, 118, 1745, 16, 45, 3958, 12497, 20555, 125, 14557, 2897, 18, 37, 1410, 757, 30, 26913, 30, 45, 11, 78, 344, 132, 12358, 190, 135, 517, 9155, 18, 15461, 30, 15640, 132, 9362, 2897, 18, 45, 657, 156, 484, 3074, 135, 303, 415, 379, 156, 9362, 2897, 121, 663, 8661, 18, 694, 66, 5512, 156, 133, 854, 11, 85, 5]\n",
            "Sample 1 Tokenized string (tokens): ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\".', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'\", 's', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\".', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'\", 'm', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'isn', \"'\", 't', '!']\n",
            "Sample 1 Decoded string: Bromwell High is a cartoon comedy . It ran at the same time as some other programs about school life , such as \" Teachers \". My 35 years in the teaching profession lead me to believe that Bromwell High ' s satire is much closer to reality than is \" Teachers \". The scramble to survive financially , the insightful students who can see right through their pathetic teachers ' pomp , the pettiness of the whole situation , all remind me of the schools I knew and their students . When I saw the episode in which a student repeatedly tried to burn down the school , I immediately recalled ......... at .......... High . A classic line : INSPECTOR : I ' m here to sack one of your teachers . STUDENT : Welcome to Bromwell High . I expect that many adults of my age think that Bromwell High is far fetched . What a pity that it isn ' t !\n",
            "================================================================================\n",
            "Sample 2 Original string: Yes its an art... to successfully make a slow paced thriller.<br /><br />The story unfolds in nice volumes while you don't even notice it happening.<br /><br />Fine performance by Robin Williams. The sexuality angles in the film can seem unnecessary and can probably affect how much you enjoy the film. However, the core plot is very engaging. The movie doesn't rush onto you and still grips you enough to keep you wondering. The direction is good. Use of lights to achieve desired affects of suspense and unexpectedness is good.<br /><br />Very nice 1 time watch if you are looking to lay back and hear a thrilling short story!\n",
            "Sample 2 Encoded tokens (token IDs): [1882, 362, 119, 255, 401, 132, 5311, 445, 66, 1425, 3460, 1847, 189, 140, 170, 140, 143, 175, 309, 6459, 117, 1099, 22045, 590, 196, 436, 11, 85, 335, 2933, 133, 3704, 189, 140, 170, 140, 143, 9650, 900, 247, 2321, 1507, 18, 175, 5346, 6857, 117, 118, 184, 291, 508, 4828, 134, 291, 851, 3498, 361, 408, 196, 637, 118, 184, 18, 1151, 16, 118, 2728, 491, 121, 244, 3166, 18, 175, 182, 633, 11, 85, 4315, 3116, 196, 134, 589, 8010, 196, 795, 132, 1001, 196, 3380, 18, 175, 1347, 121, 299, 18, 9791, 135, 5678, 132, 3416, 6871, 9243, 135, 1680, 134, 22375, 121, 299, 189, 140, 170, 140, 143, 3794, 1099, 21, 313, 317, 266, 196, 187, 1136, 132, 3607, 488, 134, 2611, 66, 6958, 1156, 309, 5]\n",
            "Sample 2 Tokenized string (tokens): ['Yes', 'its', 'an', 'art', '...', 'to', 'successfully', 'make', 'a', 'slow', 'paced', 'thriller', '.<', 'br', '/><', 'br', '/>', 'The', 'story', 'unfolds', 'in', 'nice', 'volumes', 'while', 'you', 'don', \"'\", 't', 'even', 'notice', 'it', 'happening', '.<', 'br', '/><', 'br', '/>', 'Fine', 'performance', 'by', 'Robin', 'Williams', '.', 'The', 'sexuality', 'angles', 'in', 'the', 'film', 'can', 'seem', 'unnecessary', 'and', 'can', 'probably', 'affect', 'how', 'much', 'you', 'enjoy', 'the', 'film', '.', 'However', ',', 'the', 'core', 'plot', 'is', 'very', 'engaging', '.', 'The', 'movie', 'doesn', \"'\", 't', 'rush', 'onto', 'you', 'and', 'still', 'grips', 'you', 'enough', 'to', 'keep', 'you', 'wondering', '.', 'The', 'direction', 'is', 'good', '.', 'Use', 'of', 'lights', 'to', 'achieve', 'desired', 'affects', 'of', 'suspense', 'and', 'unexpectedness', 'is', 'good', '.<', 'br', '/><', 'br', '/>', 'Very', 'nice', '1', 'time', 'watch', 'if', 'you', 'are', 'looking', 'to', 'lay', 'back', 'and', 'hear', 'a', 'thrilling', 'short', 'story', '!']\n",
            "Sample 2 Decoded string: Yes its an art ... to successfully make a slow paced thriller .< br />< br /> The story unfolds in nice volumes while you don ' t even notice it happening .< br />< br /> Fine performance by Robin Williams . The sexuality angles in the film can seem unnecessary and can probably affect how much you enjoy the film . However , the core plot is very engaging . The movie doesn ' t rush onto you and still grips you enough to keep you wondering . The direction is good . Use of lights to achieve desired affects of suspense and unexpectedness is good .< br />< br /> Very nice 1 time watch if you are looking to lay back and hear a thrilling short story !\n",
            "================================================================================\n",
            "Sample 3 Original string: There are many illnesses born in the mind of man which have been given life in modern times. Constant vigilance or accrued information in the realm of Pyschosis, have kept psychologists, counselors and psychiatrists busy with enough work to last them decades. Occasionally, some of these mental phenomenon are discover by those with no knowledge of their remedy or even of their existence. That is the premise of the film entitled \" The Night Listner.\" It tells the story of a popular radio host called Gabriel Noon (Robin Williams) who spends his evenings enthralling his audiences with vivid stories about Gay lifestyles. Perhaps its because his show is losing it's authentic veneer which causes Noon to admit he is no longer himself. Feeling abandoned by both his lover Jess (Bobby Cannavale) and his and best friend (Joe Morton), he seeks shelter in his deepening despair and isolation. It is here, a mysterious voice in the night asks him for help. Noon needs to feel useful and reaches out to the desperate voice which belongs to a 14 year old boy called Peter (Rory Culkin). In reading the boy's harrowing manuscript which depicts the early life and sexual abuse at the hands of his brutal parents, Noon is captivated and wants to help. However, things are not what they seem and Noon soon finds himself en-wrapped in an elusive and bizarre tale torn right out of a medical nightmare. This movie is pure Robin Williams and were it not for Toni Collette who plays Donna D. Logand, Sandra Oh as Anna and John Cullum as pop, this might be comical. Instead, this may prove to be one of William's more serious performances. ***\n",
            "Sample 3 Encoded tokens (token IDs): [572, 187, 484, 24509, 6270, 117, 118, 690, 135, 222, 349, 234, 421, 1102, 504, 117, 3190, 726, 18, 21209, 14958, 126, 26641, 4020, 117, 118, 5651, 135, 24712, 16, 234, 1864, 23447, 16, 25570, 134, 23056, 21831, 186, 795, 460, 132, 783, 398, 6900, 18, 26087, 16, 251, 135, 571, 6813, 8149, 187, 1701, 247, 611, 186, 257, 4570, 135, 352, 15700, 126, 335, 135, 352, 3554, 18, 871, 121, 118, 2431, 135, 118, 184, 8161, 6, 175, 1997, 22773, 915, 267, 1612, 118, 309, 135, 66, 2578, 3547, 2566, 1163, 4169, 6920, 12, 2321, 1507, 13, 238, 6817, 194, 19282, 24465, 194, 2861, 186, 5249, 1571, 286, 9658, 23920, 18, 3541, 362, 417, 194, 372, 121, 5203, 133, 11, 84, 7233, 19048, 349, 4929, 6920, 132, 1877, 142, 121, 257, 3261, 1187, 18, 22767, 9429, 247, 715, 194, 3134, 2596, 12, 4692, 5896, 13, 134, 194, 134, 539, 668, 12, 2857, 4739, 495, 142, 7613, 24219, 117, 194, 21530, 8813, 134, 14963, 18, 267, 121, 344, 16, 66, 4971, 1902, 117, 118, 1057, 2673, 376, 162, 1005, 18, 6920, 1987, 132, 597, 7762, 134, 7957, 192, 132, 118, 2874, 1902, 349, 6413, 132, 66, 2978, 930, 454, 898, 1163, 2168, 12, 5467, 5024, 490, 406, 2206, 118, 898, 11, 84, 15118, 15053, 349, 9197, 118, 1841, 504, 134, 2490, 5179, 125, 118, 3159, 135, 194, 3950, 2085, 16, 6920, 121, 11432, 134, 1707, 132, 1005, 18, 1151, 16, 702, 187, 212, 322, 252, 508, 134, 6920, 1823, 2564, 1187, 124, 17, 6204, 117, 119, 12791, 134, 4418, 2254, 6454, 605, 192, 135, 66, 7730, 5280, 18, 308, 182, 121, 2049, 2321, 1507, 134, 347, 133, 212, 162, 4694, 3176, 238, 1014, 4911, 40, 18, 9493, 16, 4399, 1650, 130, 6254, 134, 850, 15524, 130, 1344, 16, 164, 718, 149, 7562, 18, 3142, 16, 164, 578, 2767, 132, 149, 190, 135, 3911, 11, 84, 311, 1084, 1194, 18, 3150]\n",
            "Sample 3 Tokenized string (tokens): ['There', 'are', 'many', 'illnesses', 'born', 'in', 'the', 'mind', 'of', 'man', 'which', 'have', 'been', 'given', 'life', 'in', 'modern', 'times', '.', 'Constant', 'vigilance', 'or', 'accrued', 'information', 'in', 'the', 'realm', 'of', 'Pyschosis', ',', 'have', 'kept', 'psychologists', ',', 'counselors', 'and', 'psychiatrists', 'busy', 'with', 'enough', 'work', 'to', 'last', 'them', 'decades', '.', 'Occasionally', ',', 'some', 'of', 'these', 'mental', 'phenomenon', 'are', 'discover', 'by', 'those', 'with', 'no', 'knowledge', 'of', 'their', 'remedy', 'or', 'even', 'of', 'their', 'existence', '.', 'That', 'is', 'the', 'premise', 'of', 'the', 'film', 'entitled', '\"', 'The', 'Night', 'Listner', '.\"', 'It', 'tells', 'the', 'story', 'of', 'a', 'popular', 'radio', 'host', 'called', 'Gabriel', 'Noon', '(', 'Robin', 'Williams', ')', 'who', 'spends', 'his', 'evenings', 'enthralling', 'his', 'audiences', 'with', 'vivid', 'stories', 'about', 'Gay', 'lifestyles', '.', 'Perhaps', 'its', 'because', 'his', 'show', 'is', 'losing', 'it', \"'\", 's', 'authentic', 'veneer', 'which', 'causes', 'Noon', 'to', 'admit', 'he', 'is', 'no', 'longer', 'himself', '.', 'Feeling', 'abandoned', 'by', 'both', 'his', 'lover', 'Jess', '(', 'Bobby', 'Cannavale', ')', 'and', 'his', 'and', 'best', 'friend', '(', 'Joe', 'Morton', '),', 'he', 'seeks', 'shelter', 'in', 'his', 'deepening', 'despair', 'and', 'isolation', '.', 'It', 'is', 'here', ',', 'a', 'mysterious', 'voice', 'in', 'the', 'night', 'asks', 'him', 'for', 'help', '.', 'Noon', 'needs', 'to', 'feel', 'useful', 'and', 'reaches', 'out', 'to', 'the', 'desperate', 'voice', 'which', 'belongs', 'to', 'a', '14', 'year', 'old', 'boy', 'called', 'Peter', '(', 'Rory', 'Culkin', ').', 'In', 'reading', 'the', 'boy', \"'\", 's', 'harrowing', 'manuscript', 'which', 'depicts', 'the', 'early', 'life', 'and', 'sexual', 'abuse', 'at', 'the', 'hands', 'of', 'his', 'brutal', 'parents', ',', 'Noon', 'is', 'captivated', 'and', 'wants', 'to', 'help', '.', 'However', ',', 'things', 'are', 'not', 'what', 'they', 'seem', 'and', 'Noon', 'soon', 'finds', 'himself', 'en', '-', 'wrapped', 'in', 'an', 'elusive', 'and', 'bizarre', 'tale', 'torn', 'right', 'out', 'of', 'a', 'medical', 'nightmare', '.', 'This', 'movie', 'is', 'pure', 'Robin', 'Williams', 'and', 'were', 'it', 'not', 'for', 'Toni', 'Collette', 'who', 'plays', 'Donna', 'D', '.', 'Logand', ',', 'Sandra', 'Oh', 'as', 'Anna', 'and', 'John', 'Cullum', 'as', 'pop', ',', 'this', 'might', 'be', 'comical', '.', 'Instead', ',', 'this', 'may', 'prove', 'to', 'be', 'one', 'of', 'William', \"'\", 's', 'more', 'serious', 'performances', '.', '***']\n",
            "Sample 3 Decoded string: There are many illnesses born in the mind of man which have been given life in modern times . Constant vigilance or accrued information in the realm of Pyschosis , have kept psychologists , counselors and psychiatrists busy with enough work to last them decades . Occasionally , some of these mental phenomenon are discover by those with no knowledge of their remedy or even of their existence . That is the premise of the film entitled \" The Night Listner .\" It tells the story of a popular radio host called Gabriel Noon ( Robin Williams ) who spends his evenings enthralling his audiences with vivid stories about Gay lifestyles . Perhaps its because his show is losing it ' s authentic veneer which causes Noon to admit he is no longer himself . Feeling abandoned by both his lover Jess ( Bobby Cannavale ) and his and best friend ( Joe Morton ), he seeks shelter in his deepening despair and isolation . It is here , a mysterious voice in the night asks him for help . Noon needs to feel useful and reaches out to the desperate voice which belongs to a 14 year old boy called Peter ( Rory Culkin ). In reading the boy ' s harrowing manuscript which depicts the early life and sexual abuse at the hands of his brutal parents , Noon is captivated and wants to help . However , things are not what they seem and Noon soon finds himself en - wrapped in an elusive and bizarre tale torn right out of a medical nightmare . This movie is pure Robin Williams and were it not for Toni Collette who plays Donna D . Logand , Sandra Oh as Anna and John Cullum as pop , this might be comical . Instead , this may prove to be one of William ' s more serious performances . ***\n",
            "================================================================================\n",
            "Sample 4 Original string: After a brief prologue showing a masked man stalking and then slashing the throat of an older gentleman on a deserted, urban, turn of the century Australian street, we meet Julie (Rebecca Gibney) and Peter (John Adam) as they go out house hunting. They manage to get a loan for a fixer-upper on a posh Sydney street, but it turns out that physical disrepair is not the only problem with their new home. It just may be haunted.<br /><br />13 Gantry Row combines a memorable if somewhat clichéd story with good to average direction by Catherine Millar into a slightly above average shocker.<br /><br />The biggest flaws seem partially due to budget, but not wholly excusable to that hurdle. A crucial problem occurs at the beginning of the film. The opening \"thriller scene\" features some wonky editing. Freeze frames and series of stills are used to cover up the fact that there's not much action. Suspense should be created from staging, not fancy \"fix it in the mix\" techniques. There is great atmosphere in the scene from the location, the lighting, the fog and such, but the camera should be slowly following the killer and the victim, cutting back and forth from one to the other as we track down the street, showing their increasing proximity. The tracking and the cuts need to be slow. The attack needed to be longer, clearer and better blocked. As it stands, the scene has a strong \"made for television\" feel, and a low budget one at that.<br /><br />After this scene we move to the present and the flow of the film greatly improves. The story has a lot of similarities to The Amityville Horror (1979), though the budget forces a much subtler approach. Millar and scriptwriter Tony Morphett effectively create a lot of slyly creepy scenarios, often dramatic in nature instead of special effects-oriented, such as the mysterious man who arrives to take away the old slabs of iron, which had been bizarrely affixed to an interior wall.<br /><br />For some horror fans, the first section of the film might be a little heavy on realist drama. At least the first half hour of the film is primarily about Julie and Peter trying to arrange financing for the house and then trying to settle in. But Morphett writes fine, intelligent dialogue. The material is done well enough that it's often as suspenseful as the more traditional thriller aspects that arise later--especially if you've gone through similar travails while trying to buy your own house.<br /><br />Once they get settled and things begin to get weirder, even though the special effects often leave much to be desired, the ideas are good. The performances help create tension. There isn't an abundance of death and destruction in the film--there's more of an abundance of home repair nightmares. But neither menace is really the point.<br /><br />The point is human relationships. There are a number of character arcs that are very interesting. The house exists more as a metaphor and a catalyst for stress in a romantic relationship that can make it go sour and possibly destroy it. That it's in a posh neighborhood, and that the relationship is between two successful yuppies, shows that these problems do not only afflict those who can place blame with some external woe, such as money or health problems. Peter's character evolves from a striving corporate employee with \"normal\" work-based friendships to someone with more desperation as he becomes subversive, scheming to attain something more liberating and meaningful. At the same time, we learn just how shallow those professional friendships can be. Julie goes through an almost literal nervous breakdown, but finally finds liberation when she liberates herself from her failing romantic relationship.<br /><br />Although 13 Gantry Row never quite transcends its made-for-television clunkiness, as a TV movie, this is a pretty good one, with admirable ambitions. Anyone fond of haunted house films, psycho films or horror/thrillers with a bit more metaphorical depth should find plenty to enjoy. It certainly isn't worth spending $30 for a DVD (that was the price my local PBS station was asking for a copy of the film after they showed it (factoring in shipping and handling)), but it's worth a rental, and it's definitely worth watching for free.\n",
            "Sample 4 Encoded tokens (token IDs): [1196, 66, 2834, 13047, 2202, 66, 21556, 222, 14721, 134, 464, 13739, 118, 6444, 135, 119, 2694, 11443, 123, 66, 13847, 16, 4958, 16, 575, 135, 118, 4314, 2374, 1710, 16, 272, 1182, 3759, 12, 7234, 11622, 13, 134, 2168, 12, 850, 5325, 13, 130, 252, 296, 192, 924, 9876, 18, 682, 4540, 132, 280, 66, 10231, 162, 66, 23160, 17, 5213, 123, 66, 8681, 11161, 1710, 16, 207, 133, 1539, 192, 156, 7087, 19414, 121, 212, 118, 343, 1743, 186, 352, 560, 1288, 18, 267, 284, 578, 149, 3477, 189, 140, 170, 140, 143, 3304, 11885, 12027, 14210, 66, 2650, 266, 1869, 7027, 309, 186, 299, 132, 3079, 1347, 247, 11611, 11976, 404, 66, 2173, 2014, 3079, 21278, 189, 140, 170, 140, 143, 175, 2775, 3179, 508, 13251, 2160, 132, 1224, 16, 207, 212, 10240, 20821, 132, 156, 23369, 18, 37, 9090, 1743, 7068, 125, 118, 1139, 135, 118, 184, 18, 175, 1846, 6, 1847, 527, 6, 3395, 251, 19868, 2923, 18, 20943, 13586, 134, 629, 135, 13363, 187, 791, 132, 884, 277, 118, 631, 156, 329, 11, 84, 212, 408, 546, 18, 14205, 512, 149, 1903, 271, 17957, 16, 212, 10632, 6, 4596, 133, 117, 118, 2394, 6, 7186, 18, 572, 121, 402, 2433, 117, 118, 527, 271, 118, 4614, 16, 118, 4888, 16, 118, 17077, 134, 568, 16, 207, 118, 1411, 512, 149, 4623, 3171, 118, 2208, 134, 118, 2953, 16, 6902, 488, 134, 3640, 271, 190, 132, 118, 297, 130, 272, 2063, 616, 118, 1710, 16, 2202, 352, 23585, 26723, 18, 175, 21715, 134, 118, 4905, 721, 132, 149, 1425, 18, 175, 2308, 2482, 132, 149, 3261, 16, 21046, 134, 516, 16946, 18, 630, 133, 2770, 16, 118, 527, 288, 66, 1370, 6, 434, 162, 2796, 6, 597, 16, 134, 66, 531, 1224, 190, 125, 156, 189, 140, 170, 140, 143, 1196, 164, 527, 272, 2188, 132, 118, 1203, 134, 118, 4073, 135, 118, 184, 7663, 18640, 18, 175, 309, 288, 66, 603, 135, 9130, 132, 175, 21852, 3070, 12, 10882, 495, 480, 118, 1224, 4822, 66, 408, 24927, 3069, 18, 11976, 134, 13476, 7352, 15791, 7980, 2560, 66, 603, 135, 24792, 3296, 11686, 16, 1341, 2701, 117, 2427, 1130, 135, 981, 855, 17, 7130, 16, 568, 130, 118, 4971, 222, 238, 7067, 132, 825, 843, 118, 454, 20828, 135, 18501, 16, 349, 334, 421, 9302, 19364, 132, 119, 7676, 3824, 189, 140, 170, 140, 143, 879, 251, 995, 1393, 16, 118, 411, 4813, 135, 118, 184, 718, 149, 66, 511, 3182, 123, 19563, 1301, 18, 1017, 816, 118, 411, 1054, 1079, 135, 118, 184, 121, 6944, 286, 3759, 134, 2168, 958, 132, 10570, 23414, 162, 118, 924, 134, 464, 958, 132, 13338, 117, 18, 456, 15791, 8863, 1475, 16, 2320, 1331, 18, 175, 2222, 121, 677, 423, 795, 156, 133, 11, 84, 1341, 130, 5764, 130, 118, 311, 3407, 1847, 3937, 156, 12579, 1035, 570, 992, 266, 196, 11, 154, 1968, 542, 2314, 15747, 590, 958, 132, 2422, 517, 330, 924, 189, 140, 170, 140, 143, 3276, 252, 280, 14680, 134, 702, 959, 132, 280, 25455, 16, 335, 480, 118, 981, 855, 1341, 1726, 408, 132, 149, 6871, 16, 118, 2626, 187, 299, 18, 175, 1194, 1005, 2560, 3109, 18, 572, 854, 11, 85, 119, 8195, 135, 1522, 134, 5404, 117, 118, 184, 570, 329, 11, 84, 311, 135, 119, 8195, 135, 1288, 6694, 8101, 18, 456, 3487, 6931, 121, 381, 118, 493, 189, 140, 170, 140, 143, 175, 493, 121, 878, 4636, 18, 572, 187, 66, 1910, 135, 498, 8567, 156, 187, 244, 772, 18, 175, 924, 5609, 311, 130, 66, 7210, 134, 66, 27060, 162, 10055, 117, 66, 1323, 2322, 156, 291, 445, 133, 296, 4015, 134, 3163, 4981, 133, 18, 871, 133, 11, 84, 117, 66, 8681, 8176, 16, 134, 156, 118, 2322, 121, 777, 483, 3063, 11306, 16, 1120, 156, 571, 2082, 242, 212, 343, 26748, 611, 238, 291, 967, 3675, 186, 251, 26661, 10933, 16, 568, 130, 998, 126, 4251, 2082, 18, 2168, 11, 84, 498, 11074, 271, 66, 21670, 15462, 11596, 186, 6, 5297, 6, 460, 17, 1305, 10513, 132, 1090, 186, 311, 7905, 130, 142, 1894, 13528, 16, 14830, 132, 12533, 658, 311, 22542, 134, 4956, 18, 1017, 118, 613, 313, 16, 272, 1674, 284, 361, 4047, 611, 3745, 10513, 291, 149, 18, 3759, 1060, 542, 119, 881, 7918, 10493, 10701, 16, 207, 1435, 2564, 14472, 333, 359, 22543, 2756, 271, 205, 8987, 1323, 2322, 189, 140, 170, 140, 143, 1952, 3304, 11885, 12027, 487, 792, 9494, 362, 434, 17, 162, 17, 2796, 14775, 16, 130, 66, 834, 182, 16, 164, 121, 66, 706, 299, 190, 16, 186, 11293, 11277, 18, 5759, 5493, 135, 3477, 924, 474, 16, 2497, 474, 126, 995, 19, 8983, 186, 66, 686, 311, 24011, 3404, 512, 582, 3351, 132, 637, 18, 267, 1589, 854, 11, 85, 817, 6191, 8, 1784, 162, 66, 996, 12, 156, 171, 118, 4305, 303, 2216, 6600, 3472, 171, 5154, 162, 66, 2488, 135, 118, 184, 499, 252, 3141, 133, 12, 23419, 117, 22056, 134, 6901, 15876, 207, 133, 11, 84, 817, 66, 4920, 16, 134, 133, 11, 84, 1281, 817, 649, 162, 1812, 18]\n",
            "Sample 4 Tokenized string (tokens): ['After', 'a', 'brief', 'prologue', 'showing', 'a', 'masked', 'man', 'stalking', 'and', 'then', 'slashing', 'the', 'throat', 'of', 'an', 'older', 'gentleman', 'on', 'a', 'deserted', ',', 'urban', ',', 'turn', 'of', 'the', 'century', 'Australian', 'street', ',', 'we', 'meet', 'Julie', '(', 'Rebecca', 'Gibney', ')', 'and', 'Peter', '(', 'John', 'Adam', ')', 'as', 'they', 'go', 'out', 'house', 'hunting', '.', 'They', 'manage', 'to', 'get', 'a', 'loan', 'for', 'a', 'fixer', '-', 'upper', 'on', 'a', 'posh', 'Sydney', 'street', ',', 'but', 'it', 'turns', 'out', 'that', 'physical', 'disrepair', 'is', 'not', 'the', 'only', 'problem', 'with', 'their', 'new', 'home', '.', 'It', 'just', 'may', 'be', 'haunted', '.<', 'br', '/><', 'br', '/>', '13', 'Gantry', 'Row', 'combines', 'a', 'memorable', 'if', 'somewhat', 'clichéd', 'story', 'with', 'good', 'to', 'average', 'direction', 'by', 'Catherine', 'Millar', 'into', 'a', 'slightly', 'above', 'average', 'shocker', '.<', 'br', '/><', 'br', '/>', 'The', 'biggest', 'flaws', 'seem', 'partially', 'due', 'to', 'budget', ',', 'but', 'not', 'wholly', 'excusable', 'to', 'that', 'hurdle', '.', 'A', 'crucial', 'problem', 'occurs', 'at', 'the', 'beginning', 'of', 'the', 'film', '.', 'The', 'opening', '\"', 'thriller', 'scene', '\"', 'features', 'some', 'wonky', 'editing', '.', 'Freeze', 'frames', 'and', 'series', 'of', 'stills', 'are', 'used', 'to', 'cover', 'up', 'the', 'fact', 'that', 'there', \"'\", 's', 'not', 'much', 'action', '.', 'Suspense', 'should', 'be', 'created', 'from', 'staging', ',', 'not', 'fancy', '\"', 'fix', 'it', 'in', 'the', 'mix', '\"', 'techniques', '.', 'There', 'is', 'great', 'atmosphere', 'in', 'the', 'scene', 'from', 'the', 'location', ',', 'the', 'lighting', ',', 'the', 'fog', 'and', 'such', ',', 'but', 'the', 'camera', 'should', 'be', 'slowly', 'following', 'the', 'killer', 'and', 'the', 'victim', ',', 'cutting', 'back', 'and', 'forth', 'from', 'one', 'to', 'the', 'other', 'as', 'we', 'track', 'down', 'the', 'street', ',', 'showing', 'their', 'increasing', 'proximity', '.', 'The', 'tracking', 'and', 'the', 'cuts', 'need', 'to', 'be', 'slow', '.', 'The', 'attack', 'needed', 'to', 'be', 'longer', ',', 'clearer', 'and', 'better', 'blocked', '.', 'As', 'it', 'stands', ',', 'the', 'scene', 'has', 'a', 'strong', '\"', 'made', 'for', 'television', '\"', 'feel', ',', 'and', 'a', 'low', 'budget', 'one', 'at', 'that', '.<', 'br', '/><', 'br', '/>', 'After', 'this', 'scene', 'we', 'move', 'to', 'the', 'present', 'and', 'the', 'flow', 'of', 'the', 'film', 'greatly', 'improves', '.', 'The', 'story', 'has', 'a', 'lot', 'of', 'similarities', 'to', 'The', 'Amityville', 'Horror', '(', '1979', '),', 'though', 'the', 'budget', 'forces', 'a', 'much', 'subtler', 'approach', '.', 'Millar', 'and', 'scriptwriter', 'Tony', 'Morphett', 'effectively', 'create', 'a', 'lot', 'of', 'slyly', 'creepy', 'scenarios', ',', 'often', 'dramatic', 'in', 'nature', 'instead', 'of', 'special', 'effects', '-', 'oriented', ',', 'such', 'as', 'the', 'mysterious', 'man', 'who', 'arrives', 'to', 'take', 'away', 'the', 'old', 'slabs', 'of', 'iron', ',', 'which', 'had', 'been', 'bizarrely', 'affixed', 'to', 'an', 'interior', 'wall', '.<', 'br', '/><', 'br', '/>', 'For', 'some', 'horror', 'fans', ',', 'the', 'first', 'section', 'of', 'the', 'film', 'might', 'be', 'a', 'little', 'heavy', 'on', 'realist', 'drama', '.', 'At', 'least', 'the', 'first', 'half', 'hour', 'of', 'the', 'film', 'is', 'primarily', 'about', 'Julie', 'and', 'Peter', 'trying', 'to', 'arrange', 'financing', 'for', 'the', 'house', 'and', 'then', 'trying', 'to', 'settle', 'in', '.', 'But', 'Morphett', 'writes', 'fine', ',', 'intelligent', 'dialogue', '.', 'The', 'material', 'is', 'done', 'well', 'enough', 'that', 'it', \"'\", 's', 'often', 'as', 'suspenseful', 'as', 'the', 'more', 'traditional', 'thriller', 'aspects', 'that', 'arise', 'later', '--', 'especially', 'if', 'you', \"'\", 've', 'gone', 'through', 'similar', 'travails', 'while', 'trying', 'to', 'buy', 'your', 'own', 'house', '.<', 'br', '/><', 'br', '/>', 'Once', 'they', 'get', 'settled', 'and', 'things', 'begin', 'to', 'get', 'weirder', ',', 'even', 'though', 'the', 'special', 'effects', 'often', 'leave', 'much', 'to', 'be', 'desired', ',', 'the', 'ideas', 'are', 'good', '.', 'The', 'performances', 'help', 'create', 'tension', '.', 'There', 'isn', \"'\", 't', 'an', 'abundance', 'of', 'death', 'and', 'destruction', 'in', 'the', 'film', '--', 'there', \"'\", 's', 'more', 'of', 'an', 'abundance', 'of', 'home', 'repair', 'nightmares', '.', 'But', 'neither', 'menace', 'is', 'really', 'the', 'point', '.<', 'br', '/><', 'br', '/>', 'The', 'point', 'is', 'human', 'relationships', '.', 'There', 'are', 'a', 'number', 'of', 'character', 'arcs', 'that', 'are', 'very', 'interesting', '.', 'The', 'house', 'exists', 'more', 'as', 'a', 'metaphor', 'and', 'a', 'catalyst', 'for', 'stress', 'in', 'a', 'romantic', 'relationship', 'that', 'can', 'make', 'it', 'go', 'sour', 'and', 'possibly', 'destroy', 'it', '.', 'That', 'it', \"'\", 's', 'in', 'a', 'posh', 'neighborhood', ',', 'and', 'that', 'the', 'relationship', 'is', 'between', 'two', 'successful', 'yuppies', ',', 'shows', 'that', 'these', 'problems', 'do', 'not', 'only', 'afflict', 'those', 'who', 'can', 'place', 'blame', 'with', 'some', 'external', 'woe', ',', 'such', 'as', 'money', 'or', 'health', 'problems', '.', 'Peter', \"'\", 's', 'character', 'evolves', 'from', 'a', 'striving', 'corporate', 'employee', 'with', '\"', 'normal', '\"', 'work', '-', 'based', 'friendships', 'to', 'someone', 'with', 'more', 'desperation', 'as', 'he', 'becomes', 'subversive', ',', 'scheming', 'to', 'attain', 'something', 'more', 'liberating', 'and', 'meaningful', '.', 'At', 'the', 'same', 'time', ',', 'we', 'learn', 'just', 'how', 'shallow', 'those', 'professional', 'friendships', 'can', 'be', '.', 'Julie', 'goes', 'through', 'an', 'almost', 'literal', 'nervous', 'breakdown', ',', 'but', 'finally', 'finds', 'liberation', 'when', 'she', 'liberates', 'herself', 'from', 'her', 'failing', 'romantic', 'relationship', '.<', 'br', '/><', 'br', '/>', 'Although', '13', 'Gantry', 'Row', 'never', 'quite', 'transcends', 'its', 'made', '-', 'for', '-', 'television', 'clunkiness', ',', 'as', 'a', 'TV', 'movie', ',', 'this', 'is', 'a', 'pretty', 'good', 'one', ',', 'with', 'admirable', 'ambitions', '.', 'Anyone', 'fond', 'of', 'haunted', 'house', 'films', ',', 'psycho', 'films', 'or', 'horror', '/', 'thrillers', 'with', 'a', 'bit', 'more', 'metaphorical', 'depth', 'should', 'find', 'plenty', 'to', 'enjoy', '.', 'It', 'certainly', 'isn', \"'\", 't', 'worth', 'spending', '$', '30', 'for', 'a', 'DVD', '(', 'that', 'was', 'the', 'price', 'my', 'local', 'PBS', 'station', 'was', 'asking', 'for', 'a', 'copy', 'of', 'the', 'film', 'after', 'they', 'showed', 'it', '(', 'factoring', 'in', 'shipping', 'and', 'handling', ')),', 'but', 'it', \"'\", 's', 'worth', 'a', 'rental', ',', 'and', 'it', \"'\", 's', 'definitely', 'worth', 'watching', 'for', 'free', '.']\n",
            "Sample 4 Decoded string: After a brief prologue showing a masked man stalking and then slashing the throat of an older gentleman on a deserted , urban , turn of the century Australian street , we meet Julie ( Rebecca Gibney ) and Peter ( John Adam ) as they go out house hunting . They manage to get a loan for a fixer - upper on a posh Sydney street , but it turns out that physical disrepair is not the only problem with their new home . It just may be haunted .< br />< br /> 13 Gantry Row combines a memorable if somewhat clichéd story with good to average direction by Catherine Millar into a slightly above average shocker .< br />< br /> The biggest flaws seem partially due to budget , but not wholly excusable to that hurdle . A crucial problem occurs at the beginning of the film . The opening \" thriller scene \" features some wonky editing . Freeze frames and series of stills are used to cover up the fact that there ' s not much action . Suspense should be created from staging , not fancy \" fix it in the mix \" techniques . There is great atmosphere in the scene from the location , the lighting , the fog and such , but the camera should be slowly following the killer and the victim , cutting back and forth from one to the other as we track down the street , showing their increasing proximity . The tracking and the cuts need to be slow . The attack needed to be longer , clearer and better blocked . As it stands , the scene has a strong \" made for television \" feel , and a low budget one at that .< br />< br /> After this scene we move to the present and the flow of the film greatly improves . The story has a lot of similarities to The Amityville Horror ( 1979 ), though the budget forces a much subtler approach . Millar and scriptwriter Tony Morphett effectively create a lot of slyly creepy scenarios , often dramatic in nature instead of special effects - oriented , such as the mysterious man who arrives to take away the old slabs of iron , which had been bizarrely affixed to an interior wall .< br />< br /> For some horror fans , the first section of the film might be a little heavy on realist drama . At least the first half hour of the film is primarily about Julie and Peter trying to arrange financing for the house and then trying to settle in . But Morphett writes fine , intelligent dialogue . The material is done well enough that it ' s often as suspenseful as the more traditional thriller aspects that arise later -- especially if you ' ve gone through similar travails while trying to buy your own house .< br />< br /> Once they get settled and things begin to get weirder , even though the special effects often leave much to be desired , the ideas are good . The performances help create tension . There isn ' t an abundance of death and destruction in the film -- there ' s more of an abundance of home repair nightmares . But neither menace is really the point .< br />< br /> The point is human relationships . There are a number of character arcs that are very interesting . The house exists more as a metaphor and a catalyst for stress in a romantic relationship that can make it go sour and possibly destroy it . That it ' s in a posh neighborhood , and that the relationship is between two successful yuppies , shows that these problems do not only afflict those who can place blame with some external woe , such as money or health problems . Peter ' s character evolves from a striving corporate employee with \" normal \" work - based friendships to someone with more desperation as he becomes subversive , scheming to attain something more liberating and meaningful . At the same time , we learn just how shallow those professional friendships can be . Julie goes through an almost literal nervous breakdown , but finally finds liberation when she liberates herself from her failing romantic relationship .< br />< br /> Although 13 Gantry Row never quite transcends its made - for - television clunkiness , as a TV movie , this is a pretty good one , with admirable ambitions . Anyone fond of haunted house films , psycho films or horror / thrillers with a bit more metaphorical depth should find plenty to enjoy . It certainly isn ' t worth spending $ 30 for a DVD ( that was the price my local PBS station was asking for a copy of the film after they showed it ( factoring in shipping and handling )), but it ' s worth a rental , and it ' s definitely worth watching for free .\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Add a pre-tokenizer to handle whitespace properly\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize a BPE trainer with some default parameters\n",
        "trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"__ellipsis__\"])\n",
        "\n",
        "# Load the training texts into the training data\n",
        "training_data = [load_text(path) for path in data_train['path'].values]\n",
        "\n",
        "# Train the tokenizer on the cleaned text data\n",
        "tokenizer.train_from_iterator(training_data, trainer)\n",
        "\n",
        "# Encode the text data\n",
        "encoded_train = [tokenizer.encode(load_text(path)).ids for path in data_train['path'].values]\n",
        "encoded_test = [tokenizer.encode(load_text(path)).ids for path in data_test['path'].values]\n",
        "\n",
        "# Print a few text to evaluate the quality\n",
        "sample_indices = [0, 6, 12, 55]\n",
        "\n",
        "for i, sample_index in enumerate(sample_indices):\n",
        "    # Load the actual text from the file path\n",
        "    original_text = load_text(data_train['path'][sample_index])\n",
        "    encoded_sample = tokenizer.encode(original_text)\n",
        "    decoded_text = tokenizer.decode(encoded_sample.ids)\n",
        "    \n",
        "    print(f\"Sample {i+1} Original string:\", original_text)  # Original text for comparison\n",
        "    print(f\"Sample {i+1} Encoded tokens (token IDs):\", encoded_sample.ids)  # Encoded token IDs\n",
        "    print(f\"Sample {i+1} Tokenized string (tokens):\", encoded_sample.tokens)  # Encoded tokens as text\n",
        "    print(f\"Sample {i+1} Decoded string:\", decoded_text)  # Decoded string\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2c) Analysis of BPE Tokenizer Text Results\n",
        "\n",
        "The following observations were made:\n",
        "- **HTML tags and special characters** (e.g., `<br />`)\n",
        "- **Ellipses and symbols** (e.g., `...`, `.........`, `***`)\n",
        "- **Contractions** (e.g., \"isn't\", \"don’t\", \"I’m\" are still not opened but split)\n",
        "- **Hyphenated words** (e.g., \"en-wrapped\")\n",
        "- **Complex formatting and annotations** (e.g., \"The Night Listner.\")\n",
        "- **Long sentences and informal structure** (e.g., lack of clarity, informal style)\n",
        "\n",
        "Next steps:\n",
        "To improve the quality and consistency of our text data, we apply a series of normalization steps. These steps address common issues such as HTML tags, punctuation inconsistencies, contractions, informal language, and ellipses. By cleaning and standardizing the text, we aim to create a more accurate and meaningful representation for downstream processing and analysis.\n",
        "\n",
        "Stemming is not necessary, as BPE is a subword-level Tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Calculate the Accuracy of the BPE Tokenizer with Bag-of-Words and SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bpe_tokenizer(text):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    return encoded.tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize CountVectorizer with the BPE tokenizer\n",
        "bow = CountVectorizer(\n",
        "    preprocessor=load_text,  # Convert file paths to text content\n",
        "    tokenizer=bpe_tokenizer, # Use the in-memory BPE tokenizer\n",
        "    token_pattern=None       # Disable default token pattern\n",
        ")\n",
        "\n",
        "# Create Bag-of-Words embeddings\n",
        "embeddings_train = bow.fit_transform(data_train['path'].values)\n",
        "embeddings_test = bow.transform(data_test['path'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.781\n"
          ]
        }
      ],
      "source": [
        "# Initialize the SVM classifier\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# Train the classifier on the training embeddings and labels\n",
        "svm.fit(embeddings_train, data_train['label'].values)\n",
        "\n",
        "# Use the trained classifier to predict the test data labels\n",
        "predictions = svm.predict(embeddings_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(data_test['label'].values, predictions)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "666GKq5Cug-Z"
      },
      "source": [
        "### Task 2d: Normalization of the text\n",
        "\n",
        "This section focuses on preprocessing the texts to make them more suitable for text mining. Identified constraints from the analysis step will be addressed and, as far as possible, eliminated to improve processing accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_Zgm5pNuGJNM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    # Regex to match HTML tags\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZR8XyEHiGb94"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation_and_symbols(text):\n",
        "    # Remove punctuation and special symbols using regex\n",
        "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return clean_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eYMTaZhZHOlZ"
      },
      "outputs": [],
      "source": [
        "def remove_parentheses(text):\n",
        "    # Remove text inside parentheses along with parentheses\n",
        "    clean_text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9PoskZmug-b"
      },
      "source": [
        "#### Expand the contractions\n",
        "There are python libraries that focus on expanding contractions. For easier loading (if these packages are not available) we decided to create our own replacement list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NaTBJ0qfG5uy"
      },
      "outputs": [],
      "source": [
        "# Dictionary of common contractions and their expanded forms\n",
        "contractions_dict = {\n",
        "    \"i'm\": \"I am\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"i've\": \"I have\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"i'll\": \"I will\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"here's\": \"here is\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"shan't\": \"shall not\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b', flags=re.IGNORECASE)\n",
        "    def replace(match):\n",
        "        return contractions_dict.get(match.group(0).lower(), match.group(0))\n",
        "    return contractions_pattern.sub(replace, text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zrIoCD2IHqLy"
      },
      "outputs": [],
      "source": [
        "# Define a set of informal tokens to remove\n",
        "informal_tokens = {\n",
        "    \"u\": \"you\",\n",
        "    \"r\": \"are\",\n",
        "    \"lmao\": \"\",  # Remove\n",
        "    \"lol\": \"\",   # Remove\n",
        "    \"btw\": \"by the way\",\n",
        "    \"idk\": \"I do not know\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"gonna\": \"going to\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"gotta\": \"got to\",\n",
        "    \"b/c\": \"because\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"pls\": \"please\",\n",
        "    \"cuz\": \"because\",\n",
        "    \"wut\": \"what\",\n",
        "    \"smh\": \"\",  # Remove\n",
        "    \"k\": \"okay\",\n",
        "    \"ttyl\": \"talk to you later\"\n",
        "}\n",
        "\n",
        "def remove_informal_tokens(text):\n",
        "    # Replace informal tokens\n",
        "    for token, replacement in informal_tokens.items():\n",
        "        # Use regex to match whole words and replace them\n",
        "        text = re.sub(r'\\b' + re.escape(token) + r'\\b', replacement, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def fix_punctuation(text):\n",
        "    # Ensure proper spacing after punctuation\n",
        "    text = re.sub(r'\\s*([.,;:!?()])\\s*', r'\\1 ', text)  # Ensure space after punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = text.strip()  # Trim leading and trailing spaces\n",
        "    return text\n",
        "\n",
        "# Combined function to clean the text\n",
        "def clean_text(text):\n",
        "    text = remove_informal_tokens(text)  # Remove informal tokens\n",
        "    text = fix_punctuation(text)  # Fix punctuation\n",
        "    return text\n",
        "\n",
        "def preprocess_ellipses(text):\n",
        "    # Replace occurrences of three or more dots with actual ellipses\n",
        "    text = re.sub(r'(\\.\\s*){3,}', '...', text)\n",
        "    return text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81A7y_1sug-c",
        "outputId": "34d3e3d4-c1a0-4a0b-b421-28cfe52439f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kentf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = load_text(text)              # Load text from file path\n",
        "    text = expand_contractions(text)    # Expand contractions\n",
        "    text = remove_informal_tokens(text) # Replace informal tokens\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words) # Remove stopwords\n",
        "    text = fix_punctuation(text)        # Fix punctuation\n",
        "    text = preprocess_ellipses(text)    # Handle ellipses as a single token\n",
        "    text = re.sub(r'<.*?>', '', text)   # Remove HTML tags\n",
        "\n",
        "    return text  # Return the fully preprocessed and stemmed text\n",
        "\n",
        "# Apply preprocessing function to training and test data\n",
        "data_train['cleaned_text'] = data_train['path'].apply(preprocess_text)\n",
        "data_test['cleaned_text'] = data_test['path'].apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7n0tlJZug-j"
      },
      "source": [
        "#### Rerun the BPE Tokenizer on the normalized text, bag-of-words and SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9AfjxXX1AQf"
      },
      "source": [
        "As the text was previously normalized / or not in the other case, the next step is to tokenize the text.\n",
        "For this, we are employing the tokenizer with BPE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2LtJwsVeG6",
        "outputId": "9aab73cf-43b6-4f89-df74-98296e8a2179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1 Original string: Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
            "Sample 1 Cleaned string: Bromwell High cartoon comedy. It ran time programs school life, \"Teachers\". My 35 years teaching profession lead believe Bromwell High's satire much closer reality \"Teachers\". The scramble survive financially, insightful students see right pathetic teachers' pomp, pettiness whole situation, remind schools I knew students. When I saw episode student repeatedly tried burn school, I immediately recalled...High. A classic line: INSPECTOR: I sack one teachers. STUDENT: Welcome Bromwell High. I expect many adults age think Bromwell High far fetched. What pity not!\n",
            "Sample 1 Encoded tokens (token IDs): [9279, 2813, 2979, 829, 13, 273, 2901, 267, 11622, 1688, 426, 11, 1, 13951, 746, 914, 6437, 504, 14371, 6962, 684, 768, 9279, 2813, 6, 77, 8955, 339, 7045, 2549, 1, 13951, 746, 152, 26480, 4978, 11218, 11, 9160, 4899, 258, 506, 2497, 9151, 6, 8104, 11, 21456, 832, 1721, 11, 1772, 13911, 38, 2651, 4899, 13, 903, 38, 628, 693, 4300, 6365, 2089, 2253, 1688, 11, 38, 3868, 12469, 284, 2813, 13, 30, 1348, 670, 25, 26715, 25, 38, 12340, 160, 9151, 13, 15368, 25, 15559, 9279, 2813, 13, 38, 578, 412, 2917, 342, 316, 9279, 2813, 683, 9379, 13, 631, 5429, 333, 0]\n",
            "Sample 1 Tokenized string (tokens): ['Bromwell', 'High', 'cartoon', 'comedy', '.', 'It', 'ran', 'time', 'programs', 'school', 'life', ',', '\"', 'Teachers', '\".', 'My', '35', 'years', 'teaching', 'profession', 'lead', 'believe', 'Bromwell', 'High', \"'\", 's', 'satire', 'much', 'closer', 'reality', '\"', 'Teachers', '\".', 'The', 'scramble', 'survive', 'financially', ',', 'insightful', 'students', 'see', 'right', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'pettiness', 'whole', 'situation', ',', 'remind', 'schools', 'I', 'knew', 'students', '.', 'When', 'I', 'saw', 'episode', 'student', 'repeatedly', 'tried', 'burn', 'school', ',', 'I', 'immediately', 'recalled', '...', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', 'sack', 'one', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'Bromwell', 'High', '.', 'I', 'expect', 'many', 'adults', 'age', 'think', 'Bromwell', 'High', 'far', 'fetched', '.', 'What', 'pity', 'not', '!']\n",
            "Sample 1 Decoded string: Bromwell High cartoon comedy . It ran time programs school life , \" Teachers \". My 35 years teaching profession lead believe Bromwell High ' s satire much closer reality \" Teachers \". The scramble survive financially , insightful students see right pathetic teachers ' pomp , pettiness whole situation , remind schools I knew students . When I saw episode student repeatedly tried burn school , I immediately recalled ... High . A classic line : INSPECTOR : I sack one teachers . STUDENT : Welcome Bromwell High . I expect many adults age think Bromwell High far fetched . What pity not !\n",
            "================================================================================\n",
            "Sample 2 Original string: Yes its an art... to successfully make a slow paced thriller.<br /><br />The story unfolds in nice volumes while you don't even notice it happening.<br /><br />Fine performance by Robin Williams. The sexuality angles in the film can seem unnecessary and can probably affect how much you enjoy the film. However, the core plot is very engaging. The movie doesn't rush onto you and still grips you enough to keep you wondering. The direction is good. Use of lights to achieve desired affects of suspense and unexpectedness is good.<br /><br />Very nice 1 time watch if you are looking to lay back and hear a thrilling short story!\n",
            "Sample 2 Cleaned string: Yes art...successfully make slow paced thriller. The story unfolds nice volumes even notice happening. Fine performance Robin Williams. The sexuality angles film seem unnecessary probably affect much enjoy film. However, core plot engaging. The movie rush onto still grips enough keep wondering. The direction good. Use lights achieve desired affects suspense unexpectedness good. Very nice 1 time watch looking lay back hear thrilling short story!\n",
            "Sample 2 Encoded tokens (token IDs): [1870, 208, 284, 5233, 374, 1360, 3388, 1780, 13, 152, 254, 6359, 1021, 20211, 270, 2854, 3628, 13, 9562, 831, 2237, 1449, 13, 152, 5269, 6768, 159, 428, 4752, 775, 3453, 339, 557, 159, 13, 1066, 11, 2517, 419, 3063, 13, 152, 157, 6778, 3142, 513, 8023, 706, 919, 3311, 13, 152, 1286, 246, 13, 9717, 5602, 5722, 6782, 9161, 1630, 22095, 246, 13, 3714, 1021, 16, 267, 263, 1061, 3544, 418, 2404, 6881, 1093, 254, 0]\n",
            "Sample 2 Tokenized string (tokens): ['Yes', 'art', '...', 'successfully', 'make', 'slow', 'paced', 'thriller', '.', 'The', 'story', 'unfolds', 'nice', 'volumes', 'even', 'notice', 'happening', '.', 'Fine', 'performance', 'Robin', 'Williams', '.', 'The', 'sexuality', 'angles', 'film', 'seem', 'unnecessary', 'probably', 'affect', 'much', 'enjoy', 'film', '.', 'However', ',', 'core', 'plot', 'engaging', '.', 'The', 'movie', 'rush', 'onto', 'still', 'grips', 'enough', 'keep', 'wondering', '.', 'The', 'direction', 'good', '.', 'Use', 'lights', 'achieve', 'desired', 'affects', 'suspense', 'unexpectedness', 'good', '.', 'Very', 'nice', '1', 'time', 'watch', 'looking', 'lay', 'back', 'hear', 'thrilling', 'short', 'story', '!']\n",
            "Sample 2 Decoded string: Yes art ... successfully make slow paced thriller . The story unfolds nice volumes even notice happening . Fine performance Robin Williams . The sexuality angles film seem unnecessary probably affect much enjoy film . However , core plot engaging . The movie rush onto still grips enough keep wondering . The direction good . Use lights achieve desired affects suspense unexpectedness good . Very nice 1 time watch looking lay back hear thrilling short story !\n",
            "================================================================================\n",
            "Sample 3 Original string: There are many illnesses born in the mind of man which have been given life in modern times. Constant vigilance or accrued information in the realm of Pyschosis, have kept psychologists, counselors and psychiatrists busy with enough work to last them decades. Occasionally, some of these mental phenomenon are discover by those with no knowledge of their remedy or even of their existence. That is the premise of the film entitled \" The Night Listner.\" It tells the story of a popular radio host called Gabriel Noon (Robin Williams) who spends his evenings enthralling his audiences with vivid stories about Gay lifestyles. Perhaps its because his show is losing it's authentic veneer which causes Noon to admit he is no longer himself. Feeling abandoned by both his lover Jess (Bobby Cannavale) and his and best friend (Joe Morton), he seeks shelter in his deepening despair and isolation. It is here, a mysterious voice in the night asks him for help. Noon needs to feel useful and reaches out to the desperate voice which belongs to a 14 year old boy called Peter (Rory Culkin). In reading the boy's harrowing manuscript which depicts the early life and sexual abuse at the hands of his brutal parents, Noon is captivated and wants to help. However, things are not what they seem and Noon soon finds himself en-wrapped in an elusive and bizarre tale torn right out of a medical nightmare. This movie is pure Robin Williams and were it not for Toni Collette who plays Donna D. Logand, Sandra Oh as Anna and John Cullum as pop, this might be comical. Instead, this may prove to be one of William's more serious performances. ***\n",
            "Sample 3 Cleaned string: There many illnesses born mind man given life modern times. Constant vigilance accrued information realm Pyschosis, kept psychologists, counselors psychiatrists busy enough work last decades. Occasionally, mental phenomenon discover knowledge remedy even existence. That premise film entitled \" The Night Listner. \" It tells story popular radio host called Gabriel Noon( Robin Williams) spends evenings enthralling audiences vivid stories Gay lifestyles. Perhaps show losing authentic veneer causes Noon admit longer himself. Feeling abandoned lover Jess( Bobby Cannavale) best friend( Joe Morton) , seeks shelter deepening despair isolation. It here, mysterious voice night asks help. Noon needs feel useful reaches desperate voice belongs 14 year old boy called Peter( Rory Culkin) . In reading boy's harrowing manuscript depicts early life sexual abuse hands brutal parents, Noon captivated wants help. However, things seem Noon soon finds en-wrapped elusive bizarre tale torn right medical nightmare. This movie pure Robin Williams Toni Collette plays Donna D. Logand, Sandra Oh Anna John Cullum pop, might comical. Instead, may prove one William's serious performances. ***\n",
            "Sample 3 Encoded tokens (token IDs): [577, 412, 24284, 6189, 781, 191, 1024, 426, 3097, 785, 13, 21079, 15013, 26344, 3930, 5575, 24496, 11, 1811, 22116, 11, 25389, 22795, 21595, 706, 391, 698, 6818, 13, 25902, 11, 6738, 8068, 1926, 4497, 13267, 270, 3488, 13, 1080, 2361, 159, 8075, 1, 152, 1941, 22502, 13, 1, 273, 1555, 254, 2491, 3483, 2448, 1086, 4071, 6851, 7, 2237, 1449, 8, 6733, 18914, 24239, 2777, 5251, 1509, 9570, 19513, 13, 3478, 298, 5259, 6724, 19975, 4880, 6851, 1826, 3191, 3061, 13, 22814, 9342, 3041, 2512, 7, 4358, 5782, 8, 452, 587, 7, 2773, 4669, 8, 11, 7546, 23952, 21285, 8720, 14851, 13, 273, 445, 11, 4900, 1860, 976, 1920, 929, 13, 6851, 1930, 521, 7699, 7883, 2793, 1860, 6332, 2886, 379, 363, 822, 1086, 2063, 7, 5384, 4949, 8, 13, 334, 2135, 822, 6, 77, 15005, 14945, 9046, 1764, 426, 2421, 5127, 3058, 3864, 2010, 11, 6851, 10690, 1647, 929, 13, 1066, 11, 622, 428, 6851, 1763, 2481, 111, 12, 6128, 12610, 4332, 2182, 6369, 506, 7672, 8860, 13, 252, 157, 1988, 2237, 1449, 4626, 3083, 926, 4878, 33, 13, 9302, 11, 4313, 1600, 6113, 774, 15438, 1117, 11, 635, 7480, 13, 3043, 11, 499, 3310, 160, 3823, 6, 77, 1003, 1121, 13, 3051]\n",
            "Sample 3 Tokenized string (tokens): ['There', 'many', 'illnesses', 'born', 'mind', 'man', 'given', 'life', 'modern', 'times', '.', 'Constant', 'vigilance', 'accrued', 'information', 'realm', 'Pyschosis', ',', 'kept', 'psychologists', ',', 'counselors', 'psychiatrists', 'busy', 'enough', 'work', 'last', 'decades', '.', 'Occasionally', ',', 'mental', 'phenomenon', 'discover', 'knowledge', 'remedy', 'even', 'existence', '.', 'That', 'premise', 'film', 'entitled', '\"', 'The', 'Night', 'Listner', '.', '\"', 'It', 'tells', 'story', 'popular', 'radio', 'host', 'called', 'Gabriel', 'Noon', '(', 'Robin', 'Williams', ')', 'spends', 'evenings', 'enthralling', 'audiences', 'vivid', 'stories', 'Gay', 'lifestyles', '.', 'Perhaps', 'show', 'losing', 'authentic', 'veneer', 'causes', 'Noon', 'admit', 'longer', 'himself', '.', 'Feeling', 'abandoned', 'lover', 'Jess', '(', 'Bobby', 'Cannavale', ')', 'best', 'friend', '(', 'Joe', 'Morton', ')', ',', 'seeks', 'shelter', 'deepening', 'despair', 'isolation', '.', 'It', 'here', ',', 'mysterious', 'voice', 'night', 'asks', 'help', '.', 'Noon', 'needs', 'feel', 'useful', 'reaches', 'desperate', 'voice', 'belongs', '14', 'year', 'old', 'boy', 'called', 'Peter', '(', 'Rory', 'Culkin', ')', '.', 'In', 'reading', 'boy', \"'\", 's', 'harrowing', 'manuscript', 'depicts', 'early', 'life', 'sexual', 'abuse', 'hands', 'brutal', 'parents', ',', 'Noon', 'captivated', 'wants', 'help', '.', 'However', ',', 'things', 'seem', 'Noon', 'soon', 'finds', 'en', '-', 'wrapped', 'elusive', 'bizarre', 'tale', 'torn', 'right', 'medical', 'nightmare', '.', 'This', 'movie', 'pure', 'Robin', 'Williams', 'Toni', 'Collette', 'plays', 'Donna', 'D', '.', 'Logand', ',', 'Sandra', 'Oh', 'Anna', 'John', 'Cullum', 'pop', ',', 'might', 'comical', '.', 'Instead', ',', 'may', 'prove', 'one', 'William', \"'\", 's', 'serious', 'performances', '.', '***']\n",
            "Sample 3 Decoded string: There many illnesses born mind man given life modern times . Constant vigilance accrued information realm Pyschosis , kept psychologists , counselors psychiatrists busy enough work last decades . Occasionally , mental phenomenon discover knowledge remedy even existence . That premise film entitled \" The Night Listner . \" It tells story popular radio host called Gabriel Noon ( Robin Williams ) spends evenings enthralling audiences vivid stories Gay lifestyles . Perhaps show losing authentic veneer causes Noon admit longer himself . Feeling abandoned lover Jess ( Bobby Cannavale ) best friend ( Joe Morton ) , seeks shelter deepening despair isolation . It here , mysterious voice night asks help . Noon needs feel useful reaches desperate voice belongs 14 year old boy called Peter ( Rory Culkin ) . In reading boy ' s harrowing manuscript depicts early life sexual abuse hands brutal parents , Noon captivated wants help . However , things seem Noon soon finds en - wrapped elusive bizarre tale torn right medical nightmare . This movie pure Robin Williams Toni Collette plays Donna D . Logand , Sandra Oh Anna John Cullum pop , might comical . Instead , may prove one William ' s serious performances . ***\n",
            "================================================================================\n",
            "Sample 4 Original string: After a brief prologue showing a masked man stalking and then slashing the throat of an older gentleman on a deserted, urban, turn of the century Australian street, we meet Julie (Rebecca Gibney) and Peter (John Adam) as they go out house hunting. They manage to get a loan for a fixer-upper on a posh Sydney street, but it turns out that physical disrepair is not the only problem with their new home. It just may be haunted.<br /><br />13 Gantry Row combines a memorable if somewhat clichéd story with good to average direction by Catherine Millar into a slightly above average shocker.<br /><br />The biggest flaws seem partially due to budget, but not wholly excusable to that hurdle. A crucial problem occurs at the beginning of the film. The opening \"thriller scene\" features some wonky editing. Freeze frames and series of stills are used to cover up the fact that there's not much action. Suspense should be created from staging, not fancy \"fix it in the mix\" techniques. There is great atmosphere in the scene from the location, the lighting, the fog and such, but the camera should be slowly following the killer and the victim, cutting back and forth from one to the other as we track down the street, showing their increasing proximity. The tracking and the cuts need to be slow. The attack needed to be longer, clearer and better blocked. As it stands, the scene has a strong \"made for television\" feel, and a low budget one at that.<br /><br />After this scene we move to the present and the flow of the film greatly improves. The story has a lot of similarities to The Amityville Horror (1979), though the budget forces a much subtler approach. Millar and scriptwriter Tony Morphett effectively create a lot of slyly creepy scenarios, often dramatic in nature instead of special effects-oriented, such as the mysterious man who arrives to take away the old slabs of iron, which had been bizarrely affixed to an interior wall.<br /><br />For some horror fans, the first section of the film might be a little heavy on realist drama. At least the first half hour of the film is primarily about Julie and Peter trying to arrange financing for the house and then trying to settle in. But Morphett writes fine, intelligent dialogue. The material is done well enough that it's often as suspenseful as the more traditional thriller aspects that arise later--especially if you've gone through similar travails while trying to buy your own house.<br /><br />Once they get settled and things begin to get weirder, even though the special effects often leave much to be desired, the ideas are good. The performances help create tension. There isn't an abundance of death and destruction in the film--there's more of an abundance of home repair nightmares. But neither menace is really the point.<br /><br />The point is human relationships. There are a number of character arcs that are very interesting. The house exists more as a metaphor and a catalyst for stress in a romantic relationship that can make it go sour and possibly destroy it. That it's in a posh neighborhood, and that the relationship is between two successful yuppies, shows that these problems do not only afflict those who can place blame with some external woe, such as money or health problems. Peter's character evolves from a striving corporate employee with \"normal\" work-based friendships to someone with more desperation as he becomes subversive, scheming to attain something more liberating and meaningful. At the same time, we learn just how shallow those professional friendships can be. Julie goes through an almost literal nervous breakdown, but finally finds liberation when she liberates herself from her failing romantic relationship.<br /><br />Although 13 Gantry Row never quite transcends its made-for-television clunkiness, as a TV movie, this is a pretty good one, with admirable ambitions. Anyone fond of haunted house films, psycho films or horror/thrillers with a bit more metaphorical depth should find plenty to enjoy. It certainly isn't worth spending $30 for a DVD (that was the price my local PBS station was asking for a copy of the film after they showed it (factoring in shipping and handling)), but it's worth a rental, and it's definitely worth watching for free.\n",
            "Sample 4 Cleaned string: After brief prologue showing masked man stalking slashing throat older gentleman deserted, urban, turn century Australian street, meet Julie( Rebecca Gibney) Peter( John Adam) go house hunting. They manage get loan fixer-upper posh Sydney street, turns physical disrepair problem new home. It may haunted. 13 Gantry Row combines memorable somewhat clichéd story good average direction Catherine Millar slightly average shocker. The biggest flaws seem partially due budget, wholly excusable hurdle. A crucial problem occurs beginning film. The opening \"thriller scene\" features wonky editing. Freeze frames series stills used cover fact much action. Suspense created staging, fancy \"fix mix\" techniques. There great atmosphere scene location, lighting, fog such, camera slowly following killer victim, cutting back forth one track street, showing increasing proximity. The tracking cuts need slow. The attack needed longer, clearer better blocked. As stands, scene strong \"made television\" feel, low budget one that. After scene move present flow film greatly improves. The story lot similarities The Amityville Horror( 1979) , though budget forces much subtler approach. Millar scriptwriter Tony Morphett effectively create lot slyly creepy scenarios, often dramatic nature instead special effects-oriented, mysterious man arrives take away old slabs iron, bizarrely affixed interior wall. For horror fans, first section film might little heavy realist drama. At least first half hour film primarily Julie Peter trying arrange financing house trying settle in. But Morphett writes fine, intelligent dialogue. The material done well enough often suspenseful traditional thriller aspects arise later--especially gone similar travails trying buy house. Once get settled things begin get weirder, even though special effects often leave much desired, ideas good. The performances help create tension. There abundance death destruction film--there abundance home repair nightmares. But neither menace really point. The point human relationships. There number character arcs interesting. The house exists metaphor catalyst stress romantic relationship make go sour possibly destroy it. That posh neighborhood, relationship two successful yuppies, shows problems afflict place blame external woe, money health problems. Peter's character evolves striving corporate employee \"normal\" work-based friendships someone desperation becomes subversive, scheming attain something liberating meaningful. At time, learn shallow professional friendships be. Julie goes almost literal nervous breakdown, finally finds liberation liberates failing romantic relationship. Although 13 Gantry Row never quite transcends made-for-television clunkiness, TV movie, pretty good one, admirable ambitions. Anyone fond haunted house films, psycho films horror/thrillers bit metaphorical depth find plenty enjoy. It certainly worth spending $30 DVD( that price local PBS station asking copy film showed( factoring shipping handling) ) , worth rental, definitely worth watching free.\n",
            "Sample 4 Encoded tokens (token IDs): [1122, 2768, 12950, 2131, 21908, 191, 12501, 13584, 6371, 2600, 11373, 13752, 11, 4887, 11, 492, 4227, 2298, 2354, 11, 1108, 3679, 7, 7158, 11561, 8, 2063, 7, 774, 4888, 8, 255, 846, 9805, 13, 629, 4472, 229, 10686, 23044, 12, 5166, 8584, 11086, 2354, 11, 1486, 7017, 19158, 1687, 481, 1263, 13, 273, 499, 3469, 13, 3230, 11820, 11973, 14119, 2569, 1819, 6952, 254, 246, 2980, 1286, 11544, 15104, 2095, 2980, 20980, 13, 152, 2684, 3088, 428, 13175, 2082, 1151, 11, 10245, 19762, 23136, 13, 30, 9008, 1687, 6990, 1065, 159, 13, 152, 1757, 1, 1780, 442, 1, 3351, 23483, 2836, 13, 23615, 13479, 548, 13269, 703, 915, 549, 339, 462, 13, 14113, 1857, 17660, 11, 10560, 1, 4850, 2320, 1, 7111, 13, 577, 351, 2363, 442, 4552, 11, 4820, 11, 16911, 4468, 11, 1349, 4565, 3074, 2137, 2883, 11, 6823, 418, 3589, 160, 1997, 2354, 11, 2131, 20830, 26534, 13, 152, 21424, 4834, 646, 1360, 13, 152, 2228, 2415, 3191, 11, 20771, 431, 24711, 13, 555, 2680, 11, 442, 1303, 1, 361, 2708, 1, 521, 11, 449, 1151, 160, 618, 13, 1122, 442, 2268, 1127, 4502, 159, 7597, 22092, 13, 152, 254, 526, 9048, 152, 22012, 2974, 7, 10804, 8, 11, 408, 1151, 4766, 339, 21831, 2982, 13, 15104, 13390, 7270, 13596, 7899, 2478, 526, 24609, 3224, 11361, 11, 1329, 3199, 2212, 1055, 904, 780, 12, 7059, 11, 4900, 191, 6989, 739, 770, 363, 20358, 18468, 11, 9220, 26733, 7610, 3737, 13, 806, 911, 1334, 11, 343, 4743, 159, 635, 429, 3087, 19264, 1412, 13, 931, 729, 343, 975, 996, 159, 6991, 3679, 2063, 881, 10463, 23185, 846, 881, 14480, 109, 13, 386, 13596, 8785, 1176, 11, 2360, 1272, 13, 152, 2150, 595, 349, 706, 1329, 5680, 3334, 1780, 3847, 12514, 950, 487, 913, 1909, 2236, 14100, 881, 2350, 846, 13, 3197, 229, 14481, 622, 882, 229, 19524, 11, 270, 408, 904, 780, 1329, 1667, 339, 6782, 11, 2545, 246, 13, 152, 1121, 929, 2478, 3013, 13, 577, 7491, 1466, 5121, 159, 487, 1094, 7491, 1263, 6614, 7729, 13, 386, 3417, 6860, 319, 420, 13, 152, 420, 805, 4580, 13, 577, 1864, 425, 8461, 687, 13, 152, 846, 5529, 7133, 26849, 9967, 1270, 1786, 374, 255, 3923, 3066, 2803, 128, 13, 1080, 8584, 8094, 11, 1786, 411, 2967, 11240, 11, 1051, 2014, 20591, 909, 3599, 23516, 9923, 11, 916, 4201, 2014, 13, 2063, 6, 77, 425, 11152, 22353, 15370, 11250, 1, 5262, 1, 391, 12, 1241, 11159, 1012, 7839, 1852, 15705, 11, 14883, 12534, 580, 22295, 4886, 13, 931, 267, 11, 1409, 3956, 3665, 11159, 187, 13, 3679, 981, 810, 7845, 10428, 10644, 11, 1368, 2481, 14368, 22296, 8912, 1270, 1786, 13, 1892, 3230, 11820, 11973, 414, 704, 9404, 361, 12, 216, 12, 2708, 14661, 11, 749, 157, 11, 625, 246, 160, 11, 11222, 11214, 13, 5676, 5408, 3469, 846, 405, 11, 6915, 405, 911, 14, 8909, 564, 23802, 3086, 505, 3282, 557, 13, 273, 1529, 731, 6119, 3, 1724, 912, 7, 618, 4204, 2143, 6512, 3400, 5115, 2418, 159, 3040, 7, 23186, 20032, 6819, 8, 8, 11, 731, 4857, 11, 1207, 731, 571, 1796, 13]\n",
            "Sample 4 Tokenized string (tokens): ['After', 'brief', 'prologue', 'showing', 'masked', 'man', 'stalking', 'slashing', 'throat', 'older', 'gentleman', 'deserted', ',', 'urban', ',', 'turn', 'century', 'Australian', 'street', ',', 'meet', 'Julie', '(', 'Rebecca', 'Gibney', ')', 'Peter', '(', 'John', 'Adam', ')', 'go', 'house', 'hunting', '.', 'They', 'manage', 'get', 'loan', 'fixer', '-', 'upper', 'posh', 'Sydney', 'street', ',', 'turns', 'physical', 'disrepair', 'problem', 'new', 'home', '.', 'It', 'may', 'haunted', '.', '13', 'Gantry', 'Row', 'combines', 'memorable', 'somewhat', 'clichéd', 'story', 'good', 'average', 'direction', 'Catherine', 'Millar', 'slightly', 'average', 'shocker', '.', 'The', 'biggest', 'flaws', 'seem', 'partially', 'due', 'budget', ',', 'wholly', 'excusable', 'hurdle', '.', 'A', 'crucial', 'problem', 'occurs', 'beginning', 'film', '.', 'The', 'opening', '\"', 'thriller', 'scene', '\"', 'features', 'wonky', 'editing', '.', 'Freeze', 'frames', 'series', 'stills', 'used', 'cover', 'fact', 'much', 'action', '.', 'Suspense', 'created', 'staging', ',', 'fancy', '\"', 'fix', 'mix', '\"', 'techniques', '.', 'There', 'great', 'atmosphere', 'scene', 'location', ',', 'lighting', ',', 'fog', 'such', ',', 'camera', 'slowly', 'following', 'killer', 'victim', ',', 'cutting', 'back', 'forth', 'one', 'track', 'street', ',', 'showing', 'increasing', 'proximity', '.', 'The', 'tracking', 'cuts', 'need', 'slow', '.', 'The', 'attack', 'needed', 'longer', ',', 'clearer', 'better', 'blocked', '.', 'As', 'stands', ',', 'scene', 'strong', '\"', 'made', 'television', '\"', 'feel', ',', 'low', 'budget', 'one', 'that', '.', 'After', 'scene', 'move', 'present', 'flow', 'film', 'greatly', 'improves', '.', 'The', 'story', 'lot', 'similarities', 'The', 'Amityville', 'Horror', '(', '1979', ')', ',', 'though', 'budget', 'forces', 'much', 'subtler', 'approach', '.', 'Millar', 'scriptwriter', 'Tony', 'Morphett', 'effectively', 'create', 'lot', 'slyly', 'creepy', 'scenarios', ',', 'often', 'dramatic', 'nature', 'instead', 'special', 'effects', '-', 'oriented', ',', 'mysterious', 'man', 'arrives', 'take', 'away', 'old', 'slabs', 'iron', ',', 'bizarrely', 'affixed', 'interior', 'wall', '.', 'For', 'horror', 'fans', ',', 'first', 'section', 'film', 'might', 'little', 'heavy', 'realist', 'drama', '.', 'At', 'least', 'first', 'half', 'hour', 'film', 'primarily', 'Julie', 'Peter', 'trying', 'arrange', 'financing', 'house', 'trying', 'settle', 'in', '.', 'But', 'Morphett', 'writes', 'fine', ',', 'intelligent', 'dialogue', '.', 'The', 'material', 'done', 'well', 'enough', 'often', 'suspenseful', 'traditional', 'thriller', 'aspects', 'arise', 'later', '--', 'especially', 'gone', 'similar', 'travails', 'trying', 'buy', 'house', '.', 'Once', 'get', 'settled', 'things', 'begin', 'get', 'weirder', ',', 'even', 'though', 'special', 'effects', 'often', 'leave', 'much', 'desired', ',', 'ideas', 'good', '.', 'The', 'performances', 'help', 'create', 'tension', '.', 'There', 'abundance', 'death', 'destruction', 'film', '--', 'there', 'abundance', 'home', 'repair', 'nightmares', '.', 'But', 'neither', 'menace', 'really', 'point', '.', 'The', 'point', 'human', 'relationships', '.', 'There', 'number', 'character', 'arcs', 'interesting', '.', 'The', 'house', 'exists', 'metaphor', 'catalyst', 'stress', 'romantic', 'relationship', 'make', 'go', 'sour', 'possibly', 'destroy', 'it', '.', 'That', 'posh', 'neighborhood', ',', 'relationship', 'two', 'successful', 'yuppies', ',', 'shows', 'problems', 'afflict', 'place', 'blame', 'external', 'woe', ',', 'money', 'health', 'problems', '.', 'Peter', \"'\", 's', 'character', 'evolves', 'striving', 'corporate', 'employee', '\"', 'normal', '\"', 'work', '-', 'based', 'friendships', 'someone', 'desperation', 'becomes', 'subversive', ',', 'scheming', 'attain', 'something', 'liberating', 'meaningful', '.', 'At', 'time', ',', 'learn', 'shallow', 'professional', 'friendships', 'be', '.', 'Julie', 'goes', 'almost', 'literal', 'nervous', 'breakdown', ',', 'finally', 'finds', 'liberation', 'liberates', 'failing', 'romantic', 'relationship', '.', 'Although', '13', 'Gantry', 'Row', 'never', 'quite', 'transcends', 'made', '-', 'for', '-', 'television', 'clunkiness', ',', 'TV', 'movie', ',', 'pretty', 'good', 'one', ',', 'admirable', 'ambitions', '.', 'Anyone', 'fond', 'haunted', 'house', 'films', ',', 'psycho', 'films', 'horror', '/', 'thrillers', 'bit', 'metaphorical', 'depth', 'find', 'plenty', 'enjoy', '.', 'It', 'certainly', 'worth', 'spending', '$', '30', 'DVD', '(', 'that', 'price', 'local', 'PBS', 'station', 'asking', 'copy', 'film', 'showed', '(', 'factoring', 'shipping', 'handling', ')', ')', ',', 'worth', 'rental', ',', 'definitely', 'worth', 'watching', 'free', '.']\n",
            "Sample 4 Decoded string: After brief prologue showing masked man stalking slashing throat older gentleman deserted , urban , turn century Australian street , meet Julie ( Rebecca Gibney ) Peter ( John Adam ) go house hunting . They manage get loan fixer - upper posh Sydney street , turns physical disrepair problem new home . It may haunted . 13 Gantry Row combines memorable somewhat clichéd story good average direction Catherine Millar slightly average shocker . The biggest flaws seem partially due budget , wholly excusable hurdle . A crucial problem occurs beginning film . The opening \" thriller scene \" features wonky editing . Freeze frames series stills used cover fact much action . Suspense created staging , fancy \" fix mix \" techniques . There great atmosphere scene location , lighting , fog such , camera slowly following killer victim , cutting back forth one track street , showing increasing proximity . The tracking cuts need slow . The attack needed longer , clearer better blocked . As stands , scene strong \" made television \" feel , low budget one that . After scene move present flow film greatly improves . The story lot similarities The Amityville Horror ( 1979 ) , though budget forces much subtler approach . Millar scriptwriter Tony Morphett effectively create lot slyly creepy scenarios , often dramatic nature instead special effects - oriented , mysterious man arrives take away old slabs iron , bizarrely affixed interior wall . For horror fans , first section film might little heavy realist drama . At least first half hour film primarily Julie Peter trying arrange financing house trying settle in . But Morphett writes fine , intelligent dialogue . The material done well enough often suspenseful traditional thriller aspects arise later -- especially gone similar travails trying buy house . Once get settled things begin get weirder , even though special effects often leave much desired , ideas good . The performances help create tension . There abundance death destruction film -- there abundance home repair nightmares . But neither menace really point . The point human relationships . There number character arcs interesting . The house exists metaphor catalyst stress romantic relationship make go sour possibly destroy it . That posh neighborhood , relationship two successful yuppies , shows problems afflict place blame external woe , money health problems . Peter ' s character evolves striving corporate employee \" normal \" work - based friendships someone desperation becomes subversive , scheming attain something liberating meaningful . At time , learn shallow professional friendships be . Julie goes almost literal nervous breakdown , finally finds liberation liberates failing romantic relationship . Although 13 Gantry Row never quite transcends made - for - television clunkiness , TV movie , pretty good one , admirable ambitions . Anyone fond haunted house films , psycho films horror / thrillers bit metaphorical depth find plenty enjoy . It certainly worth spending $ 30 DVD ( that price local PBS station asking copy film showed ( factoring shipping handling ) ) , worth rental , definitely worth watching free .\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Add a pre-tokenizer to handle whitespace properly\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize a BPE trainer\n",
        "trainer = BpeTrainer()\n",
        "\n",
        "# Convert cleaned_text column to a list of strings for training\n",
        "training_data = data_train['cleaned_text'].tolist()\n",
        "\n",
        "# Train the tokenizer on the cleaned text data\n",
        "tokenizer.train_from_iterator(training_data, trainer)\n",
        "\n",
        "# Encode the text data for the model\n",
        "encoded_train = [tokenizer.encode(text).ids for text in data_train['cleaned_text']]\n",
        "encoded_test = [tokenizer.encode(text).ids for text in data_test['cleaned_text']]\n",
        "\n",
        "# Select a single example to decode and print\n",
        "sample_index = 0  # Choose an index, e.g., 0 for the first item\n",
        "encoded_sample = encoded_train[sample_index]\n",
        "\n",
        "# Decode the tokenized output back to text\n",
        "decoded_text = tokenizer.decode(encoded_sample)\n",
        "\n",
        "# Select additional examples to decode and print\n",
        "sample_indices = [0, 6, 12, 55]  # You can adjust these indices as needed\n",
        "\n",
        "for i, sample_index in enumerate(sample_indices):\n",
        "    original_text = load_text(data_train['path'][sample_index])\n",
        "    encoded_sample = tokenizer.encode(data_train['cleaned_text'][sample_index])\n",
        "    decoded_text = tokenizer.decode(encoded_sample.ids)\n",
        "    \n",
        "    print(f\"Sample {i+1} Original string:\", original_text)  # Original text for comparison\n",
        "    print(f\"Sample {i+1} Cleaned string:\", data_train['cleaned_text'][sample_index])  # Original text for comparison\n",
        "    print(f\"Sample {i+1} Encoded tokens (token IDs):\", encoded_sample.ids)  # Encoded token IDs\n",
        "    print(f\"Sample {i+1} Tokenized string (tokens):\", encoded_sample.tokens)  # Encoded tokens as text\n",
        "    print(f\"Sample {i+1} Decoded string:\", decoded_text)  # Decoded string\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bpe_tokenizer(text):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    return encoded.tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use preprocessed 'cleaned_text' data directly for Bag-of-Words embedding\n",
        "bow = CountVectorizer(\n",
        "    preprocessor=None,       # Text is already preprocessed in 'cleaned_text'\n",
        "    tokenizer=bpe_tokenizer, # Use the in-memory BPE tokenizer\n",
        "    token_pattern=None       # Disable default token pattern\n",
        ")\n",
        "\n",
        "# Create Bag-of-Words embeddings on preprocessed text\n",
        "embeddings_train = bow.fit_transform(data_train['cleaned_text'])\n",
        "embeddings_test = bow.transform(data_test['cleaned_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.744\n"
          ]
        }
      ],
      "source": [
        "# Initialize the SVM classifier\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# Train the classifier on the training embeddings and labels\n",
        "svm.fit(embeddings_train, data_train['label'].values)\n",
        "\n",
        "# Use the trained classifier to predict the test data labels\n",
        "predictions = svm.predict(embeddings_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(data_test['label'].values, predictions)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Analysis of the results\n",
        "\n",
        "Our Normalization made the text look nicer for human eyes but reduced the prediction accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Embedding\n",
        "In this part, the TF-IDF embedding method is applied to the dataset. We explore two scenarios: \n",
        "1. TF-IDF embedding on raw text with Byte-Pair Encoding (BPE) tokenization only.\n",
        "2. TF-IDF embedding on normalized text, where text is preprocessed with steps such as contraction expansion, stopword removal, and stemming, followed by BPE tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Shape (Raw BPE): (1000, 15000) (1000, 15000)\n",
            "TF-IDF Shape (Normalized BPE): (1000, 15000) (1000, 15000)\n"
          ]
        }
      ],
      "source": [
        "# 1. BPE Tokenization Without Normalization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Directly tokenize the raw text using the BPE tokenizer\n",
        "def bpe_tokenizer(text):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    return encoded.tokens\n",
        "\n",
        "# Initialize TF-IDF Vectorizer for raw text (no normalization)\n",
        "tfidf_vectorizer_raw = TfidfVectorizer(\n",
        "    max_features=15000,\n",
        "    ngram_range=(1, 2),\n",
        "    tokenizer=bpe_tokenizer,\n",
        "    token_pattern=None  # Disable default token pattern since we're using a custom tokenizer\n",
        ")\n",
        "\n",
        "# Apply BPE tokenization and TF-IDF on raw text\n",
        "tfidf_train_raw = tfidf_vectorizer_raw.fit_transform(data_train['path'].apply(load_text))\n",
        "tfidf_test_raw = tfidf_vectorizer_raw.transform(data_test['path'].apply(load_text))\n",
        "\n",
        "print(\"TF-IDF Shape (Raw BPE):\", tfidf_train_raw.shape, tfidf_test_raw.shape)\n",
        "\n",
        "# 2. BPE Tokenization With Normalization\n",
        "\n",
        "# Preprocess the text using the normalization function first\n",
        "data_train['normalized_text'] = data_train['path'].apply(preprocess_text)\n",
        "data_test['normalized_text'] = data_test['path'].apply(preprocess_text)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer for normalized text\n",
        "tfidf_vectorizer_normalized = TfidfVectorizer(\n",
        "    max_features=15000,\n",
        "    ngram_range=(1, 2),\n",
        "    tokenizer=bpe_tokenizer,\n",
        "    token_pattern=None  # Disable default token pattern\n",
        ")\n",
        "\n",
        "# Apply BPE tokenization and TF-IDF on normalized text\n",
        "tfidf_train_normalized = tfidf_vectorizer_normalized.fit_transform(data_train['normalized_text'])\n",
        "tfidf_test_normalized = tfidf_vectorizer_normalized.transform(data_test['normalized_text'])\n",
        "\n",
        "print(\"TF-IDF Shape (Normalized BPE):\", tfidf_train_normalized.shape, tfidf_test_normalized.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (Raw BPE): 0.801\n",
            "\n",
            "Classification Report (Raw BPE):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.80      0.80      0.80       500\n",
            "         pos       0.80      0.80      0.80       500\n",
            "\n",
            "    accuracy                           0.80      1000\n",
            "   macro avg       0.80      0.80      0.80      1000\n",
            "weighted avg       0.80      0.80      0.80      1000\n",
            "\n",
            "================================================================================\n",
            "Accuracy (Normalized BPE): 0.771\n",
            "\n",
            "Classification Report (Normalized BPE):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.76      0.80      0.78       500\n",
            "         pos       0.79      0.74      0.76       500\n",
            "\n",
            "    accuracy                           0.77      1000\n",
            "   macro avg       0.77      0.77      0.77      1000\n",
            "weighted avg       0.77      0.77      0.77      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Prediction with Raw BPE Tokenization (No Normalization)\n",
        "svm_raw = LinearSVC(dual=True)\n",
        "svm_raw.fit(tfidf_train_raw, data_train['label'])  # Train on raw BPE TF-IDF\n",
        "predictions_raw = svm_raw.predict(tfidf_test_raw)  # Predict on test data\n",
        "\n",
        "# Evaluate the classifier's performance on raw data\n",
        "accuracy_raw = accuracy_score(data_test['label'], predictions_raw)\n",
        "print(\"Accuracy (Raw BPE):\", accuracy_raw)\n",
        "print(\"\\nClassification Report (Raw BPE):\\n\", classification_report(data_test['label'], predictions_raw))\n",
        "\n",
        "# Separator for clarity in output\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 2. Prediction with Normalized BPE Tokenization\n",
        "svm_normalized = LinearSVC(dual=True)\n",
        "svm_normalized.fit(tfidf_train_normalized, data_train['label'])  # Train on normalized BPE TF-IDF\n",
        "predictions_normalized = svm_normalized.predict(tfidf_test_normalized)  # Predict on test data\n",
        "\n",
        "# Evaluate the classifier's performance on normalized data\n",
        "accuracy_normalized = accuracy_score(data_test['label'], predictions_normalized)\n",
        "print(\"Accuracy (Normalized BPE):\", accuracy_normalized)\n",
        "print(\"\\nClassification Report (Normalized BPE):\\n\", classification_report(data_test['label'], predictions_normalized))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
