{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinamilo6/Textmining/blob/main/A03_TextMining1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1D-_ww7fQTB",
        "outputId": "a74112dc-f3b8-4155-c823-9dcebb2bbf94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas sklearn nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS1_jLQofQTD"
      },
      "source": [
        "# Assignment 3 - Text Mining\n",
        "\n",
        "Project management and tools for health informatics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufi0zQidfQTE"
      },
      "source": [
        "## 1. Download and prepare data:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "The code in this section downloads the [IMDB IMDB Large Movie Review Dataset]('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz') which is the dataset you will be working on in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aOzKCiw4fQTE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sq9rCBBrfQTE"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('aclImdb'):\n",
        "    # download data:\n",
        "    urlretrieve('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', 'aclImdb.tar.gz')\n",
        "\n",
        "    # unzip data:\n",
        "    with tarfile.open('aclImdb.tar.gz') as file:\n",
        "        file.extractall('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9pzG__QfQTF"
      },
      "source": [
        "## 2. Some helper Functions:\n",
        "\n",
        "**Do not alter the code in this Section!**\n",
        "\n",
        "This section contains the code for some helper functions that will be useful for solving the assignment. Example code on how to use the functions is provided in section 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LaM3GoopfQTF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from typing import Literal, Tuple, Iterable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxsGe7MufQTF"
      },
      "source": [
        "Function for loading data into a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tKoAn698fQTG"
      },
      "outputs": [],
      "source": [
        "def load_data(split:Literal['train', 'test'], texts_per_class:int=500) -> pd.DataFrame:\n",
        "    ''' Loads the data into a pandas dataframe.'''\n",
        "    paths  = []\n",
        "    labels = []\n",
        "\n",
        "    for label in ('pos', 'neg'):\n",
        "        # get all files in the folder:\n",
        "        files = os.listdir(os.path.join('aclImdb', split, label))[:texts_per_class]\n",
        "\n",
        "        # append them to the lists:\n",
        "        paths.extend([os.path.join('aclImdb', split, label, f) for f in files])\n",
        "        labels.extend([label] * len(files))\n",
        "\n",
        "    return pd.DataFrame({'path':paths, 'label':labels})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6C1jyAtfQTG"
      },
      "source": [
        "Function for loading a specific text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y0QGuZMXfQTH"
      },
      "outputs": [],
      "source": [
        "def load_text(path:str) -> str:\n",
        "    ''' Reads a single text given the path. '''\n",
        "    # read file from disk:\n",
        "    with open(path, 'r', encoding='utf8') as file:\n",
        "        s = file.read()\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR52KxrvfQTH"
      },
      "source": [
        "Function for iterating through multiple texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NImrB2NzfQTH"
      },
      "outputs": [],
      "source": [
        "def iterate_texts(data:pd.DataFrame) -> Iterable[Tuple[str, str]]:\n",
        "    ''' Iterates through a pandas dataframe. '''\n",
        "\n",
        "    for path in data['path'].values:\n",
        "        # read file from disk:\n",
        "        with open(path, 'r', encoding='utf8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        yield text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzYgen2AfQTH"
      },
      "source": [
        "## 3. Your Code:\n",
        "\n",
        "**Alter the code below to complete the assignment!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3FnCEu6fQTH"
      },
      "source": [
        "Load the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "CeshtSl9fQTI",
        "outputId": "0c0bd430-94c5-4245-94e1-8ecfe74cc5d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               path label\n",
              "0    aclImdb/train/pos/11266_10.txt   pos\n",
              "1     aclImdb/train/pos/11392_8.txt   pos\n",
              "2    aclImdb/train/pos/10098_10.txt   pos\n",
              "3      aclImdb/train/pos/1243_9.txt   pos\n",
              "4     aclImdb/train/pos/3890_10.txt   pos\n",
              "..                              ...   ...\n",
              "995    aclImdb/train/neg/8531_3.txt   neg\n",
              "996   aclImdb/train/neg/10706_4.txt   neg\n",
              "997    aclImdb/train/neg/8834_4.txt   neg\n",
              "998    aclImdb/train/neg/3574_1.txt   neg\n",
              "999    aclImdb/train/neg/6924_1.txt   neg\n",
              "\n",
              "[1000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d43eff9-46b7-4435-baff-8a8ee2e3423d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aclImdb/train/pos/11266_10.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aclImdb/train/pos/11392_8.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aclImdb/train/pos/10098_10.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aclImdb/train/pos/1243_9.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aclImdb/train/pos/3890_10.txt</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>aclImdb/train/neg/8531_3.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>aclImdb/train/neg/10706_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>aclImdb/train/neg/8834_4.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>aclImdb/train/neg/3574_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>aclImdb/train/neg/6924_1.txt</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d43eff9-46b7-4435-baff-8a8ee2e3423d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d43eff9-46b7-4435-baff-8a8ee2e3423d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d43eff9-46b7-4435-baff-8a8ee2e3423d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4f74c715-90ae-4dbc-acbc-09a10d3d66bd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f74c715-90ae-4dbc-acbc-09a10d3d66bd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4f74c715-90ae-4dbc-acbc-09a10d3d66bd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_986cfea0-8c9b-4688-a39f-1bd60e09d857\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_986cfea0-8c9b-4688-a39f-1bd60e09d857 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_train",
              "summary": "{\n  \"name\": \"data_train\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"aclImdb/train/neg/9637_2.txt\",\n          \"aclImdb/train/neg/4064_2.txt\",\n          \"aclImdb/train/neg/10351_1.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"neg\",\n          \"pos\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data_train = load_data('train')\n",
        "data_test  = load_data('test')\n",
        "data_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qXdudbDfQTI"
      },
      "source": [
        "### Accessing the texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "g82ADF3HfQTI",
        "outputId": "426347c2-734e-4f7b-d47d-97db8c44fe82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recently watched this, but when it started I had no idea what the concept was about, what the topic was.....in short - I had no idea what it was. Was it a documentary, was it a comedy routine.....Well, it was BOTH.<br /><br />It started a little slow, but I think that\\'s because I had absolutely no idea what type of program I was viewing. But it quickly sucked me in. The episode I watched had Robert Wuhl discussing fact and fiction in history. Mainly how we (american\\'s) learn history that isn\\'t really true - and how we got to learn what we did. He did this in such a way as to keep the viewer completely entertained, and interested. I actually learned a few things and that is a true indicator of how effective this type of program can be.<br /><br />I would love to see this picked up as a series for HBO. I believe it can be just as fun and effective with a variety of topics - especially if they are \"taught\" in the same type of manner as this episode.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Sample code: load a single text\n",
        "load_text(data_train.loc[0, 'path'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGDmPlQbfQTI",
        "outputId": "66ec4c23-dc4d-4e61-e290-fd17a728e276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I recently watched this, but when it started I had no idea what the concept was about, what the topic was.....in short - I had no idea what it was. Was it a documentary, was it a comedy routine.....Well, it was BOTH.<br /><br />It started a little slow, but I think that's because I had absolutely no idea what type of program I was viewing. But it quickly sucked me in. The episode I watched had Robert Wuhl discussing fact and fiction in history. Mainly how we (american's) learn history that isn't really true - and how we got to learn what we did. He did this in such a way as to keep the viewer completely entertained, and interested. I actually learned a few things and that is a true indicator of how effective this type of program can be.<br /><br />I would love to see this picked up as a series for HBO. I believe it can be just as fun and effective with a variety of topics - especially if they are \"taught\" in the same type of manner as this episode.\n",
            "Paris is the place to be to enjoy beautiful art and music, and to fall madly in love - as is the case in this film. Boy meets girl, they fall in love, but something stands in their way of eternal happiness, the classic story.<br /><br />The wonderful music of George Gerschwin complements the great dancing by Gene Kelly and Leslie Caron. \"An American in Paris\" is a humorous, light-hearted, loving film well worth watching.<br /><br />8/10<br /><br />\n",
            "Dominion Tank Police is without a shell of a doubt, one of the most amazing shows ever produced, but not just in the field of animation. While the first part (Acts 1 and 2) mostly consists of action and fun, the second part is more serious and one should not treat the second part in the exact same way as first part. The subtleties are truly out of this world and the characterization is beyond brilliant. You must have an extra degree of intelligence to appreciate the intricacies of the second Part (Acts-3 and 4). I do have some complaints though. In the first part, the Tank Bonaparte quite literally jumps over a tank shell and it did not make any sense at all. One might also question the plausibility of Bonaparte jumping on the wing of Helicopter Gunship even though it was cool. Buaku rules.\n",
            "Every once in a while, Eddie Murphy will surprise you.<br /><br />In a movie like \"the Golden Child\", especially. This is a movie you'd figure would star maybe Harrison Ford or Kurt Russell or someone. But Eddie really does work; he's smart, he's funny, he's brave, kind, courteous, thrifty, clean and everything else a hero should be.<br /><br />Having been chosen to secure a mystic child who holds the key to protecting the world from complete evil (embodied perfectly by Dance), Eddie goes from California, to Nepal and back, all while the beautiful Kee Nang (Lewis) wonders if he's all he says he is and a crazy old holy man (Wong, perfect as always) knows that he is.<br /><br />It's exciting, breathtaking in spots, shocking and, of course, funny. Eddie is the only action hero I know who could begin a movie by making rude remarks behind some guy reading a porno magazine and end it with smart-aleck remarks about Ed McMahon.<br /><br />No problem with this \"Child\": it's a \"Golden\" find.<br /><br />Nine stars. Viva Nepal!\n",
            "This is an excellent stand-up DVD! Eddie Izzard is the funniest person I have seen in years. His routine is hilarious and makes for great conversation with others who have seen it. I HIGHLY recommend this one. The part about the history of Europe is a bit slow, but the ending jokes in French are quite good, because you don't have to speak French to get it (although if you do, it is still hilarious). Also, the parts about being a transvestite are quite good. The first scene (about San Francisco) is not great, but funny the first time. Skip over those if you can. It's almost not worth watching. However, this really is a funny, funny stand-up show that everyone should see. \"I was dead at the time!\"\n",
            "This is my all-time favorite Fred Astaire and Ginger Rogers film. The dialogue between the two is so cute and funny and very clever. Not to mention this film contains some of the best songs recorded by the two; like I'm Putting All My Eggs in One Basket and Let's Face the Music and Dance. If I remember correctly, this was the film that introduced me to Fred Astaire so I suppose because of that it will always hold a special place in my heart (sorry for the sentimental cr*p but I'm woman so get over it)All in all this film gets an 8/10 from me. The choreography was superb and also the fact that Lucille Ball is in it makes it even more awesome.\n",
            "Gosha's last great film of the 1960's. A resolute stylist with a great sense of purpose to his films, Gosha teamed up with Shintaro Katsu (of Zatoichi fame) to produce this scathing indictment of mindless nationalist loyalty. <br /><br />\"Tenchu\" (heavenly judgment) is the word that the loyalists to the emperor yell while assassinating enemies or \"traitors\" to the cause. Katsu plays up his character' simple minded allegiance to a manipulative politician all in the name of patriotic pride. Anybody who questions the politician is labeled a \"traitor\" and becomes an assassination target.<br /><br />One of the best photographed films ever, many shots are incredible compositions of form, color and light. The fight scenes are frequent and very bloody and brutal. The blood becomes a part of the color palette Gosha uses for his images. Gorgeous and disturbing. While the personal story is simple to follow, the historical background is complicated and while a basic history lesson for this time in Japan would be very helpful you can struggle through the film without it. The few drawbacks to the film are the music track, the length and Katsu's occasional scenery chewing. He has a drunken scene that's way over the top for a film but actually a very accurate depiction of a drunk.<br /><br />Downbeat but one of the great chambara films.\n",
            "Okay, truthfully, I saw the previews for this movie and thought to myself, what are the producers thinking? Hutton, Jolie, and DUCHOVNY? How could the monotoned actor possibly compete with Jolie's natural power on the screen? But surprisingly, the two had the kind of chemistry that showed intense caring without a kiss. Even David's humor matched up to Jolie's spark and fire. As for Hutton, he played the psycho very well, contrasting with David's calm delivery of life threatening situations. Overall, I was very impressed with the writing and character development. I gave it 8 stars.\n",
            "I must admit I'm a little surprised and disappointed at some of the very negative comments this film seems to provoke. I think its a great horror/sci fi film. Colonel Steve West (Alex Rebar) returns to Earth after an historical space flight to Saturn. While in space he contracted some bizarre and unknown disease. He wakes up in a hospital bed, he looks in a mirror and before his very eyes his face is melting! Escaping the hospitals supervision, he hides out in some local woods surrounding a small town. Unfortunately he starts to develop a rapidly growing hunger that can only be satisfied by eating other people. He must feed on human flesh and drink the blood of others to survive! Stalking human prey he begins his reign of terror! Its up to his old friend Dr Ted Nelson (Burr DeBenning) to find him and try and help him. He has to work alone as his boss General Perry (Myron Healey) wants it kept ultra quiet. Nelson can't even tell his wife Judy (Ann Sweeny). However, Sheriff Blake (Micheal Alldredge) becomes suspicious as General Perry turns up just as some of the local townspeople start turning up half eaten. I don't really understand why this film gets such negative reviews, what do people expect? Anyway, I really like this film. The star of the film are unquestionably Rick Bakers Special Make-up and gore effects which for the most part are excellent, and the fact their all prosthetic effects and no rubbish horrible CGI makes them even better. Writer and Director William Sachs isn't afraid to use them either, we get some nice long lingering close up shots of the incredible melting man and they hold up very well, even now. Photography, music and direction are a little bit dull, but professional enough. The script manages to create some sympathy for the the monster, shots of him looking longingly into Ted Nelsons house, or when he sees his own reflection in some water and reacts violently. The ending, set in a large factory of some sort, is pretty downbeat so don't expect any happy ending. Which surprised me. Also, the script doesn't really do anything with the premise, he just walks around melting and killing, with his friend trying to stop him, maybe a bit too simple. Personally I think the worst bit of the film is near the start when the fat nurse runs down a hospital corridor in slow motion, her screams are also portrayed in slow motion too, it looks and sounds totally ridiculous! You need to see it to believe it! I like this film a lot and recommend it to 70's and 80's horror/sci fi fans. A bit of a favourite of mine.\n",
            "The movie may be great. I just watched it last night, but feel unable to give an honest opinion of it because I read the book first. The book is so much better than the movie that I was disappointed with the film. If you plan to watch \"Of Human Bondage,\" don't read the book beforehand. On the other hand, the book is so good, and contains so much more than the love affair Phillip has with Mildred, you could still enjoy it after seeing the movie. I do not make this claim lightly. I average reading a book every 4 days, and read such disparate authors as Danielle Steel, Ovid, Faulkner, Plato, and Shakespeare. \"Of Human Bondage\" gets my vote as one of the top ten novels ever written.\n",
            "Well, \"Cube\" (1997), Vincenzo's first movie, was one of the most interesting and tricky ideas that I've ever seen when talking about movies. They had just one scenery, a bunch of actors and a plot. So, what made it so special were all the effective direction, great dialogs and a bizarre condition that characters had to deal like rats in a labyrinth. His second movie, \"Cypher\" (2002), was all about its story, but it wasn't so good as \"Cube\" but here are the characters being tested like rats again.<br /><br />\"Nothing\" is something very interesting and gets Vincenzo coming back to his 'Cube days', locking the characters once again in a very different space with no time once more playing with the characters like playing with rats in an experience room. But instead of a thriller sci-fi (even some of the promotional teasers and trailers erroneous seemed like that), \"Nothing\" is a loose and light comedy that for sure can be called a modern satire about our society and also about the intolerant world we're living. Once again Vicenzo amaze us with a great idea into a so small kind of thing. 2 actors and a blinding white scenario, that's all you got most part of time and you don't need more than that. While \"Cube\" is a claustrophobic experience and \"Cypher\" confusing, \"Nothing\" is completely the opposite but at the same time also desperate.<br /><br />This movie proves once again that a smart idea means much more than just a millionaire budget. Of course that the movie fails sometimes, but its prime idea means a lot and offsets any flaws. There's nothing more to be said about this movie because everything is a brilliant surprise and a totally different experience that I had in movies since \"Cube\".\n",
            "This is definitely one of the best kung fu movies ever, and may be one of the best movies ever... It's got a great plot that functions like a puzzle, with lots of intrigue and suspense. This film is full of cat and mouse games and deceptions, with people hiding their identities and their natures. The characters in this film live and breath much more than your average kung fu movie characters. They are all interesting and compelling and the movie does a good job at giving them scenes to show their personality's and desires.<br /><br />The fight scenes play out like little stories and many of them are very original and exciting. It has cool training sequences and martial arts skills that are so awesome they enter the realm of fantasy. There are 5 members of the poison clan each one with his own style that mimics the special skill of a venomous animal. The styles of each of these characters are fun to watch and you can see the techniques they use in training applied during the film... When this happens, The director uses quick cutting back to the training scene to draw a parallel. These cuts are accompanied by music changes and sound effects and the whole thing really works nicely.<br /><br />One thing about this movie that is very original is the way it treats death. The director Chang Cheh was obviously very concerned that the film not trivialize death. This makes some of the scenes in the movie much more effective. We actually care when people are killed in this film. This is because the camera lingers on the horror of death even when the bad guys are killed. Some of the sequences in this movie are truly gut wrenching. When characters go in search of vengeance you really feel their anger and pain.<br /><br />At the same time, this is also a fun movie. It has all the typical things you expect from a traditional kung fu film. There is bad dubbing, The characters are willing to fight at the drop of a hat. Some of the sound effects are hilarious and at times the behavior of the characters is incredibly unrealistic... all this just adds to the greatness of the film.<br /><br />And lets not forget that this director was a visual stylist much more gifted than most of his contemporaries. If you watch this movie closely you will notice that the technical prowess on display is virtuostic. Everything goes by so fast (because of the quick cutting style and the rapid camera movements of the genre) that it is easy to overlook how beautiful the movie really is. The lighting and composition are spectacular at times. The camera work and movement is extremely sophisticated along with very interesting fast paced editing... In the scenes that portray suspense and intrigue for example, imagine Hitchcock moving at about twice the speed. Chang Cheh was truly a master craftsman and artist who knew his genre and was able to produce important material while working within it's confines. He doesn't rattle the boat of the kung fu genre film, but in a subtle way his skills permeate every scene and every shot and they add greatly to the quality of the work. He is an important filmmaker who continues to influence many people.<br /><br />This is the real package A kung fu movie that delivers on every level. It's art, it's trash, it's emotionally moving, and it's fun, it has a true sense of morality, but doesn't allow that morality to get in the way of delivering good action. I recommend it to everybody whether you are a fan of this genre or not.\n",
            "Zane and Beringer will keep you on the edge of your seats. I don't typically go for military/war movies, but this was worth my time.<br /><br />It was serious, but it was also humorous. Beringer's character proved to be heroic and honest. No matter what, you know that he's got your back.<br /><br />Zane's character developed throughout the film. He wasn't just a suit, he definitely proved that he could be a hero and handle a gun.<br /><br />The ending through me a little though. It didn't really go with the action throughout the film, but I'm glad that I saw it, nonetheless.<br /><br />It's worth checking out.\n",
            "Police Story is a stunning series of set pieces for Jackie Chan to show his unique talents and bravery. Some of the stunts here are among Jackie's best and most dangerous the whole mall fight finale is probably Jackie's greatest single fight sequence, more brutal and less comedic then say Project A or Drunken Master at the end of the fight you can almost feel the pain of the impact.<br /><br />But unfortunately the rest of the film doesn't hold up to this quality as it is a rather formulaic cop thriller with some comedy elements. I always prefer JC in films such as Project A where his natural comic talents shine through. In the serious confines of some elements of Police Story it just doesn't work for me. Having said that though this is still up there with Jackies best films due to the incredible stunt work and sheer spectacle.<br /><br />As usual with Hong Kong films avoid the English dubbed DVD version as it is truly awful stick with the subtitles.<br /><br />Great stunts, OK movie a fine starting point if you've not seen a JC movie before and well worth a watch for any movie fan 7/10\n",
            "Francesca Annis, Michael Kitchen AND Robson Green!! Wow, what a trio...OK, so this is no Anna Karenina, but it is a good love story, very well-written and well-acted by all. Even a few 'laugh-out-loud' moments mixed in with some pretty serious observations on fidelity, age bias, and parental aging/Alzheimer's issues.<br /><br />Quirky guitar music added to the story as well.<br /><br />While I have been a fan of Ms. Annis' since 'Lillie' (in the '70s) and Mr. Kitchen's since 'The Buccaneers' and 'Enchanted April', I have only recently discovered Mr. Green ('Me and Mrs. Jones', 'Touching Evil', etc.), making me ask the question - why had I not seen 'Reckless' until recently??!! Admittedly more of a 'chick flick' than something a man will sit through, it is perfect for a rainy afternoon's lazy viewing.\n",
            "\"Ruby in Paradise\" is a beautiful, coming-of-age story about a young woman, Ruby Lee Gissing, escaping her stifling roots to become herself. Although the title character is played artfully by the gorgeous Ashley Judd -- in likely her first movie role, albeit one to be quite proud of -- the emphasis is not upon becoming \"somebody,\" a la the next Madonna (whether Jesus' mother or the lurid, attention-hungry singer).<br /><br />It instead emphasizes following ones' instincts and being somewhat introspective about them, to grow into one's ideal, adult self. NOTE: This isn't an action movie!!! It uses an occasional voice-over narration (by Ms. Judd) while writing in her journal -- and oh, I see I've just lost the male half of the readers out there. But be patient with this beautiful movie, where we learn that one's bliss can be discovered in -- oh, I dunno, carrying water and chopping wood.<br /><br />Actor/director/writer Todd Field, who played Nick Nightingale in \"Eyes Wide Shut,\" co-stars as Ruby Lee's noble love interest, one who helps her heal her idea of relationships implanted from youth.<br /><br />But not even his character is the answer for Ruby Lee: There's no external hero imposed upon her. The ultimate message is that we are responsible for ourselves. Writer/director Victor Nunez, who also wrote/directed \"Ulee's Gold,\" did an amazing job showing a young woman growing into herself -- confronting age-old challenges of good v. evil along the way.<br /><br />The supporting cast is also stellar, and the music used, particularly the cuts by chanteuse Sam Phillips (whom I hear is the wife of T. Bone Burnett), is right on -- most especially \"Trying to Hold on to the Earth.\" Now, when I hear the first few chords of that song, tears spring to my eyes, Pavlovian and unbidden -- not sure if it's the music, or the indelible connection to the movie's quiet, charming message of empowerment.<br /><br />This movie is highly recommended for any young person trying to find his/her way. For any woman of any age, it is a must see! The downside: It is NOT on DVD, except in Spanish. (We learned, however, that it is legal to make one copy of a VHS version, which can be readily found online. My beloved husband found someone with a VHS copy and got a DVD copy made for me.) Although this treasure of a movie occasionally pops up on-air  on an indie channel, usually  you can't count on that when you might need it most as a tonic to soothe the pressures of the world. So buy a copy for yourself.<br /><br />This movie should have a major re-release, and it would, if I were Queen of Hollywood.<br /><br />-- Figgy Jones\n",
            "I saw it at the Legacy Theater in the Joseph Smith Memorial Building in Salt Lake City this morning. I'm going to assume that one's level of enjoyment during this movie will largely be based on one's level of acceptance of Joseph's story.<br /><br />However, that aside it was very well made, well acted, and had a nice score. If you get to Salt Lake City, it is a must to see it in the Legacy Theater. I have never been in a nicer theater as far as picture quality, sound quality and ambiance in my entire life...I wonder if the Church would let me watch Batman Begins there! Being that I'm LDS and regard Joseph as a prophet, I was touched in several places and was brought to tears quite a few times...which I presume is expected since they handed out tissues BEFORE the movie started! Anyway, I'm told that this film is available in several LDS Visitor Centers around the globe, if you have 70 minutes check it out because whether you believe Joseph Smith or not, he tells a fascinating story.\n",
            "This is not a \"loose\", but a precise, faithful remake of 1958 Monicelli's classic \"I Soliti Ignoti\" with Toto', Mastroianni, Gassman, Cardinale etc. And that's the reason is good, it copies all the funny characters and the plot, even in details (like the scene where the photographer steals the camera from the local market).<br /><br />I have watched the superb old version many times and I knew by heart all the gangs and the ending but I still enjoyed \"Welcome to Collinwood\", which has its own freshness and atmosphere. It is interesting to see how the life and ways of the little thieves in 1950's Italy are adapted to 2002's USA. Things haven't changed much. 8/10.\n",
            "I have been a fan of Pushing Daisies since the very beginning. It is wonderfully thought up, and Bryan Fuller has the most remarkable ideas for this show.<br /><br />It is unbelievable on how much TV has been needing a creative, original show like Pushing Daisies. It is a huge relief to see a show, that is unlike the rest, where as, if you compared it to some of the newer shows, such as Scrubs and House, you would see the similarities, and it does get tedious at moments to see shows so close in identity.<br /><br />With a magnificent cast, wonderful script, and hilarity in every episode, Pushing Daisies is, by-far, one of the most remarkable shows on your television.\n",
            "If Saura hadn't done anything like this before, Iberia would be a milestone. Now it still deserves inclusion to honor a great director and a great cinematic conservator of Spanish culture, but he has done a lot like this before, and though we can applaud the riches he has given us, we have to pick and choose favorites and high points among similar films which include Blood Wedding (1981), Carmen (1983), El Amore Brujo (1986), Sevillanas (1992), Salomé (2002) and Tango (1998). I would choose Saura's 1995 Flamenco as his most unique and potent cultural document, next to which Iberia pales.<br /><br />Iberia is conceived as a series of interpretations of the music of Isaac Manuel Francisco Albéniz (1860-1909) and in particular his \"Iberia\" suite for piano. Isaac Albéniz was a great contributor to the externalization of Spanish musical culture -- its re-formatting for a non-Spanish audience. He moved to France in his early thirties and was influenced by French composers. His \"Iberia\" suite is an imaginative synthesis of Spanish folk music with the styles of Liszt, Dukas and d'Indy. He traveled around performing his compositions, which are a kind of beautiful standardization of Spanish rhythms and melodies, not as homogenized as Ravel's Bolero but moving in that direction. Naturally, the Spanish have repossessed Albéniz, and in Iberia, the performers reinterpret his compositions in terms of various more ethnic and regional dances and styles. But the source is a tamed and diluted form of Spanish musical and dance culture compared to the echt Spanishness of pure flamenco. Flamenco, coming out of the region of Andalusia, is a deeply felt amalgam of gitane, Hispano-Arabic, and Jewish cultures. Iberia simply is the peninsula comprising Spain, Portugal, Andorra and Gibraltar; the very concept is more diluted. <br /><br />Saura's Flamenco is an unstoppably intense ethnic mix of music, singing, dancing and that peacock manner of noble preening that is the essence of Spanish style, the way a man and a woman carries himself or herself with pride verging on arrogance and elegance and panache -- even bullfights and the moves of the torero are full of it -- in a series of electric sequences without introduction or conclusion; they just are. Saura always emphasized the staginess of his collaborations with choreographer Antonio Gades and other artists. In his 1995 Flamenco he dropped any pretense of a story and simply has singers, musicians, and dancers move on and off a big sound stage with nice lighting and screens, flats, and mirrors arranged by cinematographer Vittorio Storaro, another of the Spanish filmmaker's important collaborators. The beginnings and endings of sequences in Flamenco are often rough, but atmospheric, marked only by the rumble and rustle of shuffling feet and a mixture of voices. Sometimes the film keeps feeding when a performance is over and you see the dancer bend over, sigh, or laugh; or somebody just unexpectedly says something. In Flamenco more than any of Saura's other musical films it's the rapt, intense interaction of singers and dancers and rhythmically clapping participant observers shouting impulsive olé's that is the \"story\" and creates the magic. Because Saura has truly made magic, and perhaps best so when he dropped any sort of conventional story.<br /><br />Iberia is in a similar style to some of Saura's purest musical films: no narration, no dialogue, only brief titles to indicate the type of song or the region, beginning with a pianist playing Albeniz's music and gradually moving to a series of dance sequences and a little singing. In flamenco music, the fundamental element is the unaccompanied voice, and that voice is the most unmistakable and unique contribution to world music. It relates to other songs in other ethnicities, but nothing quite equals its raw raucous unique ugly-beautiful cry that defies you to do anything but listen to it with the closest attention. Then comes the clapping and the foot stomping, and then the dancing, combined with the other elements. There is only one flamenco song in Iberia. If you love Saura's Flamenco, you'll want to see Iberia, but you'll be a bit disappointed. The style is there; some of the great voices and dancing and music are there. But Iberia's source and conception doom it to a lesser degree of power and make it a less rich and intense cultural experience.\n"
          ]
        }
      ],
      "source": [
        "# Sample code: iterate through all texts\n",
        "for text in iterate_texts(data_train[:20]):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of sample texts to print\n",
        "n_samples = 7\n",
        "\n",
        "# Assume 'data_train' is a DataFrame or list containing your text data.\n",
        "# Instead of accessing 'text', access the 'path' column,\n",
        "# which likely contains the file paths to your text data.\n",
        "# Then, load the text content from those paths.\n",
        "X_train = [load_text(data_train.loc[i, 'path']) for i in range(n_samples)]\n",
        "\n",
        "# Print some sample texts from the training set\n",
        "for i in range(n_samples):\n",
        "    print(f\"Sample {i+1}:\\n\")\n",
        "    print(X_train[i])  # Print the text\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soE8Qj9hsF0E",
        "outputId": "5472688b-1226-47f0-8f9a-86cc9635972d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "\n",
            "I recently watched this, but when it started I had no idea what the concept was about, what the topic was.....in short - I had no idea what it was. Was it a documentary, was it a comedy routine.....Well, it was BOTH.<br /><br />It started a little slow, but I think that's because I had absolutely no idea what type of program I was viewing. But it quickly sucked me in. The episode I watched had Robert Wuhl discussing fact and fiction in history. Mainly how we (american's) learn history that isn't really true - and how we got to learn what we did. He did this in such a way as to keep the viewer completely entertained, and interested. I actually learned a few things and that is a true indicator of how effective this type of program can be.<br /><br />I would love to see this picked up as a series for HBO. I believe it can be just as fun and effective with a variety of topics - especially if they are \"taught\" in the same type of manner as this episode.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 2:\n",
            "\n",
            "Paris is the place to be to enjoy beautiful art and music, and to fall madly in love - as is the case in this film. Boy meets girl, they fall in love, but something stands in their way of eternal happiness, the classic story.<br /><br />The wonderful music of George Gerschwin complements the great dancing by Gene Kelly and Leslie Caron. \"An American in Paris\" is a humorous, light-hearted, loving film well worth watching.<br /><br />8/10<br /><br />\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 3:\n",
            "\n",
            "Dominion Tank Police is without a shell of a doubt, one of the most amazing shows ever produced, but not just in the field of animation. While the first part (Acts 1 and 2) mostly consists of action and fun, the second part is more serious and one should not treat the second part in the exact same way as first part. The subtleties are truly out of this world and the characterization is beyond brilliant. You must have an extra degree of intelligence to appreciate the intricacies of the second Part (Acts-3 and 4). I do have some complaints though. In the first part, the Tank Bonaparte quite literally jumps over a tank shell and it did not make any sense at all. One might also question the plausibility of Bonaparte jumping on the wing of Helicopter Gunship even though it was cool. Buaku rules.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 4:\n",
            "\n",
            "Every once in a while, Eddie Murphy will surprise you.<br /><br />In a movie like \"the Golden Child\", especially. This is a movie you'd figure would star maybe Harrison Ford or Kurt Russell or someone. But Eddie really does work; he's smart, he's funny, he's brave, kind, courteous, thrifty, clean and everything else a hero should be.<br /><br />Having been chosen to secure a mystic child who holds the key to protecting the world from complete evil (embodied perfectly by Dance), Eddie goes from California, to Nepal and back, all while the beautiful Kee Nang (Lewis) wonders if he's all he says he is and a crazy old holy man (Wong, perfect as always) knows that he is.<br /><br />It's exciting, breathtaking in spots, shocking and, of course, funny. Eddie is the only action hero I know who could begin a movie by making rude remarks behind some guy reading a porno magazine and end it with smart-aleck remarks about Ed McMahon.<br /><br />No problem with this \"Child\": it's a \"Golden\" find.<br /><br />Nine stars. Viva Nepal!\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 5:\n",
            "\n",
            "This is an excellent stand-up DVD! Eddie Izzard is the funniest person I have seen in years. His routine is hilarious and makes for great conversation with others who have seen it. I HIGHLY recommend this one. The part about the history of Europe is a bit slow, but the ending jokes in French are quite good, because you don't have to speak French to get it (although if you do, it is still hilarious). Also, the parts about being a transvestite are quite good. The first scene (about San Francisco) is not great, but funny the first time. Skip over those if you can. It's almost not worth watching. However, this really is a funny, funny stand-up show that everyone should see. \"I was dead at the time!\"\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 6:\n",
            "\n",
            "This is my all-time favorite Fred Astaire and Ginger Rogers film. The dialogue between the two is so cute and funny and very clever. Not to mention this film contains some of the best songs recorded by the two; like I'm Putting All My Eggs in One Basket and Let's Face the Music and Dance. If I remember correctly, this was the film that introduced me to Fred Astaire so I suppose because of that it will always hold a special place in my heart (sorry for the sentimental cr*p but I'm woman so get over it)All in all this film gets an 8/10 from me. The choreography was superb and also the fact that Lucille Ball is in it makes it even more awesome.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Sample 7:\n",
            "\n",
            "Gosha's last great film of the 1960's. A resolute stylist with a great sense of purpose to his films, Gosha teamed up with Shintaro Katsu (of Zatoichi fame) to produce this scathing indictment of mindless nationalist loyalty. <br /><br />\"Tenchu\" (heavenly judgment) is the word that the loyalists to the emperor yell while assassinating enemies or \"traitors\" to the cause. Katsu plays up his character' simple minded allegiance to a manipulative politician all in the name of patriotic pride. Anybody who questions the politician is labeled a \"traitor\" and becomes an assassination target.<br /><br />One of the best photographed films ever, many shots are incredible compositions of form, color and light. The fight scenes are frequent and very bloody and brutal. The blood becomes a part of the color palette Gosha uses for his images. Gorgeous and disturbing. While the personal story is simple to follow, the historical background is complicated and while a basic history lesson for this time in Japan would be very helpful you can struggle through the film without it. The few drawbacks to the film are the music track, the length and Katsu's occasional scenery chewing. He has a drunken scene that's way over the top for a film but actually a very accurate depiction of a drunk.<br /><br />Downbeat but one of the great chambara films.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIMITATIONS/ISSUES CAPTURED FROM THE ABOVE TEXT SAMPLES:\n",
        "\n",
        "HTML tags and special characters\n",
        "\n",
        "Punctuation and symbols (&)\n",
        "\n",
        "Contractions (\"isn't\", \"I'll\", \"I'm\")\n",
        "\n",
        "Parentheses and annotations (\"(Crouching Tiger) \")\n",
        "\n",
        "Informal formatting (\"my rating is ****\")\n",
        "\n",
        "Ambiguity and polysemy (\"dictators\")\n",
        "\n",
        "Long and complexes sentences\n",
        "\n",
        "Informal language (\"what can be so bad about that?\")\n",
        "\n",
        "Quotation marks (\"dictators\", \"sin\")\n",
        "\n",
        "\n",
        "\n",
        "We will elaborate about them and add our conclusions in the report."
      ],
      "metadata": {
        "id": "yWw3AZ8SumVn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHLiIMDifQTI"
      },
      "source": [
        "### A simple pipeline:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOYzJ1-SfQTI"
      },
      "source": [
        "**White-Space tokenization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GJjcIUkHfQTI"
      },
      "outputs": [],
      "source": [
        "def tokenize(text:str):\n",
        "    ''' An example tokenization function. '''\n",
        "\n",
        "    # simple white-space tokenization:\n",
        "    return text.lower().split()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Example sentence to encode\n",
        "s = \"Hello, this is a test sentence for the tokenizer.\"\n",
        "\n",
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Add a pre-tokenizer to handle whitespace properly\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize a BPE trainer with some default parameters\n",
        "trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"])\n",
        "\n",
        "# Instead of reading files, let's provide training data directly as strings\n",
        "# Simulate a dataset by giving it a list of strings (or paths to actual files)\n",
        "training_data = [\"Hello world\", \"This is a test\", \"We are training the tokenizer\"]\n",
        "\n",
        "# The trainer expects file paths, but we can create files dynamically (or mock them)\n",
        "# For now, let's assume you have access to these files or use in-memory data.\n",
        "tokenizer.train_from_iterator(training_data, trainer)\n",
        "\n",
        "# Encode the string\n",
        "t = tokenizer.encode(s)\n",
        "\n",
        "# Decode the tokenized output\n",
        "decoded_s = tokenizer.decode(t.ids)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original string:\", s)\n",
        "print(\"Encoded tokens:\", t.tokens)\n",
        "print(\"Decoded string:\", decoded_s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2LtJwsVeG6",
        "outputId": "8e02ba69-5c71-416f-c9cb-c7d14cd7a8ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original string: Hello, this is a test sentence for the tokenizer.\n",
            "Encoded tokens: ['Hello', 't', 'h', 'is', 'is', 'a', 'test', 's', 'en', 't', 'en', 'e', 'or', 'the', 'tokenizer']\n",
            "Decoded string: Hello t h is is a test s en t en e or the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GddqcHfvfQTJ"
      },
      "source": [
        "**Bag-of-words Embedding:**\n",
        "\n",
        "See documentation of [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGnHx6ThfQTJ",
        "outputId": "571df6ee-d4a4-48ca-c916-ec41e46a7aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create a simple bag of words embedding:\n",
        "bow = CountVectorizer(\n",
        "\n",
        "    # the next line converts the filepaths to the actual texts:\n",
        "    preprocessor = load_text,\n",
        "\n",
        "    # tokenization function from above:\n",
        "    tokenizer = tokenize\n",
        "\n",
        ")\n",
        "\n",
        "# train the embedding:\n",
        "embeddings_train = bow.fit_transform(data_train['path'].values)\n",
        "\n",
        "# vectorize test data:\n",
        "embeddings_test = bow.transform(data_test['path'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qbhJp3tfQTJ"
      },
      "source": [
        "**Classification with a linear SVM**\n",
        "\n",
        "See documentation of [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUb6hTYVfQTJ",
        "outputId": "5fd476a2-8e83-4f53-b553-feb9ebee6c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.766\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = LinearSVC()\n",
        "\n",
        "# train classifier:\n",
        "svm.fit(embeddings_train, data_train['label'].values)\n",
        "\n",
        "# test classifier:\n",
        "predictions = svm.predict(embeddings_test)\n",
        "\n",
        "# Calculate Accuracy:\n",
        "print('Accuracy:', accuracy_score(data_test['label'].values, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZATION TASK**"
      ],
      "metadata": {
        "id": "XLPUZJFu6gVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement of the chosen tokenizer: BPE\n",
        "Tokenize a few sample texts\n",
        "Accuracy score when using BPE tokenization"
      ],
      "metadata": {
        "id": "V6BYL7Lc58N1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataset 'data_train' and 'data_test' contain paths to text files in the 'path' column\n",
        "# and the labels in the 'label' column.\n",
        "\n",
        "# Step 1: Define the function to load the text from file paths\n",
        "def load_text(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "# Step 2: Define the BPE tokenizer and train it on your dataset\n",
        "def train_bpe_tokenizer(train_texts):\n",
        "    tokenizer = Tokenizer(BPE())\n",
        "    trainer = BpeTrainer()\n",
        "    tokenizer.train_from_iterator(train_texts, trainer=trainer)\n",
        "    return tokenizer\n",
        "\n",
        "# Step 3: Tokenize the input text using the trained BPE tokenizer\n",
        "def tokenize_bpe(tokenizer, text):\n",
        "    return tokenizer.encode(text).tokens\n",
        "\n",
        "# Step 4: Create a function to convert texts into embeddings using BPE tokenization\n",
        "def bpe_vectorize(tokenizer, texts, vectorizer=None): # Add vectorizer argument\n",
        "    # First, tokenize each text using the BPE tokenizer\n",
        "    tokenized_texts = [tokenize_bpe(tokenizer, text) for text in texts]\n",
        "\n",
        "    # Use CountVectorizer to convert tokens to vectors\n",
        "    # If vectorizer is provided, use it; otherwise, create a new one\n",
        "    if vectorizer is None:\n",
        "        vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=None, token_pattern=None, lowercase=False)\n",
        "        embeddings = vectorizer.fit_transform(tokenized_texts)\n",
        "    else:\n",
        "        embeddings = vectorizer.transform(tokenized_texts)  # Use existing vectorizer for transform\n",
        "    return embeddings, vectorizer # Return the vectorizer as well\n",
        "\n",
        "# Step 5: Load the actual text content from file paths for both training and test sets\n",
        "train_texts = [load_text(filepath) for filepath in data_train['path']]\n",
        "test_texts = [load_text(filepath) for filepath in data_test['path']]\n",
        "\n",
        "# Step 6: Train the BPE tokenizer on the actual training texts\n",
        "bpe_tokenizer = train_bpe_tokenizer(train_texts)\n",
        "\n",
        "# Print a few tokenized sample texts from the dataset\n",
        "print(\"Sample Tokenized Texts using BPE Tokenizer:\")\n",
        "for text in train_texts[:5]:  # Display first 5 tokenized texts as an example\n",
        "    print(tokenize_bpe(bpe_tokenizer, text))\n",
        "\n",
        "# Step 7: Create embeddings for both training and test sets using BPE tokenizer\n",
        "embeddings_train_bpe, vectorizer = bpe_vectorize(bpe_tokenizer, train_texts) # Get vectorizer from training\n",
        "embeddings_test_bpe, _ = bpe_vectorize(bpe_tokenizer, test_texts, vectorizer=vectorizer) # Reuse vectorizer for test\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "# Step 8: Train and test a classifier (SVM) on BPE tokenized data\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# Train the classifier on the training embeddings\n",
        "svm.fit(embeddings_train_bpe, data_train['label'].values)\n",
        "\n",
        "# Test the classifier on the test embeddings\n",
        "predictions = svm.predict(embeddings_test_bpe)\n",
        "\n",
        "# Calculate the accuracy on the test set\n",
        "accuracy_bpe = accuracy_score(data_test['label'].values, predictions)\n",
        "print(f'Accuracy using BPE Tokenization: {accuracy_bpe:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3wGTuLBzAeu",
        "outputId": "27b7d6c6-e97b-470c-90a2-fd353ad5b9cf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Tokenized Texts using BPE Tokenizer:\n",
            "['I recently ', 'watched ', 'this', ', but when it ', 'started ', 'I had ', 'no idea ', 'what the ', 'concept ', 'was ', 'about, ', 'what the ', 'topic ', 'was', '.....', 'in ', 'short ', '- ', 'I had ', 'no idea ', 'what it ', 'was. W', 'as ', 'it a ', 'documentar', 'y, was ', 'it a ', 'comedy ', 'rout', 'ine', '.....', 'Well', ', it was ', 'BO', 'TH', '.<br /><br />It ', 'started ', 'a little ', 'slow', ', but ', 'I think', \" that's \", 'because I had ', 'absolutely ', 'no idea what ', 'type of ', 'program ', 'I was ', 'viewing', '. But ', 'it ', 'quickly ', 'sucked ', 'me ', 'in. The ', 'episode ', 'I watched ', 'had ', 'Robert ', 'Wu', 'h', 'l ', 'discussing ', 'fact ', 'and ', 'fiction ', 'in ', 'history', '. M', 'ainly ', 'how ', 'we (', 'american', \"'s) \", 'learn ', 'history ', 'that ', \"isn't really \", 'true ', '- and ', 'how ', 'we ', 'got to ', 'learn ', 'what we ', 'did', '. He ', 'did this ', 'in such a ', 'way as to ', 'keep the ', 'viewer ', 'completely ', 'entertain', 'ed, and ', 'interest', 'ed. I ', 'actually ', 'learned ', 'a few', ' thing', 's and ', 'that is a ', 'true ', 'indic', 'ator ', 'of how ', 'effective ', 'this type of ', 'program ', 'can be', '.<br /><br />I would ', 'love ', 'to see this ', 'picked up ', 'as ', 'a seri', 'es for ', 'HBO', '. I believe ', 'it can be ', 'just as ', 'fun and ', 'effective ', 'with a ', 'variety of ', 'topic', 's - ', 'especially ', 'if they are ', '\"', 'taugh', 't\" ', 'in the same ', 'type of ', 'manner ', 'as this ', 'episode', '.']\n",
            "['Paris ', 'is the ', 'place ', 'to be ', 'to enjoy ', 'beautiful ', 'art and ', 'music', ', and to ', 'fall ', 'mad', 'ly in love ', '- ', 'as ', 'is the case ', 'in this film', '. B', 'oy ', 'meets ', 'girl', ', they ', 'fall ', 'in lov', 'e, but ', 'something ', 'stands ', 'in their ', 'way of ', 'etern', 'al ', 'happiness', ', the ', 'classic ', 'story', '.<br /><br />The ', 'wonderful ', 'music of ', 'George ', 'G', 'ers', 'ch', 'win ', 'complement', 's the ', 'great ', 'dancing by ', 'Gene Kelly ', 'and ', 'Leslie ', 'Car', 'on. \"', 'An American in Par', 'is', '\" is a ', 'humor', 'ous, ', 'light', '-heart', 'ed, ', 'loving ', 'film well worth ', 'watch', 'ing.<br /><br />', '8/10', '<br /><br />']\n",
            "['Domin', 'ion ', 'Tank ', 'Police ', 'is ', 'without a ', 'shell ', 'of a ', 'doub', 't, ', 'one of the most ', 'amazing ', 'shows ', 'ever ', 'produc', 'ed, but ', 'not just ', 'in the ', 'field ', 'of animation', '. While ', 'the first ', 'part ', '(Act', 's ', '1 and ', '2) ', 'mostly ', 'consists of ', 'action and ', 'fun, the ', 'second ', 'part ', 'is more ', 'serious and ', 'one ', 'should ', 'not ', 'tre', 'at the ', 'second ', 'part in the ', 'exact same ', 'way as ', 'first part', '. The ', 'subt', 'let', 'ies ', 'are truly ', 'out of this ', 'world ', 'and the ', 'characteriz', 'ation is ', 'beyond ', 'brilliant', '. You ', 'must have ', 'an extra ', 'degree of ', 'intelligence ', 'to ', 'appreciate the ', 'intric', 'acies of the ', 'second ', 'Part ', '(Act', 's-', '3 and ', '4', '). I ', 'do ', 'have some ', 'complaint', 's ', 'though', '. In the ', 'first part', ', the ', 'Tank ', 'Bon', 'apart', 'e ', 'quite ', 'literally ', 'jumps ', 'over a t', 'ank ', 'shell ', 'and it ', 'did not ', 'make any ', 'sense at all', '. One ', 'might ', 'also ', 'question', ' the ', 'plaus', 'ibility ', 'of ', 'Bon', 'apart', 'e ', 'jump', 'ing on the ', 'w', 'ing of ', 'Hel', 'icop', 'ter ', 'Gun', 'ship ', 'even though ', 'it was ', 'cool', '. Bu', 'ak', 'u ', 'rul', 'es.']\n",
            "['Every ', 'once in a ', 'while, ', 'Eddie Murphy ', 'will ', 'surprise ', 'you', '.<br /><br />In ', 'a movie ', 'like \"the ', 'Golden ', 'Ch', 'ild', '\", ', 'especially', '. This is a ', 'movie ', \"you'd \", 'figure ', 'would ', 'star ', 'maybe ', 'Harr', 'ison ', 'Ford ', 'or ', 'Kurt ', 'Russell ', 'or ', 'someon', 'e. But ', 'Eddie ', 'really does ', 'work', \"; he's \", 'smart, ', \"he's \", 'funny, ', \"he's \", 'brav', 'e, ', 'kind, ', 'cour', 'teous', ', th', 'rif', 'ty, ', 'cle', 'an and ', 'everything else ', 'a ', 'hero ', 'should be', '.<br /><br />', 'Having ', 'been ', 'chosen to ', 'secure ', 'a ', 'myst', 'ic ', 'child ', 'who ', 'hold', 's the ', 'key ', 'to ', 'protect', 'ing the world ', 'from ', 'complete ', 'evil ', '(', 'embo', 'died ', 'perfectly ', 'by ', 'Danc', 'e), ', 'Eddie ', 'goes ', 'from ', 'Californi', 'a, ', 'to ', 'Nep', 'al and ', 'back', ', all ', 'while the ', 'beautiful ', 'Ke', 'e ', 'N', 'ang ', '(', 'Lew', 'is) ', 'wonders ', 'if ', \"he's \", 'all ', 'he says ', 'he is ', 'and a ', 'crazy ', 'old ', 'holy ', 'man (', 'Wong', ', ', 'perfect ', 'as alway', 's) ', 'know', 's that he ', 'is', \".<br /><br />It's \", 'excit', 'ing, ', 'breathtaking ', 'in ', 'spots, ', 'shocking ', 'and, of course, ', 'funny', '. Ed', 'die ', 'is the only ', 'action ', 'hero ', 'I know ', 'who could ', 'beg', 'in a movie ', 'by making ', 'rude ', 'remarks ', 'behind ', 'some guy ', 'reading a ', 'porno ', 'magaz', 'ine and ', 'end ', 'it with ', 'smart-', 'al', 'eck ', 'remarks ', 'about ', 'Ed ', 'Mc', 'Ma', 'hon', '.<br /><br />No ', 'problem ', 'with this ', '\"Ch', 'ild', '\": ', \"it's a \", '\"', 'Gold', 'en\" ', 'find', '.<br /><br />', 'Nine ', 'stars. ', 'Viva ', 'Nep', 'al', '!']\n",
            "['This is an ', 'excellent ', 'stand-up ', 'DVD', '! ', 'Eddie Izzard ', 'is the funniest ', 'person ', 'I have seen ', 'in years', '. His ', 'routine ', 'is ', 'hilarious and ', 'makes for great ', 'conversation ', 'with ', 'others ', 'who have ', 'seen it. I ', 'HI', 'GH', 'LY ', 'recommend this ', 'one. The ', 'part ', 'about the ', 'history ', 'of ', 'Europe ', 'is a bit ', 'slow', ', but the ', 'ending ', 'jokes ', 'in French ', 'are quite ', 'good, ', 'because ', \"you don't \", 'have to ', 'speak ', 'French ', 'to get ', 'it (', 'although ', 'if you ', 'do', ', it is ', 'still ', 'hilarious', ')', '. Also', ', the ', 'parts ', 'about ', 'being a ', 'trans', 'vest', 'ite ', 'are quite ', 'good', '. The first ', 'scene ', '(', 'about ', 'San Francis', 'co', ') ', 'is not ', 'great, but ', 'funny ', 'the first ', 'time. S', 'kip ', 'over', ' those ', 'if you ', 'can', \". It's almost \", 'not ', 'worth watching', '. However', ', this ', 'really is a ', 'funny, ', 'funny ', 'stand-up ', 'show that ', 'everyone ', 'should ', 'see. ', '\"I was ', 'dead ', 'at the time', '!\"']\n",
            "Accuracy using BPE Tokenization: 0.7270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noted accuracy is lower with BPE tokenization compared to white space tokenization (0.7270 vs 0.766)\n",
        "\n",
        "Given above samples of tokenized texts, further text normalization like stemming or custom regular expressions is needed.\n",
        "\n",
        "\n",
        "First normalization applied without implementing contraction expansion."
      ],
      "metadata": {
        "id": "AjiyLitL6qYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Assuming your dataset 'data_train' and 'data_test' contain paths to text files in the 'path' column\n",
        "# and the labels in the 'label' column.\n",
        "\n",
        "# Step 1: Define the function to load and normalize the text from file paths\n",
        "def load_and_normalize_text(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    # Normalize the text\n",
        "    text = normalize_text(text)\n",
        "    return text\n",
        "\n",
        "# Step 2: Define the normalization function\n",
        "def normalize_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Optional: Expand contractions (e.g., \"don't\" to \"do not\")\n",
        "    # You can implement contraction expansion here if needed\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    # Reconstruct the text from stemmed tokens\n",
        "    normalized_text = ' '.join(stemmed_tokens)\n",
        "    return normalized_text\n",
        "\n",
        "# Step 3: Define the BPE tokenizer and train it on your dataset\n",
        "def train_bpe_tokenizer(train_texts):\n",
        "    tokenizer = Tokenizer(BPE())\n",
        "    trainer = BpeTrainer()\n",
        "    tokenizer.train_from_iterator(train_texts, trainer=trainer)\n",
        "    return tokenizer\n",
        "\n",
        "# Step 4: Tokenize the input text using the trained BPE tokenizer\n",
        "def tokenize_bpe(tokenizer, text):\n",
        "    return tokenizer.encode(text).tokens\n",
        "\n",
        "# Step 5: Create a function to convert texts into embeddings using BPE tokenization\n",
        "def bpe_vectorize(tokenizer, texts):\n",
        "    # First, tokenize each text using the BPE tokenizer\n",
        "    tokenized_texts = [tokenize_bpe(tokenizer, text) for text in texts]\n",
        "\n",
        "    # Use CountVectorizer to convert tokens to vectors\n",
        "    vectorizer = CountVectorizer(\n",
        "        tokenizer=lambda x: x,\n",
        "        preprocessor=None,\n",
        "        token_pattern=None,\n",
        "        lowercase=False\n",
        "    )\n",
        "    embeddings = vectorizer.fit_transform(tokenized_texts)\n",
        "    return embeddings, vectorizer\n",
        "\n",
        "# Step 6: Load and normalize the actual text content from file paths for both training and test sets\n",
        "train_texts = [load_and_normalize_text(filepath) for filepath in data_train['path']]\n",
        "test_texts = [load_and_normalize_text(filepath) for filepath in data_test['path']]\n",
        "\n",
        "# Step 7: Train the BPE tokenizer on the normalized training texts\n",
        "bpe_tokenizer = train_bpe_tokenizer(train_texts)\n",
        "\n",
        "# Print a few tokenized sample texts from the dataset\n",
        "print(\"Sample Tokenized Texts using BPE Tokenizer with Normalization:\")\n",
        "for text in train_texts[:5]:  # Display first 5 tokenized texts as an example\n",
        "    print(tokenize_bpe(bpe_tokenizer, text))\n",
        "\n",
        "# Step 8: Create embeddings for both training and test sets using BPE tokenizer\n",
        "embeddings_train_bpe_norm, vectorizer = bpe_vectorize(bpe_tokenizer, train_texts)\n",
        "embeddings_test_bpe_norm = vectorizer.transform(\n",
        "    [tokenize_bpe(bpe_tokenizer, text) for text in test_texts]\n",
        ")\n",
        "\n",
        "# Step 9: Train and test a classifier (SVM) on BPE tokenized data with normalization\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# Train the classifier on the training embeddings\n",
        "svm.fit(embeddings_train_bpe_norm, data_train['label'].values)\n",
        "\n",
        "# Test the classifier on the test embeddings\n",
        "predictions = svm.predict(embeddings_test_bpe_norm)\n",
        "\n",
        "# Calculate the accuracy on the test set\n",
        "accuracy_bpe_norm = accuracy_score(data_test['label'].values, predictions)\n",
        "print(f'Accuracy using BPE Tokenization with Normalization: {accuracy_bpe_norm:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClseHaEO1MAm",
        "outputId": "3285c859-75f1-4534-f680-9486038783b2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Tokenized Texts using BPE Tokenizer with Normalization:\n",
            "['i recent ', 'watch thi ', 'but when it ', 'start ', 'i had no idea ', 'what the ', 'concept ', 'wa about ', 'what the ', 'topic ', 'was', 'in short ', 'i had no idea what ', 'it wa wa ', 'it a ', 'documentari ', 'wa ', 'it a comedi ', 'rout', 'ine', 'wel ', 'it wa ', 'both', 'br br it ', 'start a littl slow ', 'but i ', 'think that ', 'becaus ', 'i had ', 'absolut ', 'no idea what ', 'type of ', 'program ', 'i wa ', 'view ', 'but it ', 'quickli ', 'suck ', 'me ', 'in the ', 'episod ', 'i watch ', 'had ', 'robert ', 'wu', 'hl ', 'discuss ', 'fact ', 'and ', 'fiction ', 'in histori ', 'mainli ', 'how ', 'we ', 'american ', 'learn ', 'histori ', 'that ', 'isnt realli ', 'true ', 'and how ', 'we got ', 'to learn ', 'what we ', 'did ', 'he did ', 'thi ', 'in such a ', 'way as ', 'to keep the viewer ', 'complet ', 'entertain and ', 'interest ', 'i actual ', 'learn ', 'a few ', 'thing ', 'and that ', 'is a true ', 'indic of how ', 'effect ', 'thi type of ', 'program ', 'can ', 'be', 'br br i ', 'would love ', 'to see thi ', 'pick up ', 'as a ', 'seri for ', 'hbo ', 'i believ ', 'it can be ', 'just as ', 'fun and ', 'effect ', 'with a ', 'varieti of ', 'topic ', 'especi ', 'if they are ', 'taught ', 'in the same ', 'type of ', 'manner ', 'as thi ', 'ep', 'is', 'od']\n",
            "['pari ', 'is the ', 'place ', 'to be ', 'to enjoy ', 'beauti ', 'art ', 'and music ', 'and to ', 'fall madli in love ', 'as is the case ', 'in thi film ', 'boy meet ', 'girl they ', 'fall in love ', 'but ', 'someth ', 'stand ', 'in their ', 'way of ', 'etern ', 'happi ', 'the classic ', 'story', 'br br the ', 'wonder music ', 'of georg ', 'ger', 'sch', 'win ', 'complement ', 'the great ', 'danc by ', 'gene kelli ', 'and ', 'lesli ', 'car', 'on an ', 'american in pari ', 'is a ', 'humor ', 'lightheart ', 'love ', 'film ', 'well worth ', 'watch', 'ingbr br ', '810', 'br br']\n",
            "['dom', 'ini', 'on ', 'tank ', 'polic ', 'is ', 'without a ', 'shell ', 'of a doubt ', 'one of the most ', 'amaz ', 'show ever ', 'produc ', 'but not ', 'just ', 'in the ', 'field ', 'of anim ', 'while ', 'the first part ', 'act ', '1 and ', '2 ', 'mostli ', 'consist of ', 'action and ', 'fun ', 'the second part ', 'is more ', 'seriou ', 'and one ', 'should not ', 'treat the ', 'second part ', 'in the ', 'exact same ', 'way as ', 'first ', 'part ', 'the subtleti ', 'are truli ', 'out of thi world ', 'and the ', 'character ', 'is beyond ', 'brilliant ', 'you ', 'must have ', 'an extra ', 'degre ', 'of intellig ', 'to appreci ', 'the intricaci ', 'of the ', 'second part ', 'ac', 'ts', '3 and ', '4 ', 'i do ', 'have some ', 'complaint ', 'though ', 'in the first part ', 'the ', 'tank ', 'bonapart ', 'quit liter ', 'jump ', 'over a ', 'tank ', 'shell ', 'and it ', 'did not ', 'make ani sens at all ', 'one might ', 'also ', 'question the ', 'plausibl ', 'of ', 'bonapart ', 'jump ', 'on the ', 'wing of ', 'helicopt ', 'gun', 'ship ', 'even though ', 'it wa cool ', 'bu', 'aku ', 'ru', 'le']\n",
            "['everi onc in a while ', 'eddi murphi ', 'will ', 'surpris ', 'youbr br ', 'in a movi ', 'like the ', 'golden ', 'child ', 'especi ', 'thi is a ', 'movi ', 'youd ', 'figur ', 'would ', 'star ', 'mayb ', 'harrison ', 'ford ', 'or ', 'kurt russel ', 'or someon ', 'but ', 'eddi ', 'realli doe ', 'work he ', 'smart ', 'he ', 'funni ', 'he ', 'brave ', 'kind ', 'cour', 'te', 'ou ', 'thri', 'fti ', 'clean and ', 'everyth els ', 'a ', 'hero ', 'should ', 'bebr br ', 'have been ', 'chosen to ', 'secur ', 'a ', 'mystic ', 'child who ', 'hold the ', 'key ', 'to protect ', 'the world ', 'from ', 'complet ', 'evil ', 'embodi ', 'perfectli ', 'by danc ', 'eddi ', 'goe ', 'from ', 'california ', 'to ', 'ne', 'pal and ', 'back ', 'all ', 'while the ', 'beauti ', 'kee ', 'nang ', 'lewi ', 'wonder ', 'if he ', 'all ', 'he say ', 'he is ', 'and a ', 'crazi ', 'old ', 'holi ', 'man ', 'wong ', 'perfect ', 'as alway ', 'know ', 'that he ', 'isbr br ', 'it ', 'excit ', 'breathtak ', 'in spot ', 'shock ', 'and of cours ', 'funni ', 'eddi ', 'is the onli ', 'action ', 'hero ', 'i know ', 'who could ', 'beg', 'in a movi ', 'by make ', 'rude ', 'remark ', 'behind ', 'some guy ', 'read a ', 'porno ', 'magazin and ', 'end ', 'it with ', 'smar', 'tal', 'eck ', 'remark ', 'about ', 'ed ', 'mcma', 'hon', 'br br no ', 'problem with thi ', 'child ', 'it a ', 'golden ', 'fin', 'dbr br ', 'nine ', 'star ', 'viva ', 'ne', 'pal']\n",
            "['thi is an excel ', 'standup ', 'dvd ', 'eddi izzard ', 'is ', 'the funniest ', 'person ', 'i have seen ', 'in year ', 'hi ', 'routin ', 'is hilari ', 'and make ', 'for great ', 'convers ', 'with ', 'other ', 'who have seen ', 'it ', 'i highli recommend ', 'thi one ', 'the part ', 'about the ', 'histori ', 'of europ ', 'is a bit ', 'slow ', 'but the end ', 'joke ', 'in french ', 'are quit ', 'good becaus ', 'you dont ', 'have to ', 'speak french ', 'to get ', 'it ', 'although ', 'if you do ', 'it is still ', 'hilari ', 'also the ', 'part ', 'about ', 'be a ', 'transvestit ', 'are quit ', 'good ', 'the first ', 'scene ', 'about ', 'san francisco ', 'is not ', 'great but ', 'funni ', 'the first time ', 'skip ', 'over ', 'those ', 'if you can ', 'it almost ', 'not ', 'worth watch ', 'howev thi ', 'realli is a ', 'funni ', 'funni ', 'standup ', 'show that ', 'everyon ', 'should ', 'see ', 'i wa ', 'dead ', 'at the time']\n",
            "Accuracy using BPE Tokenization with Normalization: 0.7440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noted accuracy is slightly higher with BPE tokenization and some normalization compared to BPE tokenization alone (0.7440 vs 0.7270)\n",
        "\n",
        "\n",
        "Further text normalization implementing contraction expansion."
      ],
      "metadata": {
        "id": "hF3hAvhW8GTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK components are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 1: Define the function to load the text from file paths\n",
        "def load_text(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "# Step 2: Define the text normalization function with stemming, regex, and contraction expansion\n",
        "def normalize_text(text):\n",
        "    # Expand contractions\n",
        "    text = contractions.fix(text)  # Expands things like \"can't\" to \"cannot\", \"I'm\" to \"I am\"\n",
        "\n",
        "    # Lowercase all text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove non-alphabetical characters using regex (you can modify this for custom cleaning)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize words using nltk's word_tokenize\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Apply stemming using PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Join the stemmed words back into a single string\n",
        "    normalized_text = ' '.join(stemmed_words)\n",
        "\n",
        "    return normalized_text\n",
        "\n",
        "# Step 3: Define the BPE tokenizer and train it on your dataset\n",
        "def train_bpe_tokenizer(train_texts):\n",
        "    tokenizer = Tokenizer(BPE())\n",
        "    trainer = BpeTrainer()\n",
        "    tokenizer.train_from_iterator(train_texts, trainer=trainer)\n",
        "    return tokenizer\n",
        "\n",
        "# Step 4: Tokenize the input text using the trained BPE tokenizer\n",
        "def tokenize_bpe(tokenizer, text):\n",
        "    return tokenizer.encode(text).tokens\n",
        "\n",
        "# Step 5: Create a function to convert texts into embeddings using BPE tokenization\n",
        "def bpe_vectorize(tokenizer, texts, vectorizer=None): # Add vectorizer argument\n",
        "    # First, tokenize each text using the BPE tokenizer\n",
        "    tokenized_texts = [tokenize_bpe(tokenizer, text) for text in texts]\n",
        "\n",
        "    # Use CountVectorizer to convert tokens to vectors\n",
        "    # If vectorizer is provided, use it; otherwise, create a new one\n",
        "    if vectorizer is None:\n",
        "        vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=None, token_pattern=None, lowercase=False)\n",
        "        embeddings = vectorizer.fit_transform(tokenized_texts)\n",
        "    else:\n",
        "        embeddings = vectorizer.transform(tokenized_texts)  # Use existing vectorizer for transform\n",
        "    return embeddings, vectorizer # Return the vectorizer as well\n",
        "\n",
        "\n",
        "# Step 6: Load the actual text content from file paths for both training and test sets\n",
        "train_texts = [load_text(filepath) for filepath in data_train['path']]\n",
        "test_texts = [load_text(filepath) for filepath in data_test['path']]\n",
        "\n",
        "# Step 7: Normalize the texts using the custom normalization function (with contraction expansion)\n",
        "normalized_train_texts = [normalize_text(text) for text in train_texts]\n",
        "normalized_test_texts = [normalize_text(text) for text in test_texts]\n",
        "\n",
        "# Step 8: Train the BPE tokenizer on the normalized training data\n",
        "bpe_tokenizer_normalized = train_bpe_tokenizer(normalized_train_texts)\n",
        "\n",
        "# Step 9: Print a few tokenized sample texts from the normalized dataset\n",
        "print(\"Sample Tokenized Texts using BPE Tokenizer with Normalization:\")\n",
        "for text in normalized_train_texts[:5]:  # Display first 5 tokenized texts as an example\n",
        "    print(tokenize_bpe(bpe_tokenizer_normalized, text))\n",
        "\n",
        "# Step 10: Create embeddings for both training and test sets using BPE tokenizer on normalized texts\n",
        "embeddings_train_bpe_normalized, vectorizer = bpe_vectorize(bpe_tokenizer_normalized, normalized_train_texts) # Get vectorizer from training\n",
        "embeddings_test_bpe_normalized, _ = bpe_vectorize(bpe_tokenizer_normalized, normalized_test_texts, vectorizer=vectorizer) # Reuse vectorizer for test\n",
        "\n",
        "\n",
        "# Step 11: Train and test a classifier (SVM) on BPE tokenized normalized data\n",
        "svm = LinearSVC(dual=False, max_iter=5000)\n",
        "\n",
        "# Train the classifier on the training embeddings\n",
        "svm.fit(embeddings_train_bpe_normalized, data_train['label'].values)\n",
        "\n",
        "# Test the classifier on the test embeddings\n",
        "predictions_normalized = svm.predict(embeddings_test_bpe_normalized)\n",
        "\n",
        "# Step 12: Calculate the accuracy on the test set with the additional normalization\n",
        "accuracy_bpe_normalized = accuracy_score(data_test['label'].values, predictions_normalized)\n",
        "print(f'Accuracy using BPE Tokenization with Normalization (including contraction expansion): {accuracy_bpe_normalized:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkATN9lD4hoK",
        "outputId": "5d9c6b91-d2e4-4a48-81cb-9bf33bf3b742"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Tokenized Texts using BPE Tokenizer with Normalization:\n",
            "['i recent ', 'watch thi ', 'but when it ', 'start ', 'i had no idea ', 'what the ', 'concept ', 'wa about ', 'what the ', 'topic ', 'was', 'in short ', 'i had no idea ', 'what it wa ', 'wa it a ', 'documentari ', 'wa it a ', 'comedi ', 'routine', 'wel ', 'it wa ', 'both', 'br br it ', 'start a littl slow ', 'but ', 'i think that is ', 'becaus i ', 'had', ' absolut ', 'no idea what ', 'type of program ', 'i wa ', 'view ', 'but it ', 'quickli ', 'suck ', 'me ', 'in the ', 'episod ', 'i watch ', 'had ', 'robert ', 'wu', 'hl ', 'discuss ', 'fact ', 'and ', 'fiction ', 'in histori ', 'mainli ', 'how ', 'we ', 'american ', 'learn ', 'histori', ' that is not ', 'realli ', 'true ', 'and ', 'how ', 'we got ', 'to learn ', 'what we ', 'did he ', 'did thi ', 'in such a ', 'way', ' as to ', 'keep the viewer ', 'complet ', 'entertain and ', 'interest ', 'i actual ', 'learn a ', 'few', ' thing', ' and that ', 'is a true ', 'indic of ', 'how ', 'effect ', 'thi ', 'type of program ', 'can ', 'bebr br ', 'i would love ', 'to see thi ', 'pick up', ' as a ', 'seri for ', 'hbo ', 'i believ ', 'it can be ', 'just as ', 'fun and ', 'effect ', 'with a ', 'varieti of ', 'topic ', 'especi ', 'if they are ', 'taught ', 'in the same ', 'type of ', 'manner', ' as thi ', 'episod']\n",
            "['pari ', 'is the ', 'place ', 'to be ', 'to ', 'enjoy ', 'beauti', ' art and ', 'music and ', 'to fall ', 'madli in love ', 'as is the case ', 'in thi film ', 'boy meet girl', ' they ', 'fall in love ', 'but ', 'someth ', 'stand ', 'in their ', 'way of ', 'etern ', 'happi the ', 'classic ', 'story', 'br br the ', 'wonder ', 'music ', 'of georg ', 'ger', 'sch', 'win ', 'complement ', 'the ', 'great ', 'danc by ', 'gene ', 'kelli and ', 'lesli car', 'on', ' an american in pari ', 'is a ', 'humor ', 'lightheart ', 'love ', 'film well worth ', 'watch', 'ingbr br ', 'br br']\n",
            "['domini', 'on ', 'tank ', 'polic ', 'is ', 'without a ', 'she will ', 'of a doubt ', 'one of the most ', 'amaz ', 'show ever ', 'produc ', 'but not ', 'just ', 'in the ', 'field ', 'of anim ', 'while the ', 'first part ', 'act and ', 'mostli ', 'consist ', 'of', ' action and ', 'fun the ', 'second part ', 'is more ', 'seriou and ', 'one ', 'should not ', 'treat the ', 'second ', 'part in the ', 'exact same ', 'way as ', 'first part ', 'the ', 'subtle', 'ti are ', 'truli ', 'out of thi ', 'world and the ', 'character ', 'is beyond ', 'brilliant ', 'you ', 'must have ', 'an extra ', 'degre of ', 'intellig ', 'to appreci the ', 'intricaci ', 'of the ', 'second part ', 'act ', 'and i ', 'do ', 'have some ', 'complaint ', 'though ', 'in the first ', 'part the ', 'tank ', 'bonapart ', 'quit ', 'liter ', 'jump ', 'over a ', 'tank ', 'she will', ' and it ', 'did not ', 'make ani sens at all ', 'one might ', 'also ', 'question the ', 'plausibl ', 'of ', 'bonapart ', 'jump ', 'on the ', 'wing of ', 'helicopt ', 'gun', 'ship ', 'even though ', 'it wa cool ', 'bu', 'ak', 'u ', 'ru', 'le']\n",
            "['everi onc in a while ', 'eddi murphi ', 'will ', 'surpris ', 'youbr br ', 'in a movi ', 'like the ', 'golden ', 'child ', 'especi', ' thi is a ', 'movi ', 'you would ', 'figur ', 'would ', 'star ', 'mayb ', 'har', 'rison ', 'ford ', 'or ', 'kurt russel ', 'or someon ', 'but ', 'eddi ', 'realli doe ', 'work ', 'he is ', 'smart ', 'he is ', 'funni ', 'he is ', 'brave ', 'kind ', 'cour', 'te', 'ou', ' thri', 'fti ', 'clean and ', 'everyth els', ' a ', 'hero ', 'should ', 'bebr br ', 'have been ', 'chosen to ', 'secur', ' a ', 'mystic ', 'child who ', 'hold the ', 'key ', 'to protect ', 'the world ', 'from ', 'complet ', 'evil ', 'em', 'bodi ', 'perfectli ', 'by danc ', 'eddi ', 'goe ', 'from ', 'california ', 'to ', 'ne', 'pal and ', 'back', ' all ', 'while the ', 'beauti ', 'ke', 'e ', 'nan', 'g ', 'lewi ', 'wonder ', 'if he ', 'is all ', 'he say ', 'he is', ' and a ', 'crazi ', 'old ', 'holi ', 'man ', 'wong ', 'perfect ', 'as alway ', 'know', ' that he ', 'isbr br it is ', 'excit ', 'breathtak ', 'in spot ', 'shock and ', 'of cours ', 'funni ', 'eddi ', 'is the ', 'onli', ' action ', 'hero ', 'i know ', 'who could ', 'beg', 'in a movi ', 'by make ', 'rude ', 'remark ', 'behind ', 'some guy ', 'read a ', 'porno ', 'magazin and ', 'end ', 'it with ', 'smartal', 'eck ', 'remark', ' about ', 'ed ', 'mcma', 'hon', 'br br no ', 'problem with thi ', 'child ', 'it is a ', 'golden ', 'fin', 'dbr br ', 'nine ', 'star ', 'viva ', 'ne', 'pal']\n",
            "['thi is an excel ', 'standup ', 'dvd ', 'eddi izzard ', 'is the funniest ', 'person ', 'i have seen ', 'in year ', 'hi ', 'routin ', 'is ', 'hilari and ', 'make ', 'for great ', 'convers ', 'with ', 'other ', 'who have seen it ', 'i ', 'highli recommend', ' thi one the ', 'part ', 'about the ', 'histori of ', 'europ ', 'is a bit ', 'slow ', 'but the end ', 'joke ', 'in french', ' are quit ', 'good becaus ', 'you do not have to ', 'speak french ', 'to get it ', 'although ', 'if you do ', 'it is still ', 'hilari', ' also the ', 'part about ', 'be a ', 'transvestit ', 'are quit ', 'good the ', 'first ', 'scene ', 'about ', 'san francisco ', 'is not ', 'great but ', 'funni the ', 'first time ', 'skip ', 'over', ' those ', 'if you can ', 'it is almost ', 'not ', 'worth watch ', 'howev thi ', 'realli is a ', 'funni ', 'funni ', 'standup ', 'show that ', 'everyon should ', 'see ', 'i wa ', 'dead', ' at the time']\n",
            "Accuracy using BPE Tokenization with Normalization (including contraction expansion): 0.7130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noted accuracy is actually lower with BPE tokenization with Normalization and inclusion of contraction expansion compared to BPE tokenization with Normalization without the contraction expansion (0.7130 vs 0.7440)"
      ],
      "metadata": {
        "id": "YfWCh9_K8lMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YpZRlF9T9F-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}